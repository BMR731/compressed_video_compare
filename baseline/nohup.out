/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r2plus1d_18.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
Epoch: [0][0/475],	Time 27.493 (27.493)	Data 1.201 (1.201)	siamese Loss 5.2434 (5.2434)	classifier Loss 0.1564 (0.1564)	
Epoch: [0][20/475],	Time 0.468 (1.705)	Data 0.002 (0.059)	siamese Loss 2.9730 (48.2052)	classifier Loss 0.1716 (0.2137)	
Epoch: [0][40/475],	Time 0.467 (1.073)	Data 0.003 (0.032)	siamese Loss 0.7861 (25.0043)	classifier Loss 0.1800 (0.1954)	
Epoch: [0][60/475],	Time 0.479 (0.856)	Data 0.003 (0.022)	siamese Loss 1.1559 (17.1198)	classifier Loss 0.1731 (0.1885)	
Epoch: [0][80/475],	Time 0.467 (0.746)	Data 0.002 (0.017)	siamese Loss 0.7557 (13.0380)	classifier Loss 0.1636 (0.1845)	
Epoch: [0][100/475],	Time 0.467 (0.680)	Data 0.003 (0.014)	siamese Loss 0.3026 (10.5396)	classifier Loss 0.1701 (0.1829)	
Epoch: [0][120/475],	Time 0.465 (0.636)	Data 0.003 (0.012)	siamese Loss 0.3605 (8.8644)	classifier Loss 0.1767 (0.1822)	
Epoch: [0][140/475],	Time 0.471 (0.604)	Data 0.002 (0.011)	siamese Loss 0.2149 (7.6527)	classifier Loss 0.1705 (0.1816)	
Epoch: [0][160/475],	Time 0.463 (0.580)	Data 0.003 (0.010)	siamese Loss 0.3297 (6.7456)	classifier Loss 0.1850 (0.1809)	
Epoch: [0][180/475],	Time 0.474 (0.562)	Data 0.003 (0.009)	siamese Loss 0.2990 (6.0399)	classifier Loss 0.1803 (0.1803)	
Epoch: [0][200/475],	Time 0.468 (0.547)	Data 0.002 (0.009)	siamese Loss 0.3514 (5.4697)	classifier Loss 0.1821 (0.1800)	
Epoch: [0][220/475],	Time 0.468 (0.535)	Data 0.002 (0.008)	siamese Loss 0.3095 (5.0076)	classifier Loss 0.2034 (0.1805)	
Epoch: [0][240/475],	Time 0.459 (0.525)	Data 0.003 (0.008)	siamese Loss 0.2224 (4.6217)	classifier Loss 0.2196 (0.1808)	
Epoch: [0][260/475],	Time 0.470 (0.516)	Data 0.002 (0.007)	siamese Loss 0.2609 (4.2927)	classifier Loss 0.1479 (0.1809)	
Epoch: [0][280/475],	Time 0.460 (0.509)	Data 0.002 (0.007)	siamese Loss 0.1952 (4.0111)	classifier Loss 0.1713 (0.1811)	
Epoch: [0][300/475],	Time 0.460 (0.502)	Data 0.004 (0.007)	siamese Loss 0.4314 (3.7672)	classifier Loss 0.1909 (0.1812)	
Epoch: [0][320/475],	Time 0.486 (0.496)	Data 0.005 (0.006)	siamese Loss 0.3328 (3.5529)	classifier Loss 0.1912 (0.1809)	
Epoch: [0][340/475],	Time 0.457 (0.490)	Data 0.003 (0.006)	siamese Loss 0.3543 (3.3639)	classifier Loss 0.1753 (0.1809)	
Epoch: [0][360/475],	Time 0.461 (0.486)	Data 0.005 (0.006)	siamese Loss 0.3115 (3.1953)	classifier Loss 0.2046 (0.1809)	
Epoch: [0][380/475],	Time 0.470 (0.482)	Data 0.002 (0.006)	siamese Loss 0.3135 (3.0430)	classifier Loss 0.1774 (0.1805)	
Epoch: [0][400/475],	Time 0.463 (0.479)	Data 0.003 (0.006)	siamese Loss 0.2559 (2.9083)	classifier Loss 0.1686 (0.1814)	
Epoch: [0][420/475],	Time 0.460 (0.475)	Data 0.003 (0.006)	siamese Loss 0.3138 (2.7858)	classifier Loss 0.1916 (0.1810)	
Epoch: [0][440/475],	Time 0.459 (0.472)	Data 0.003 (0.005)	siamese Loss 0.3117 (2.6740)	classifier Loss 0.1730 (0.1809)	
Epoch: [0][460/475],	Time 0.461 (0.469)	Data 0.003 (0.005)	siamese Loss 0.2642 (2.5713)	classifier Loss 0.1642 (0.1805)	
Validate: [0/46]	Time 3.165 (3.165)	siamese loss 1.3589 (1.3589)	clf loss 0.6905 (0.6905)	
Validate: [20/46]	Time 0.174 (0.322)	siamese loss 1.0563 (1.3986)	clf loss 0.6736 (0.6923)	
Validate: [40/46]	Time 0.183 (0.254)	siamese loss 1.1863 (1.4388)	clf loss 0.6930 (0.6973)	
Validating Results: siamese Loss 1.41407, classification loss 0.69638, Accuracy: 49.000%
Epoch: [1][0/475],	Time 2.805 (2.805)	Data 2.403 (2.403)	siamese Loss 0.3489 (0.3489)	classifier Loss 0.1776 (0.1776)	
Epoch: [1][20/475],	Time 0.459 (0.517)	Data 0.003 (0.116)	siamese Loss 0.2762 (0.3231)	classifier Loss 0.1728 (0.1750)	
Epoch: [1][40/475],	Time 0.464 (0.461)	Data 0.003 (0.061)	siamese Loss 0.2688 (0.3132)	classifier Loss 0.1689 (0.1741)	
Epoch: [1][60/475],	Time 0.457 (0.442)	Data 0.003 (0.042)	siamese Loss 0.1414 (0.3162)	classifier Loss 0.1543 (0.1756)	
Epoch: [1][80/475],	Time 0.464 (0.432)	Data 0.003 (0.032)	siamese Loss 0.2783 (0.3054)	classifier Loss 0.1776 (0.1751)	
Epoch: [1][100/475],	Time 0.458 (0.426)	Data 0.003 (0.026)	siamese Loss 0.3390 (0.3026)	classifier Loss 0.1712 (0.1749)	
Epoch: [1][120/475],	Time 0.470 (0.423)	Data 0.002 (0.022)	siamese Loss 0.3126 (0.3024)	classifier Loss 0.1719 (0.1746)	
Epoch: [1][140/475],	Time 0.472 (0.421)	Data 0.003 (0.020)	siamese Loss 0.2581 (0.3039)	classifier Loss 0.1827 (0.1748)	
Epoch: [1][160/475],	Time 0.463 (0.419)	Data 0.003 (0.018)	siamese Loss 0.2015 (0.3030)	classifier Loss 0.1735 (0.1747)	
Epoch: [1][180/475],	Time 0.469 (0.418)	Data 0.003 (0.016)	siamese Loss 0.3926 (0.3052)	classifier Loss 0.1793 (0.1746)	
Epoch: [1][200/475],	Time 0.452 (0.417)	Data 0.003 (0.015)	siamese Loss 0.3677 (0.3057)	classifier Loss 0.1699 (0.1745)	
Epoch: [1][220/475],	Time 0.458 (0.416)	Data 0.008 (0.014)	siamese Loss 0.2736 (0.3047)	classifier Loss 0.1973 (0.1745)	
Epoch: [1][240/475],	Time 0.455 (0.414)	Data 0.003 (0.013)	siamese Loss 0.3944 (0.3056)	classifier Loss 0.1791 (0.1746)	
Epoch: [1][260/475],	Time 0.462 (0.414)	Data 0.003 (0.012)	siamese Loss 0.2378 (0.3057)	classifier Loss 0.1697 (0.1748)	
Epoch: [1][280/475],	Time 0.464 (0.414)	Data 0.004 (0.011)	siamese Loss 0.4354 (0.3098)	classifier Loss 0.1605 (0.1747)	
Epoch: [1][300/475],	Time 0.459 (0.413)	Data 0.003 (0.011)	siamese Loss 0.1758 (0.3089)	classifier Loss 0.1785 (0.1751)	
Epoch: [1][320/475],	Time 0.455 (0.413)	Data 0.003 (0.010)	siamese Loss 0.2827 (0.3089)	classifier Loss 0.1739 (0.1751)	
Epoch: [1][340/475],	Time 0.455 (0.412)	Data 0.003 (0.010)	siamese Loss 0.2980 (0.3089)	classifier Loss 0.1713 (0.1751)	
Epoch: [1][360/475],	Time 0.460 (0.412)	Data 0.003 (0.009)	siamese Loss 0.2278 (0.3078)	classifier Loss 0.1721 (0.1750)	
Epoch: [1][380/475],	Time 0.455 (0.411)	Data 0.003 (0.009)	siamese Loss 0.3321 (0.3082)	classifier Loss 0.1738 (0.1749)	
Epoch: [1][400/475],	Time 0.462 (0.411)	Data 0.003 (0.009)	siamese Loss 0.3604 (0.3093)	classifier Loss 0.1736 (0.1748)	
Epoch: [1][420/475],	Time 0.460 (0.411)	Data 0.004 (0.008)	siamese Loss 0.2145 (0.3098)	classifier Loss 0.1741 (0.1748)	
Epoch: [1][440/475],	Time 0.457 (0.411)	Data 0.003 (0.008)	siamese Loss 0.4005 (0.3092)	classifier Loss 0.1710 (0.1747)	
Epoch: [1][460/475],	Time 0.460 (0.410)	Data 0.003 (0.008)	siamese Loss 0.1986 (0.3092)	classifier Loss 0.1732 (0.1746)	
Epoch: [2][0/475],	Time 2.619 (2.619)	Data 2.308 (2.308)	siamese Loss 0.2975 (0.2975)	classifier Loss 0.1731 (0.1731)	
Epoch: [2][20/475],	Time 0.458 (0.510)	Data 0.003 (0.112)	siamese Loss 0.2552 (0.3193)	classifier Loss 0.1750 (0.1743)	
Epoch: [2][40/475],	Time 0.454 (0.457)	Data 0.003 (0.059)	siamese Loss 0.3069 (0.3150)	classifier Loss 0.1722 (0.1738)	
Epoch: [2][60/475],	Time 0.449 (0.441)	Data 0.003 (0.040)	siamese Loss 0.1806 (0.3128)	classifier Loss 0.1673 (0.1736)	
Epoch: [2][80/475],	Time 0.468 (0.433)	Data 0.003 (0.031)	siamese Loss 0.3190 (0.3135)	classifier Loss 0.1726 (0.1736)	
Epoch: [2][100/475],	Time 0.459 (0.427)	Data 0.003 (0.026)	siamese Loss 0.3727 (0.3140)	classifier Loss 0.1749 (0.1735)	
Epoch: [2][120/475],	Time 0.466 (0.424)	Data 0.003 (0.022)	siamese Loss 0.2273 (0.3144)	classifier Loss 0.1741 (0.1736)	
Epoch: [2][140/475],	Time 0.469 (0.421)	Data 0.002 (0.019)	siamese Loss 0.3259 (0.3109)	classifier Loss 0.1732 (0.1735)	
Epoch: [2][160/475],	Time 0.477 (0.420)	Data 0.003 (0.017)	siamese Loss 0.2581 (0.3086)	classifier Loss 0.1722 (0.1735)	
Epoch: [2][180/475],	Time 0.467 (0.419)	Data 0.003 (0.015)	siamese Loss 0.3028 (0.3092)	classifier Loss 0.1765 (0.1736)	
Epoch: [2][200/475],	Time 0.461 (0.419)	Data 0.003 (0.014)	siamese Loss 0.2668 (0.3080)	classifier Loss 0.1800 (0.1737)	
Epoch: [2][220/475],	Time 0.453 (0.418)	Data 0.003 (0.013)	siamese Loss 0.2135 (0.3074)	classifier Loss 0.1785 (0.1737)	
Epoch: [2][240/475],	Time 0.473 (0.418)	Data 0.003 (0.012)	siamese Loss 0.2902 (0.3079)	classifier Loss 0.1734 (0.1736)	
Epoch: [2][260/475],	Time 0.470 (0.417)	Data 0.003 (0.012)	siamese Loss 0.4245 (0.3063)	classifier Loss 0.1823 (0.1737)	
Epoch: [2][280/475],	Time 0.460 (0.417)	Data 0.003 (0.011)	siamese Loss 0.2974 (0.3070)	classifier Loss 0.1722 (0.1738)	
Epoch: [2][300/475],	Time 0.459 (0.416)	Data 0.003 (0.011)	siamese Loss 0.2256 (0.3065)	classifier Loss 0.1720 (0.1737)	
Epoch: [2][320/475],	Time 0.470 (0.416)	Data 0.002 (0.010)	siamese Loss 0.2526 (0.3051)	classifier Loss 0.1720 (0.1737)	
Epoch: [2][340/475],	Time 0.475 (0.416)	Data 0.002 (0.010)	siamese Loss 0.2123 (0.3049)	classifier Loss 0.1675 (0.1737)	
Epoch: [2][360/475],	Time 0.472 (0.416)	Data 0.002 (0.009)	siamese Loss 0.2673 (0.3046)	classifier Loss 0.1747 (0.1737)	
Epoch: [2][380/475],	Time 0.457 (0.415)	Data 0.003 (0.009)	siamese Loss 0.3735 (0.3065)	classifier Loss 0.1701 (0.1737)	
Epoch: [2][400/475],	Time 0.471 (0.415)	Data 0.002 (0.009)	siamese Loss 0.3349 (0.3060)	classifier Loss 0.1697 (0.1737)	
Epoch: [2][420/475],	Time 0.477 (0.415)	Data 0.002 (0.008)	siamese Loss 0.3641 (0.3058)	classifier Loss 0.1735 (0.1738)	
Epoch: [2][440/475],	Time 0.482 (0.415)	Data 0.003 (0.008)	siamese Loss 0.2374 (0.3059)	classifier Loss 0.1723 (0.1738)	
Epoch: [2][460/475],	Time 0.470 (0.415)	Data 0.002 (0.008)	siamese Loss 0.4812 (0.3070)	classifier Loss 0.1672 (0.1738)	
Epoch: [3][0/475],	Time 3.094 (3.094)	Data 2.762 (2.762)	siamese Loss 0.2751 (0.2751)	classifier Loss 0.1863 (0.1863)	
Epoch: [3][20/475],	Time 0.455 (0.541)	Data 0.002 (0.133)	siamese Loss 0.3954 (0.3003)	classifier Loss 0.1742 (0.1759)	
Epoch: [3][40/475],	Time 0.473 (0.479)	Data 0.003 (0.070)	siamese Loss 0.3475 (0.2933)	classifier Loss 0.1723 (0.1742)	
Epoch: [3][60/475],	Time 0.466 (0.458)	Data 0.002 (0.048)	siamese Loss 0.3361 (0.2928)	classifier Loss 0.1760 (0.1741)	
Epoch: [3][80/475],	Time 0.470 (0.447)	Data 0.003 (0.036)	siamese Loss 0.2712 (0.2919)	classifier Loss 0.1717 (0.1737)	
Epoch: [3][100/475],	Time 0.472 (0.440)	Data 0.002 (0.030)	siamese Loss 0.2478 (0.2956)	classifier Loss 0.1687 (0.1740)	
Epoch: [3][120/475],	Time 0.470 (0.436)	Data 0.002 (0.025)	siamese Loss 0.2807 (0.2942)	classifier Loss 0.1734 (0.1737)	
Epoch: [3][140/475],	Time 0.458 (0.433)	Data 0.001 (0.022)	siamese Loss 0.3343 (0.2945)	classifier Loss 0.1726 (0.1736)	
Epoch: [3][160/475],	Time 0.490 (0.431)	Data 0.002 (0.020)	siamese Loss 0.2372 (0.3051)	classifier Loss 0.1682 (0.1734)	
Epoch: [3][180/475],	Time 0.467 (0.429)	Data 0.003 (0.018)	siamese Loss 0.3144 (0.3026)	classifier Loss 0.1675 (0.1736)	
Epoch: [3][200/475],	Time 0.461 (0.427)	Data 0.003 (0.016)	siamese Loss 0.2436 (0.3028)	classifier Loss 0.1768 (0.1736)	
Epoch: [3][220/475],	Time 0.460 (0.426)	Data 0.003 (0.015)	siamese Loss 0.2841 (0.3018)	classifier Loss 0.1735 (0.1736)	
Epoch: [3][240/475],	Time 0.473 (0.424)	Data 0.003 (0.014)	siamese Loss 0.3818 (0.3044)	classifier Loss 0.1761 (0.1736)	
Epoch: [3][260/475],	Time 0.475 (0.424)	Data 0.002 (0.013)	siamese Loss 0.2288 (0.3037)	classifier Loss 0.1729 (0.1736)	
Epoch: [3][280/475],	Time 0.462 (0.423)	Data 0.002 (0.012)	siamese Loss 0.2992 (0.3025)	classifier Loss 0.1723 (0.1735)	
Epoch: [3][300/475],	Time 0.468 (0.422)	Data 0.002 (0.012)	siamese Loss 0.3430 (0.3043)	classifier Loss 0.1712 (0.1736)	
Epoch: [3][320/475],	Time 0.472 (0.421)	Data 0.002 (0.011)	siamese Loss 0.2751 (0.3040)	classifier Loss 0.1730 (0.1736)	
Epoch: [3][340/475],	Time 0.463 (0.421)	Data 0.003 (0.011)	siamese Loss 0.3110 (0.3035)	classifier Loss 0.1749 (0.1736)	
Epoch: [3][360/475],	Time 0.462 (0.420)	Data 0.002 (0.010)	siamese Loss 0.3941 (0.3040)	classifier Loss 0.1802 (0.1736)	
Epoch: [3][380/475],	Time 0.467 (0.420)	Data 0.003 (0.010)	siamese Loss 0.2934 (0.3047)	classifier Loss 0.1748 (0.1736)	
Epoch: [3][400/475],	Time 0.471 (0.419)	Data 0.004 (0.009)	siamese Loss 0.4217 (0.3057)	classifier Loss 0.1699 (0.1735)	
Epoch: [3][420/475],	Time 0.463 (0.419)	Data 0.002 (0.009)	siamese Loss 0.2521 (0.3039)	classifier Loss 0.1743 (0.1736)	
Epoch: [3][440/475],	Time 0.466 (0.419)	Data 0.002 (0.009)	siamese Loss 0.2961 (0.3042)	classifier Loss 0.1757 (0.1736)	
Epoch: [3][460/475],	Time 0.471 (0.418)	Data 0.002 (0.009)	siamese Loss 0.2966 (0.3033)	classifier Loss 0.1742 (0.1736)	
Epoch: [4][0/475],	Time 2.535 (2.535)	Data 2.216 (2.216)	siamese Loss 0.4489 (0.4489)	classifier Loss 0.1798 (0.1798)	
Epoch: [4][20/475],	Time 0.480 (0.516)	Data 0.002 (0.108)	siamese Loss 0.3961 (0.3289)	classifier Loss 0.1763 (0.1745)	
Epoch: [4][40/475],	Time 0.468 (0.466)	Data 0.002 (0.056)	siamese Loss 0.2465 (0.3098)	classifier Loss 0.1716 (0.1738)	
Epoch: [4][60/475],	Time 0.467 (0.449)	Data 0.002 (0.039)	siamese Loss 0.2072 (0.3022)	classifier Loss 0.1733 (0.1737)	
Epoch: [4][80/475],	Time 0.468 (0.440)	Data 0.003 (0.030)	siamese Loss 0.3545 (0.2963)	classifier Loss 0.1741 (0.1737)	
Epoch: [4][100/475],	Time 0.467 (0.433)	Data 0.002 (0.024)	siamese Loss 0.4224 (0.2928)	classifier Loss 0.1803 (0.1736)	
Epoch: [4][120/475],	Time 0.464 (0.429)	Data 0.002 (0.021)	siamese Loss 0.3883 (0.2935)	classifier Loss 0.1761 (0.1737)	
Epoch: [4][140/475],	Time 0.461 (0.425)	Data 0.003 (0.018)	siamese Loss 0.2352 (0.2941)	classifier Loss 0.1729 (0.1738)	
Epoch: [4][160/475],	Time 0.461 (0.423)	Data 0.003 (0.016)	siamese Loss 0.3117 (0.2949)	classifier Loss 0.1723 (0.1737)	
Epoch: [4][180/475],	Time 0.459 (0.420)	Data 0.002 (0.015)	siamese Loss 0.3043 (0.2986)	classifier Loss 0.1731 (0.1737)	
Epoch: [4][200/475],	Time 0.462 (0.419)	Data 0.002 (0.014)	siamese Loss 0.2688 (0.2983)	classifier Loss 0.1747 (0.1736)	
Epoch: [4][220/475],	Time 0.458 (0.418)	Data 0.002 (0.013)	siamese Loss 0.2814 (0.2981)	classifier Loss 0.1733 (0.1737)	
Epoch: [4][240/475],	Time 0.455 (0.416)	Data 0.002 (0.012)	siamese Loss 0.2397 (0.2987)	classifier Loss 0.1727 (0.1736)	
Epoch: [4][260/475],	Time 0.464 (0.415)	Data 0.002 (0.011)	siamese Loss 0.3221 (0.2977)	classifier Loss 0.1737 (0.1736)	
Epoch: [4][280/475],	Time 0.462 (0.414)	Data 0.004 (0.010)	siamese Loss 0.2900 (0.2976)	classifier Loss 0.1731 (0.1736)	
Epoch: [4][300/475],	Time 0.459 (0.414)	Data 0.003 (0.010)	siamese Loss 0.3693 (0.2965)	classifier Loss 0.1741 (0.1736)	
Epoch: [4][320/475],	Time 0.459 (0.413)	Data 0.003 (0.009)	siamese Loss 0.2580 (0.2969)	classifier Loss 0.1734 (0.1736)	
Epoch: [4][340/475],	Time 0.457 (0.412)	Data 0.003 (0.009)	siamese Loss 0.2697 (0.2972)	classifier Loss 0.1732 (0.1735)	
Epoch: [4][360/475],	Time 0.458 (0.412)	Data 0.003 (0.009)	siamese Loss 0.2563 (0.2966)	classifier Loss 0.1752 (0.1735)	
Epoch: [4][380/475],	Time 0.460 (0.411)	Data 0.002 (0.008)	siamese Loss 0.3285 (0.2968)	classifier Loss 0.1745 (0.1735)	
Epoch: [4][400/475],	Time 0.462 (0.411)	Data 0.003 (0.008)	siamese Loss 0.2124 (0.2965)	classifier Loss 0.1725 (0.1735)	
Epoch: [4][420/475],	Time 0.462 (0.411)	Data 0.004 (0.008)	siamese Loss 0.2320 (0.2967)	classifier Loss 0.1773 (0.1735)	
Epoch: [4][440/475],	Time 0.459 (0.410)	Data 0.002 (0.008)	siamese Loss 0.2799 (0.2964)	classifier Loss 0.1707 (0.1735)	
Epoch: [4][460/475],	Time 0.459 (0.410)	Data 0.002 (0.007)	siamese Loss 0.3236 (0.2951)	classifier Loss 0.1756 (0.1736)	
Epoch: [5][0/475],	Time 3.072 (3.072)	Data 2.718 (2.718)	siamese Loss 0.2363 (0.2363)	classifier Loss 0.1761 (0.1761)	
Epoch: [5][20/475],	Time 0.485 (0.547)	Data 0.002 (0.133)	siamese Loss 0.2892 (0.2687)	classifier Loss 0.1645 (0.1727)	
Epoch: [5][40/475],	Time 0.471 (0.482)	Data 0.003 (0.069)	siamese Loss 0.2041 (0.2763)	classifier Loss 0.1645 (0.1736)	
Epoch: [5][60/475],	Time 0.472 (0.460)	Data 0.002 (0.047)	siamese Loss 0.2186 (0.2838)	classifier Loss 0.1755 (0.1736)	
Epoch: [5][80/475],	Time 0.471 (0.449)	Data 0.003 (0.036)	siamese Loss 0.2644 (0.2874)	classifier Loss 0.1726 (0.1737)	
Epoch: [5][100/475],	Time 0.478 (0.442)	Data 0.003 (0.030)	siamese Loss 0.1871 (0.2854)	classifier Loss 0.1702 (0.1735)	
Epoch: [5][120/475],	Time 0.471 (0.436)	Data 0.002 (0.025)	siamese Loss 0.4057 (0.2866)	classifier Loss 0.1748 (0.1735)	
Epoch: [5][140/475],	Time 0.461 (0.432)	Data 0.003 (0.022)	siamese Loss 0.3482 (0.2872)	classifier Loss 0.1751 (0.1735)	
Epoch: [5][160/475],	Time 0.466 (0.430)	Data 0.008 (0.020)	siamese Loss 0.2675 (0.2882)	classifier Loss 0.1727 (0.1734)	
Epoch: [5][180/475],	Time 0.462 (0.427)	Data 0.002 (0.018)	siamese Loss 0.3431 (0.2889)	classifier Loss 0.1737 (0.1734)	
Epoch: [5][200/475],	Time 0.473 (0.426)	Data 0.002 (0.016)	siamese Loss 0.2141 (0.2901)	classifier Loss 0.1729 (0.1734)	
Epoch: [5][220/475],	Time 0.469 (0.424)	Data 0.003 (0.015)	siamese Loss 0.2896 (0.2895)	classifier Loss 0.1728 (0.1734)	
Epoch: [5][240/475],	Time 0.460 (0.423)	Data 0.003 (0.014)	siamese Loss 0.2214 (0.2903)	classifier Loss 0.1728 (0.1734)	
Epoch: [5][260/475],	Time 0.458 (0.421)	Data 0.004 (0.013)	siamese Loss 0.2586 (0.2912)	classifier Loss 0.1730 (0.1734)	
Epoch: [5][280/475],	Time 0.471 (0.421)	Data 0.008 (0.012)	siamese Loss 0.2847 (0.2934)	classifier Loss 0.1740 (0.1734)	
Epoch: [5][300/475],	Time 0.463 (0.420)	Data 0.003 (0.012)	siamese Loss 0.3718 (0.2939)	classifier Loss 0.1701 (0.1734)	
Epoch: [5][320/475],	Time 0.467 (0.419)	Data 0.003 (0.011)	siamese Loss 0.2771 (0.2936)	classifier Loss 0.1756 (0.1735)	
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r2plus1d_18.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
Epoch: [0][0/475],	Time 19.551 (19.551)	Data 1.251 (1.251)	siamese Loss 5.4238 (5.4238)	classifier Loss 0.1558 (0.1558)	
Epoch: [0][20/475],	Time 0.466 (1.317)	Data 0.003 (0.062)	siamese Loss 2.8176 (12.0079)	classifier Loss 0.1796 (0.2302)	
Epoch: [0][40/475],	Time 0.463 (0.875)	Data 0.002 (0.033)	siamese Loss 0.8375 (10.6453)	classifier Loss 0.1753 (0.2020)	
Epoch: [0][60/475],	Time 0.453 (0.722)	Data 0.003 (0.023)	siamese Loss 0.6942 (7.3923)	classifier Loss 0.1925 (0.1922)	
Epoch: [0][80/475],	Time 0.464 (0.645)	Data 0.003 (0.018)	siamese Loss 0.4116 (5.7140)	classifier Loss 0.1798 (0.1885)	
Epoch: [0][100/475],	Time 0.460 (0.599)	Data 0.003 (0.015)	siamese Loss 0.3260 (4.6696)	classifier Loss 0.1757 (0.1859)	
Epoch: [0][120/475],	Time 0.467 (0.568)	Data 0.004 (0.013)	siamese Loss 0.4621 (3.9632)	classifier Loss 0.1604 (0.1839)	
Epoch: [0][140/475],	Time 0.457 (0.544)	Data 0.002 (0.012)	siamese Loss 0.1752 (3.4533)	classifier Loss 0.1739 (0.1826)	
Epoch: [0][160/475],	Time 0.454 (0.527)	Data 0.003 (0.010)	siamese Loss 0.4042 (3.0689)	classifier Loss 0.1817 (0.1819)	
Epoch: [0][180/475],	Time 0.468 (0.513)	Data 0.002 (0.010)	siamese Loss 0.2081 (2.7696)	classifier Loss 0.1759 (0.1814)	
Epoch: [0][200/475],	Time 0.457 (0.502)	Data 0.003 (0.009)	siamese Loss 0.3676 (2.5293)	classifier Loss 0.1805 (0.1815)	
Epoch: [0][220/475],	Time 0.449 (0.494)	Data 0.003 (0.008)	siamese Loss 0.3218 (2.3317)	classifier Loss 0.1742 (0.1813)	
Epoch: [0][240/475],	Time 0.457 (0.487)	Data 0.003 (0.008)	siamese Loss 0.3044 (2.1683)	classifier Loss 0.1866 (0.1810)	
Epoch: [0][260/475],	Time 0.457 (0.480)	Data 0.002 (0.007)	siamese Loss 0.4083 (2.0323)	classifier Loss 0.1802 (0.1809)	
Epoch: [0][280/475],	Time 0.458 (0.475)	Data 0.002 (0.007)	siamese Loss 0.4395 (1.9129)	classifier Loss 0.1715 (0.1810)	
Epoch: [0][300/475],	Time 0.459 (0.470)	Data 0.003 (0.007)	siamese Loss 0.2496 (1.8092)	classifier Loss 0.1770 (0.1807)	
Epoch: [0][320/475],	Time 0.456 (0.466)	Data 0.002 (0.006)	siamese Loss 0.3806 (1.7174)	classifier Loss 0.1715 (0.1803)	
Epoch: [0][340/475],	Time 0.462 (0.462)	Data 0.003 (0.006)	siamese Loss 0.3729 (1.6363)	classifier Loss 0.1724 (0.1800)	
Epoch: [0][360/475],	Time 0.461 (0.459)	Data 0.002 (0.006)	siamese Loss 0.2830 (1.5632)	classifier Loss 0.1700 (0.1798)	
Epoch: [0][380/475],	Time 0.463 (0.456)	Data 0.002 (0.006)	siamese Loss 0.4198 (1.5001)	classifier Loss 0.1844 (0.1803)	
Epoch: [0][400/475],	Time 0.459 (0.453)	Data 0.002 (0.006)	siamese Loss 0.3516 (1.4425)	classifier Loss 0.1494 (0.1804)	
Epoch: [0][420/475],	Time 0.462 (0.451)	Data 0.002 (0.005)	siamese Loss 0.3250 (1.3906)	classifier Loss 0.1705 (0.1804)	
Epoch: [0][440/475],	Time 0.460 (0.448)	Data 0.002 (0.005)	siamese Loss 0.4282 (1.3444)	classifier Loss 0.1703 (0.1801)	
Epoch: [0][460/475],	Time 0.462 (0.446)	Data 0.002 (0.005)	siamese Loss 0.3132 (1.3020)	classifier Loss 0.1778 (0.1800)	
Validate: [0/46]	Time 2.666 (2.666)	siamese loss 1.1957 (1.1957)	clf loss 0.7238 (0.7238)	
Validate: [20/46]	Time 0.175 (0.296)	siamese loss 1.6728 (1.1732)	clf loss 0.6325 (0.7112)	
Validate: [40/46]	Time 0.173 (0.237)	siamese loss 1.3049 (1.2975)	clf loss 0.7234 (0.7290)	
Validating Results: siamese Loss 1.27814, classification loss 0.72407, Accuracy: 49.000%
Epoch: [1][0/475],	Time 2.799 (2.799)	Data 2.343 (2.343)	siamese Loss 0.4303 (0.4303)	classifier Loss 0.1890 (0.1890)	
Epoch: [1][20/475],	Time 0.460 (0.516)	Data 0.002 (0.114)	siamese Loss 0.3585 (0.3430)	classifier Loss 0.1683 (0.1771)	
Epoch: [1][40/475],	Time 0.458 (0.460)	Data 0.002 (0.059)	siamese Loss 0.3411 (0.3494)	classifier Loss 0.1757 (0.1768)	
Epoch: [1][60/475],	Time 0.457 (0.441)	Data 0.003 (0.041)	siamese Loss 0.4571 (0.3455)	classifier Loss 0.1703 (0.1757)	
Epoch: [1][80/475],	Time 0.454 (0.431)	Data 0.002 (0.031)	siamese Loss 0.4122 (0.3512)	classifier Loss 0.1778 (0.1750)	
Epoch: [1][100/475],	Time 0.453 (0.425)	Data 0.003 (0.025)	siamese Loss 0.5715 (0.3497)	classifier Loss 0.1601 (0.1761)	
Epoch: [1][120/475],	Time 0.461 (0.421)	Data 0.002 (0.022)	siamese Loss 0.3240 (0.3500)	classifier Loss 0.1740 (0.1756)	
Epoch: [1][140/475],	Time 0.458 (0.418)	Data 0.002 (0.019)	siamese Loss 0.4296 (0.3480)	classifier Loss 0.1698 (0.1760)	
Epoch: [1][160/475],	Time 0.456 (0.416)	Data 0.003 (0.017)	siamese Loss 0.2583 (0.3464)	classifier Loss 0.1868 (0.1760)	
Epoch: [1][180/475],	Time 0.457 (0.414)	Data 0.002 (0.015)	siamese Loss 0.2860 (0.3442)	classifier Loss 0.1727 (0.1757)	
Epoch: [1][200/475],	Time 0.459 (0.413)	Data 0.002 (0.014)	siamese Loss 0.3075 (0.3425)	classifier Loss 0.1719 (0.1755)	
Epoch: [1][220/475],	Time 0.453 (0.412)	Data 0.003 (0.013)	siamese Loss 0.2957 (0.3425)	classifier Loss 0.1738 (0.1756)	
Epoch: [1][240/475],	Time 0.455 (0.411)	Data 0.002 (0.012)	siamese Loss 0.4345 (0.3438)	classifier Loss 0.1693 (0.1754)	
Epoch: [1][260/475],	Time 0.450 (0.410)	Data 0.002 (0.011)	siamese Loss 0.3956 (0.3455)	classifier Loss 0.1800 (0.1756)	
Epoch: [1][280/475],	Time 0.448 (0.409)	Data 0.003 (0.011)	siamese Loss 0.2368 (0.3449)	classifier Loss 0.1811 (0.1758)	
Epoch: [1][300/475],	Time 0.458 (0.409)	Data 0.003 (0.010)	siamese Loss 0.3263 (0.3430)	classifier Loss 0.1656 (0.1758)	
Epoch: [1][320/475],	Time 0.458 (0.408)	Data 0.002 (0.010)	siamese Loss 0.2041 (0.3427)	classifier Loss 0.1716 (0.1759)	
Epoch: [1][340/475],	Time 0.457 (0.408)	Data 0.002 (0.009)	siamese Loss 0.2843 (0.3426)	classifier Loss 0.1738 (0.1758)	
Epoch: [1][360/475],	Time 0.454 (0.407)	Data 0.002 (0.009)	siamese Loss 0.1694 (0.3406)	classifier Loss 0.1691 (0.1756)	
Epoch: [1][380/475],	Time 0.457 (0.407)	Data 0.003 (0.008)	siamese Loss 0.3762 (0.3424)	classifier Loss 0.1750 (0.1756)	
Epoch: [1][400/475],	Time 0.458 (0.407)	Data 0.004 (0.008)	siamese Loss 0.4040 (0.3431)	classifier Loss 0.1641 (0.1759)	
Epoch: [1][420/475],	Time 0.455 (0.406)	Data 0.003 (0.008)	siamese Loss 0.2510 (0.3426)	classifier Loss 0.1754 (0.1758)	
Epoch: [1][440/475],	Time 0.455 (0.406)	Data 0.003 (0.008)	siamese Loss 0.4711 (0.3412)	classifier Loss 0.1850 (0.1757)	
Epoch: [1][460/475],	Time 0.452 (0.406)	Data 0.002 (0.007)	siamese Loss 0.2081 (0.3420)	classifier Loss 0.2035 (0.1755)	
Epoch: [2][0/475],	Time 2.646 (2.646)	Data 2.338 (2.338)	siamese Loss 0.1828 (0.1828)	classifier Loss 0.1737 (0.1737)	
Epoch: [2][20/475],	Time 0.454 (0.508)	Data 0.004 (0.114)	siamese Loss 0.5436 (0.3387)	classifier Loss 0.1800 (0.1790)	
Epoch: [2][40/475],	Time 0.459 (0.456)	Data 0.003 (0.060)	siamese Loss 0.4482 (0.3350)	classifier Loss 0.1739 (0.1769)	
Epoch: [2][60/475],	Time 0.456 (0.438)	Data 0.002 (0.041)	siamese Loss 0.2547 (0.3398)	classifier Loss 0.1745 (0.1759)	
Epoch: [2][80/475],	Time 0.457 (0.428)	Data 0.003 (0.032)	siamese Loss 0.3892 (0.3413)	classifier Loss 0.1720 (0.1751)	
Epoch: [2][100/475],	Time 0.455 (0.423)	Data 0.004 (0.026)	siamese Loss 0.4464 (0.3420)	classifier Loss 0.1730 (0.1750)	
Epoch: [2][120/475],	Time 0.456 (0.419)	Data 0.002 (0.022)	siamese Loss 0.3580 (0.3417)	classifier Loss 0.1747 (0.1748)	
Epoch: [2][140/475],	Time 0.455 (0.416)	Data 0.002 (0.019)	siamese Loss 0.4039 (0.3428)	classifier Loss 0.1721 (0.1749)	
Epoch: [2][160/475],	Time 0.455 (0.414)	Data 0.002 (0.017)	siamese Loss 0.3309 (0.3442)	classifier Loss 0.1777 (0.1753)	
Epoch: [2][180/475],	Time 0.457 (0.413)	Data 0.002 (0.015)	siamese Loss 0.3707 (0.3450)	classifier Loss 0.1732 (0.1754)	
Epoch: [2][200/475],	Time 0.456 (0.411)	Data 0.004 (0.014)	siamese Loss 0.4457 (0.3431)	classifier Loss 0.1630 (0.1754)	
Epoch: [2][220/475],	Time 0.458 (0.410)	Data 0.003 (0.013)	siamese Loss 0.2569 (0.3426)	classifier Loss 0.1750 (0.1754)	
Epoch: [2][240/475],	Time 0.459 (0.410)	Data 0.002 (0.012)	siamese Loss 0.3762 (0.3399)	classifier Loss 0.1764 (0.1753)	
Epoch: [2][260/475],	Time 0.458 (0.409)	Data 0.002 (0.011)	siamese Loss 0.3321 (0.3386)	classifier Loss 0.1700 (0.1752)	
Epoch: [2][280/475],	Time 0.458 (0.408)	Data 0.002 (0.011)	siamese Loss 0.3193 (0.3375)	classifier Loss 0.1753 (0.1751)	
Epoch: [2][300/475],	Time 0.458 (0.408)	Data 0.002 (0.010)	siamese Loss 0.3859 (0.3363)	classifier Loss 0.1951 (0.1750)	
Epoch: [2][320/475],	Time 0.459 (0.407)	Data 0.002 (0.010)	siamese Loss 0.3161 (0.3368)	classifier Loss 0.1788 (0.1749)	
Epoch: [2][340/475],	Time 0.461 (0.407)	Data 0.002 (0.009)	siamese Loss 0.3117 (0.3343)	classifier Loss 0.1669 (0.1749)	
Epoch: [2][360/475],	Time 0.460 (0.407)	Data 0.002 (0.009)	siamese Loss 0.4469 (0.3350)	classifier Loss 0.1663 (0.1749)	
Epoch: [2][380/475],	Time 0.456 (0.406)	Data 0.003 (0.009)	siamese Loss 0.3571 (0.3339)	classifier Loss 0.1792 (0.1754)	
Epoch: [2][400/475],	Time 0.456 (0.406)	Data 0.002 (0.008)	siamese Loss 0.3520 (0.3307)	classifier Loss 0.1983 (0.1752)	
Epoch: [2][420/475],	Time 0.462 (0.406)	Data 0.002 (0.008)	siamese Loss 0.3626 (0.3296)	classifier Loss 0.1802 (0.1753)	
Epoch: [2][440/475],	Time 0.463 (0.406)	Data 0.002 (0.008)	siamese Loss 0.4518 (0.3293)	classifier Loss 0.1597 (0.1754)	
Epoch: [2][460/475],	Time 0.462 (0.406)	Data 0.003 (0.007)	siamese Loss 0.2303 (0.3287)	classifier Loss 0.1751 (0.1753)	
Epoch: [3][0/475],	Time 2.843 (2.843)	Data 2.525 (2.525)	siamese Loss 0.3138 (0.3138)	classifier Loss 0.1869 (0.1869)	
Epoch: [3][20/475],	Time 0.458 (0.519)	Data 0.002 (0.122)	siamese Loss 0.2438 (0.3167)	classifier Loss 0.1780 (0.1769)	
Epoch: [3][40/475],	Time 0.461 (0.462)	Data 0.002 (0.064)	siamese Loss 0.3819 (0.3185)	classifier Loss 0.1667 (0.1760)	
Epoch: [3][60/475],	Time 0.459 (0.442)	Data 0.002 (0.043)	siamese Loss 0.3073 (0.3162)	classifier Loss 0.1849 (0.1762)	
Epoch: [3][80/475],	Time 0.459 (0.432)	Data 0.003 (0.033)	siamese Loss 0.3455 (0.3204)	classifier Loss 0.1744 (0.1760)	
Epoch: [3][100/475],	Time 0.452 (0.427)	Data 0.002 (0.027)	siamese Loss 0.4530 (0.3188)	classifier Loss 0.1688 (0.1757)	
Epoch: [3][120/475],	Time 0.449 (0.423)	Data 0.003 (0.023)	siamese Loss 0.2464 (0.3162)	classifier Loss 0.1703 (0.1755)	
Epoch: [3][140/475],	Time 0.450 (0.420)	Data 0.004 (0.020)	siamese Loss 0.3332 (0.3115)	classifier Loss 0.1744 (0.1751)	
Epoch: [3][160/475],	Time 0.455 (0.418)	Data 0.002 (0.018)	siamese Loss 0.4678 (0.3090)	classifier Loss 0.1846 (0.1747)	
Epoch: [3][180/475],	Time 0.462 (0.416)	Data 0.002 (0.016)	siamese Loss 0.3678 (0.3092)	classifier Loss 0.1894 (0.1749)	
Epoch: [3][200/475],	Time 0.463 (0.415)	Data 0.002 (0.015)	siamese Loss 0.2229 (0.3114)	classifier Loss 0.1839 (0.1749)	
Epoch: [3][220/475],	Time 0.465 (0.414)	Data 0.002 (0.014)	siamese Loss 0.2542 (0.3143)	classifier Loss 0.2008 (0.1747)	
Epoch: [3][240/475],	Time 0.461 (0.413)	Data 0.002 (0.013)	siamese Loss 0.3329 (0.3137)	classifier Loss 0.1735 (0.1747)	
Epoch: [3][260/475],	Time 0.461 (0.412)	Data 0.002 (0.012)	siamese Loss 0.3202 (0.3133)	classifier Loss 0.1757 (0.1746)	
Epoch: [3][280/475],	Time 0.454 (0.412)	Data 0.003 (0.011)	siamese Loss 0.2882 (0.3120)	classifier Loss 0.1681 (0.1745)	
Epoch: [3][300/475],	Time 0.456 (0.411)	Data 0.002 (0.011)	siamese Loss 0.2701 (0.3133)	classifier Loss 0.1743 (0.1745)	
Epoch: [3][320/475],	Time 0.457 (0.410)	Data 0.002 (0.010)	siamese Loss 0.3240 (0.3136)	classifier Loss 0.1826 (0.1745)	
Epoch: [3][340/475],	Time 0.462 (0.410)	Data 0.002 (0.010)	siamese Loss 0.3622 (0.3144)	classifier Loss 0.1709 (0.1744)	
Epoch: [3][360/475],	Time 0.460 (0.410)	Data 0.002 (0.009)	siamese Loss 0.3581 (0.3140)	classifier Loss 0.1750 (0.1744)	
Epoch: [3][380/475],	Time 0.463 (0.409)	Data 0.002 (0.009)	siamese Loss 0.2593 (0.3136)	classifier Loss 0.1763 (0.1745)	
Epoch: [3][400/475],	Time 0.463 (0.409)	Data 0.002 (0.009)	siamese Loss 0.3940 (0.3134)	classifier Loss 0.1718 (0.1745)	
Epoch: [3][420/475],	Time 0.462 (0.409)	Data 0.002 (0.008)	siamese Loss 0.1902 (0.3120)	classifier Loss 0.1781 (0.1745)	
Epoch: [3][440/475],	Time 0.460 (0.408)	Data 0.003 (0.008)	siamese Loss 0.2718 (0.3114)	classifier Loss 0.1719 (0.1744)	
Epoch: [3][460/475],	Time 0.456 (0.408)	Data 0.002 (0.008)	siamese Loss 0.2418 (0.3104)	classifier Loss 0.1669 (0.1744)	
Epoch: [4][0/475],	Time 2.650 (2.650)	Data 2.343 (2.343)	siamese Loss 0.2647 (0.2647)	classifier Loss 0.1722 (0.1722)	
Epoch: [4][20/475],	Time 0.466 (0.513)	Data 0.003 (0.114)	siamese Loss 0.2812 (0.3091)	classifier Loss 0.1722 (0.1732)	
Epoch: [4][40/475],	Time 0.459 (0.460)	Data 0.003 (0.060)	siamese Loss 0.2809 (0.3140)	classifier Loss 0.1732 (0.1731)	
Epoch: [4][60/475],	Time 0.459 (0.441)	Data 0.003 (0.041)	siamese Loss 0.2371 (0.3058)	classifier Loss 0.1761 (0.1736)	
Epoch: [4][80/475],	Time 0.453 (0.432)	Data 0.002 (0.031)	siamese Loss 0.2248 (0.3029)	classifier Loss 0.1705 (0.1737)	
Epoch: [4][100/475],	Time 0.460 (0.426)	Data 0.002 (0.026)	siamese Loss 0.3414 (0.3035)	classifier Loss 0.1733 (0.1736)	
Epoch: [4][120/475],	Time 0.462 (0.422)	Data 0.002 (0.022)	siamese Loss 0.2252 (0.3020)	classifier Loss 0.1757 (0.1736)	
Epoch: [4][140/475],	Time 0.460 (0.420)	Data 0.002 (0.019)	siamese Loss 0.2998 (0.2968)	classifier Loss 0.1737 (0.1734)	
Epoch: [4][160/475],	Time 0.456 (0.418)	Data 0.003 (0.017)	siamese Loss 0.2925 (0.2978)	classifier Loss 0.1662 (0.1734)	
Epoch: [4][180/475],	Time 0.462 (0.416)	Data 0.003 (0.015)	siamese Loss 0.4472 (0.2980)	classifier Loss 0.2012 (0.1736)	
Epoch: [4][200/475],	Time 0.457 (0.415)	Data 0.002 (0.014)	siamese Loss 0.3910 (0.2967)	classifier Loss 0.1749 (0.1735)	
Epoch: [4][220/475],	Time 0.460 (0.414)	Data 0.002 (0.013)	siamese Loss 0.3344 (0.2969)	classifier Loss 0.1706 (0.1735)	
Epoch: [4][240/475],	Time 0.461 (0.413)	Data 0.002 (0.012)	siamese Loss 0.3096 (0.2965)	classifier Loss 0.1728 (0.1736)	
Epoch: [4][260/475],	Time 0.459 (0.412)	Data 0.002 (0.011)	siamese Loss 0.3210 (0.2981)	classifier Loss 0.1783 (0.1737)	
Epoch: [4][280/475],	Time 0.460 (0.412)	Data 0.002 (0.011)	siamese Loss 0.2239 (0.2984)	classifier Loss 0.1703 (0.1736)	
Epoch: [4][300/475],	Time 0.465 (0.411)	Data 0.002 (0.010)	siamese Loss 0.4141 (0.2988)	classifier Loss 0.1709 (0.1736)	
Epoch: [4][320/475],	Time 0.460 (0.410)	Data 0.002 (0.010)	siamese Loss 0.3495 (0.2981)	classifier Loss 0.1643 (0.1736)	
Epoch: [4][340/475],	Time 0.462 (0.410)	Data 0.002 (0.009)	siamese Loss 0.2513 (0.2983)	classifier Loss 0.1726 (0.1736)	
Epoch: [4][360/475],	Time 0.458 (0.410)	Data 0.003 (0.009)	siamese Loss 0.2875 (0.2981)	classifier Loss 0.1774 (0.1736)	
Epoch: [4][380/475],	Time 0.461 (0.409)	Data 0.002 (0.008)	siamese Loss 0.3763 (0.2980)	classifier Loss 0.1748 (0.1736)	
Epoch: [4][400/475],	Time 0.464 (0.409)	Data 0.002 (0.008)	siamese Loss 0.3408 (0.2989)	classifier Loss 0.1757 (0.1736)	
Epoch: [4][420/475],	Time 0.462 (0.409)	Data 0.002 (0.008)	siamese Loss 0.2927 (0.2996)	classifier Loss 0.1736 (0.1736)	
Epoch: [4][440/475],	Time 0.460 (0.409)	Data 0.003 (0.008)	siamese Loss 0.2575 (0.2992)	classifier Loss 0.1781 (0.1735)	
Epoch: [4][460/475],	Time 0.461 (0.408)	Data 0.003 (0.007)	siamese Loss 0.4503 (0.2994)	classifier Loss 0.1744 (0.1735)	
Epoch: [5][0/475],	Time 2.732 (2.732)	Data 2.404 (2.404)	siamese Loss 0.2302 (0.2302)	classifier Loss 0.1729 (0.1729)	
Epoch: [5][20/475],	Time 0.463 (0.522)	Data 0.003 (0.116)	siamese Loss 0.3336 (0.2945)	classifier Loss 0.1715 (0.1735)	
Epoch: [5][40/475],	Time 0.455 (0.464)	Data 0.003 (0.061)	siamese Loss 0.5219 (0.3187)	classifier Loss 0.1896 (0.1749)	
Epoch: [5][60/475],	Time 0.459 (0.445)	Data 0.004 (0.042)	siamese Loss 0.3795 (0.3144)	classifier Loss 0.1684 (0.1749)	
Epoch: [5][80/475],	Time 0.456 (0.435)	Data 0.003 (0.032)	siamese Loss 0.2959 (0.3115)	classifier Loss 0.1803 (0.1747)	
Epoch: [5][100/475],	Time 0.462 (0.429)	Data 0.003 (0.026)	siamese Loss 0.1989 (0.3123)	classifier Loss 0.1933 (0.1744)	
Epoch: [5][120/475],	Time 0.462 (0.425)	Data 0.002 (0.022)	siamese Loss 0.2996 (0.3147)	classifier Loss 0.1774 (0.1740)	
Epoch: [5][140/475],	Time 0.457 (0.422)	Data 0.003 (0.019)	siamese Loss 0.2260 (0.3128)	classifier Loss 0.1778 (0.1740)	
Epoch: [5][160/475],	Time 0.454 (0.420)	Data 0.002 (0.017)	siamese Loss 0.3257 (0.3082)	classifier Loss 0.1721 (0.1740)	
Epoch: [5][180/475],	Time 0.470 (0.418)	Data 0.002 (0.016)	siamese Loss 0.2386 (0.3087)	classifier Loss 0.1786 (0.1740)	
Epoch: [5][200/475],	Time 0.461 (0.417)	Data 0.002 (0.014)	siamese Loss 0.3743 (0.3065)	classifier Loss 0.1790 (0.1740)	
Epoch: [5][220/475],	Time 0.471 (0.416)	Data 0.002 (0.013)	siamese Loss 0.2806 (0.3069)	classifier Loss 0.1731 (0.1739)	
Epoch: [5][240/475],	Time 0.459 (0.415)	Data 0.003 (0.012)	siamese Loss 0.3084 (0.3066)	classifier Loss 0.1728 (0.1739)	
Epoch: [5][260/475],	Time 0.460 (0.414)	Data 0.003 (0.012)	siamese Loss 0.3098 (0.3069)	classifier Loss 0.1732 (0.1738)	
Epoch: [5][280/475],	Time 0.461 (0.413)	Data 0.002 (0.011)	siamese Loss 0.3830 (0.3067)	classifier Loss 0.1688 (0.1738)	
Epoch: [5][300/475],	Time 0.457 (0.412)	Data 0.002 (0.010)	siamese Loss 0.3585 (0.3070)	classifier Loss 0.1715 (0.1738)	
Epoch: [5][320/475],	Time 0.459 (0.412)	Data 0.003 (0.010)	siamese Loss 0.1378 (0.3069)	classifier Loss 0.1791 (0.1738)	
Epoch: [5][340/475],	Time 0.466 (0.411)	Data 0.002 (0.009)	siamese Loss 0.2110 (0.3069)	classifier Loss 0.1723 (0.1738)	
Epoch: [5][360/475],	Time 0.462 (0.411)	Data 0.002 (0.009)	siamese Loss 0.3356 (0.3069)	classifier Loss 0.1742 (0.1738)	
Epoch: [5][380/475],	Time 0.463 (0.410)	Data 0.002 (0.009)	siamese Loss 0.3130 (0.3065)	classifier Loss 0.1713 (0.1738)	
Epoch: [5][400/475],	Time 0.455 (0.410)	Data 0.002 (0.008)	siamese Loss 0.3736 (0.3062)	classifier Loss 0.1737 (0.1737)	
Epoch: [5][420/475],	Time 0.461 (0.410)	Data 0.002 (0.008)	siamese Loss 0.3780 (0.3067)	classifier Loss 0.1701 (0.1737)	
Epoch: [5][440/475],	Time 0.455 (0.410)	Data 0.002 (0.008)	siamese Loss 0.2762 (0.3071)	classifier Loss 0.1793 (0.1738)	
Epoch: [5][460/475],	Time 0.462 (0.409)	Data 0.002 (0.008)	siamese Loss 0.3798 (0.3075)	classifier Loss 0.1733 (0.1738)	
Validate: [0/46]	Time 2.645 (2.645)	siamese loss 1.4762 (1.4762)	clf loss 0.6965 (0.6965)	
Validate: [20/46]	Time 0.173 (0.298)	siamese loss 0.7398 (1.2832)	clf loss 0.6756 (0.6906)	
Validate: [40/46]	Time 0.182 (0.239)	siamese loss 1.0130 (1.3884)	clf loss 0.6867 (0.6942)	
Validating Results: siamese Loss 1.36716, classification loss 0.69366, Accuracy: 49.000%
Epoch: [6][0/475],	Time 3.059 (3.059)	Data 2.749 (2.749)	siamese Loss 0.4026 (0.4026)	classifier Loss 0.1761 (0.1761)	
Epoch: [6][20/475],	Time 0.457 (0.529)	Data 0.002 (0.132)	siamese Loss 0.3424 (0.3232)	classifier Loss 0.1729 (0.1741)	
Epoch: [6][40/475],	Time 0.461 (0.467)	Data 0.003 (0.069)	siamese Loss 0.2958 (0.3156)	classifier Loss 0.1708 (0.1738)	
Epoch: [6][60/475],	Time 0.463 (0.446)	Data 0.002 (0.047)	siamese Loss 0.1925 (0.3005)	classifier Loss 0.1749 (0.1740)	
Epoch: [6][80/475],	Time 0.457 (0.435)	Data 0.002 (0.036)	siamese Loss 0.3912 (0.2999)	classifier Loss 0.1763 (0.1739)	
Epoch: [6][100/475],	Time 0.458 (0.428)	Data 0.003 (0.029)	siamese Loss 0.3361 (0.3024)	classifier Loss 0.1725 (0.1739)	
Epoch: [6][120/475],	Time 0.463 (0.424)	Data 0.002 (0.025)	siamese Loss 0.3091 (0.3075)	classifier Loss 0.1740 (0.1738)	
Epoch: [6][140/475],	Time 0.458 (0.421)	Data 0.002 (0.022)	siamese Loss 0.3128 (0.3089)	classifier Loss 0.1735 (0.1738)	
Epoch: [6][160/475],	Time 0.456 (0.419)	Data 0.002 (0.019)	siamese Loss 0.3644 (0.3080)	classifier Loss 0.1727 (0.1738)	
Epoch: [6][180/475],	Time 0.455 (0.417)	Data 0.003 (0.017)	siamese Loss 0.3534 (0.3098)	classifier Loss 0.1718 (0.1738)	
Epoch: [6][200/475],	Time 0.460 (0.415)	Data 0.002 (0.016)	siamese Loss 0.3701 (0.3099)	classifier Loss 0.1727 (0.1737)	
Epoch: [6][220/475],	Time 0.448 (0.414)	Data 0.003 (0.015)	siamese Loss 0.2897 (0.3096)	classifier Loss 0.1727 (0.1737)	
Epoch: [6][240/475],	Time 0.460 (0.413)	Data 0.002 (0.014)	siamese Loss 0.3467 (0.3085)	classifier Loss 0.1730 (0.1737)	
Epoch: [6][260/475],	Time 0.456 (0.412)	Data 0.002 (0.013)	siamese Loss 0.2053 (0.3091)	classifier Loss 0.1736 (0.1736)	
Epoch: [6][280/475],	Time 0.458 (0.412)	Data 0.002 (0.012)	siamese Loss 0.2692 (0.3081)	classifier Loss 0.1755 (0.1736)	
Epoch: [6][300/475],	Time 0.463 (0.411)	Data 0.002 (0.011)	siamese Loss 0.3105 (0.3089)	classifier Loss 0.1780 (0.1736)	
Epoch: [6][320/475],	Time 0.457 (0.410)	Data 0.002 (0.011)	siamese Loss 0.2186 (0.3089)	classifier Loss 0.1754 (0.1736)	
Epoch: [6][340/475],	Time 0.457 (0.410)	Data 0.002 (0.010)	siamese Loss 0.1973 (0.3078)	classifier Loss 0.1854 (0.1737)	
Epoch: [6][360/475],	Time 0.459 (0.410)	Data 0.002 (0.010)	siamese Loss 0.3127 (0.3088)	classifier Loss 0.1730 (0.1737)	
Epoch: [6][380/475],	Time 0.454 (0.409)	Data 0.002 (0.010)	siamese Loss 0.3574 (0.3079)	classifier Loss 0.1753 (0.1737)	
Epoch: [6][400/475],	Time 0.461 (0.409)	Data 0.003 (0.009)	siamese Loss 0.5291 (0.3085)	classifier Loss 0.1801 (0.1737)	
Epoch: [6][420/475],	Time 0.457 (0.409)	Data 0.003 (0.009)	siamese Loss 0.2220 (0.3078)	classifier Loss 0.1730 (0.1736)	
Epoch: [6][440/475],	Time 0.458 (0.408)	Data 0.003 (0.009)	siamese Loss 0.3669 (0.3073)	classifier Loss 0.1744 (0.1736)	
Epoch: [6][460/475],	Time 0.461 (0.408)	Data 0.003 (0.008)	siamese Loss 0.3432 (0.3068)	classifier Loss 0.1714 (0.1736)	
Epoch: [7][0/475],	Time 2.661 (2.661)	Data 2.356 (2.356)	siamese Loss 0.2262 (0.2262)	classifier Loss 0.1702 (0.1702)	
Epoch: [7][20/475],	Time 0.463 (0.512)	Data 0.002 (0.114)	siamese Loss 0.3419 (0.3168)	classifier Loss 0.1729 (0.1737)	
Epoch: [7][40/475],	Time 0.458 (0.458)	Data 0.002 (0.059)	siamese Loss 0.3645 (0.3020)	classifier Loss 0.1782 (0.1731)	
Epoch: [7][60/475],	Time 0.459 (0.439)	Data 0.002 (0.041)	siamese Loss 0.4147 (0.3021)	classifier Loss 0.1866 (0.1736)	
Epoch: [7][80/475],	Time 0.460 (0.430)	Data 0.002 (0.031)	siamese Loss 0.1849 (0.3027)	classifier Loss 0.1742 (0.1738)	
Epoch: [7][100/475],	Time 0.458 (0.424)	Data 0.002 (0.026)	siamese Loss 0.3369 (0.3051)	classifier Loss 0.1738 (0.1737)	
Epoch: [7][120/475],	Time 0.465 (0.420)	Data 0.003 (0.022)	siamese Loss 0.2870 (0.3003)	classifier Loss 0.1732 (0.1737)	
Epoch: [7][140/475],	Time 0.462 (0.418)	Data 0.002 (0.019)	siamese Loss 0.3266 (0.2999)	classifier Loss 0.1773 (0.1737)	
Epoch: [7][160/475],	Time 0.460 (0.416)	Data 0.003 (0.017)	siamese Loss 0.3243 (0.2974)	classifier Loss 0.1746 (0.1735)	
Epoch: [7][180/475],	Time 0.460 (0.414)	Data 0.003 (0.015)	siamese Loss 0.4090 (0.3011)	classifier Loss 0.1780 (0.1736)	
Epoch: [7][200/475],	Time 0.471 (0.414)	Data 0.003 (0.014)	siamese Loss 0.2620 (0.3026)	classifier Loss 0.1710 (0.1736)	
Epoch: [7][220/475],	Time 0.467 (0.413)	Data 0.002 (0.013)	siamese Loss 0.3366 (0.3036)	classifier Loss 0.1738 (0.1736)	
Epoch: [7][240/475],	Time 0.461 (0.413)	Data 0.002 (0.012)	siamese Loss 0.3035 (0.3054)	classifier Loss 0.1718 (0.1736)	
Epoch: [7][260/475],	Time 0.459 (0.412)	Data 0.003 (0.011)	siamese Loss 0.2568 (0.3050)	classifier Loss 0.1787 (0.1736)	
Epoch: [7][280/475],	Time 0.457 (0.412)	Data 0.003 (0.011)	siamese Loss 0.2437 (0.3040)	classifier Loss 0.1757 (0.1737)	
Epoch: [7][300/475],	Time 0.456 (0.411)	Data 0.003 (0.010)	siamese Loss 0.3798 (0.3031)	classifier Loss 0.1732 (0.1737)	
Epoch: [7][320/475],	Time 0.459 (0.411)	Data 0.002 (0.010)	siamese Loss 0.2646 (0.3033)	classifier Loss 0.1722 (0.1736)	
Epoch: [7][340/475],	Time 0.460 (0.411)	Data 0.003 (0.009)	siamese Loss 0.2927 (0.3041)	classifier Loss 0.1713 (0.1736)	
Epoch: [7][360/475],	Time 0.459 (0.410)	Data 0.002 (0.009)	siamese Loss 0.2580 (0.3045)	classifier Loss 0.1779 (0.1736)	
Epoch: [7][380/475],	Time 0.462 (0.410)	Data 0.002 (0.009)	siamese Loss 0.3615 (0.3058)	classifier Loss 0.1729 (0.1737)	
Epoch: [7][400/475],	Time 0.459 (0.410)	Data 0.002 (0.008)	siamese Loss 0.1773 (0.3062)	classifier Loss 0.1738 (0.1736)	
Epoch: [7][420/475],	Time 0.474 (0.410)	Data 0.003 (0.008)	siamese Loss 0.2983 (0.3060)	classifier Loss 0.1791 (0.1736)	
Epoch: [7][440/475],	Time 0.465 (0.409)	Data 0.002 (0.008)	siamese Loss 0.2590 (0.3047)	classifier Loss 0.1753 (0.1736)	
Epoch: [7][460/475],	Time 0.451 (0.409)	Data 0.003 (0.008)	siamese Loss 0.3652 (0.3057)	classifier Loss 0.1713 (0.1736)	
Epoch: [8][0/475],	Time 2.778 (2.778)	Data 2.471 (2.471)	siamese Loss 0.4102 (0.4102)	classifier Loss 0.1714 (0.1714)	
Epoch: [8][20/475],	Time 0.459 (0.519)	Data 0.002 (0.120)	siamese Loss 0.2605 (0.3245)	classifier Loss 0.1729 (0.1725)	
Epoch: [8][40/475],	Time 0.458 (0.462)	Data 0.003 (0.062)	siamese Loss 0.3278 (0.3084)	classifier Loss 0.1717 (0.1730)	
Epoch: [8][60/475],	Time 0.452 (0.443)	Data 0.002 (0.043)	siamese Loss 0.3837 (0.3137)	classifier Loss 0.1723 (0.1731)	
Epoch: [8][80/475],	Time 0.461 (0.433)	Data 0.002 (0.033)	siamese Loss 0.3629 (0.3112)	classifier Loss 0.1697 (0.1732)	
Epoch: [8][100/475],	Time 0.462 (0.427)	Data 0.002 (0.027)	siamese Loss 0.1965 (0.3083)	classifier Loss 0.1780 (0.1734)	
Epoch: [8][120/475],	Time 0.474 (0.423)	Data 0.003 (0.023)	siamese Loss 0.2748 (0.3065)	classifier Loss 0.1766 (0.1735)	
Epoch: [8][140/475],	Time 0.461 (0.421)	Data 0.002 (0.020)	siamese Loss 0.3938 (0.3050)	classifier Loss 0.1746 (0.1735)	
Epoch: [8][160/475],	Time 0.461 (0.418)	Data 0.002 (0.018)	siamese Loss 0.3609 (0.3063)	classifier Loss 0.1735 (0.1735)	
Epoch: [8][180/475],	Time 0.455 (0.417)	Data 0.004 (0.016)	siamese Loss 0.2624 (0.3070)	classifier Loss 0.1732 (0.1735)	
Epoch: [8][200/475],	Time 0.476 (0.416)	Data 0.003 (0.015)	siamese Loss 0.2402 (0.3101)	classifier Loss 0.1740 (0.1735)	
Epoch: [8][220/475],	Time 0.460 (0.415)	Data 0.003 (0.014)	siamese Loss 0.2521 (0.3104)	classifier Loss 0.1762 (0.1735)	
Epoch: [8][240/475],	Time 0.455 (0.414)	Data 0.003 (0.013)	siamese Loss 0.3580 (0.3101)	classifier Loss 0.1728 (0.1735)	
Epoch: [8][260/475],	Time 0.455 (0.414)	Data 0.002 (0.012)	siamese Loss 0.2848 (0.3098)	classifier Loss 0.1736 (0.1735)	
Epoch: [8][280/475],	Time 0.472 (0.413)	Data 0.004 (0.011)	siamese Loss 0.3318 (0.3101)	classifier Loss 0.1736 (0.1735)	
Epoch: [8][300/475],	Time 0.458 (0.413)	Data 0.003 (0.011)	siamese Loss 0.2474 (0.3082)	classifier Loss 0.1703 (0.1735)	
Epoch: [8][320/475],	Time 0.462 (0.412)	Data 0.003 (0.010)	siamese Loss 0.3527 (0.3085)	classifier Loss 0.1717 (0.1735)	
Epoch: [8][340/475],	Time 0.454 (0.412)	Data 0.003 (0.010)	siamese Loss 0.3753 (0.3092)	classifier Loss 0.1737 (0.1735)	
Epoch: [8][360/475],	Time 0.469 (0.411)	Data 0.003 (0.009)	siamese Loss 0.3863 (0.3093)	classifier Loss 0.1730 (0.1735)	
Epoch: [8][380/475],	Time 0.456 (0.411)	Data 0.004 (0.009)	siamese Loss 0.2709 (0.3089)	classifier Loss 0.1737 (0.1735)	
Epoch: [8][400/475],	Time 0.465 (0.411)	Data 0.003 (0.009)	siamese Loss 0.3612 (0.3082)	classifier Loss 0.1734 (0.1735)	
Epoch: [8][420/475],	Time 0.460 (0.410)	Data 0.002 (0.008)	siamese Loss 0.3411 (0.3091)	classifier Loss 0.1774 (0.1735)	
Epoch: [8][440/475],	Time 0.473 (0.410)	Data 0.003 (0.008)	siamese Loss 0.3665 (0.3087)	classifier Loss 0.1748 (0.1735)	
Epoch: [8][460/475],	Time 0.463 (0.410)	Data 0.002 (0.008)	siamese Loss 0.2211 (0.3079)	classifier Loss 0.1724 (0.1735)	
Epoch: [9][0/475],	Time 2.966 (2.966)	Data 2.651 (2.651)	siamese Loss 0.4025 (0.4025)	classifier Loss 0.1772 (0.1772)	
Epoch: [9][20/475],	Time 0.459 (0.528)	Data 0.002 (0.128)	siamese Loss 0.3683 (0.3024)	classifier Loss 0.1751 (0.1736)	
Epoch: [9][40/475],	Time 0.457 (0.467)	Data 0.003 (0.067)	siamese Loss 0.4096 (0.3013)	classifier Loss 0.1740 (0.1733)	
Epoch: [9][60/475],	Time 0.464 (0.446)	Data 0.002 (0.045)	siamese Loss 0.3228 (0.3002)	classifier Loss 0.1740 (0.1735)	
Epoch: [9][80/475],	Time 0.462 (0.436)	Data 0.002 (0.035)	siamese Loss 0.3374 (0.2967)	classifier Loss 0.1732 (0.1734)	
Epoch: [9][100/475],	Time 0.460 (0.429)	Data 0.002 (0.028)	siamese Loss 0.2018 (0.2945)	classifier Loss 0.1666 (0.1732)	
Epoch: [9][120/475],	Time 0.459 (0.425)	Data 0.003 (0.024)	siamese Loss 0.2165 (0.2913)	classifier Loss 0.1626 (0.1728)	
Epoch: [9][140/475],	Time 0.460 (0.422)	Data 0.003 (0.021)	siamese Loss 0.3776 (0.2930)	classifier Loss 0.1908 (0.1731)	
Epoch: [9][160/475],	Time 0.453 (0.419)	Data 0.003 (0.019)	siamese Loss 0.2903 (0.2938)	classifier Loss 0.1785 (0.1736)	
Epoch: [9][180/475],	Time 0.462 (0.418)	Data 0.002 (0.017)	siamese Loss 0.2647 (0.2935)	classifier Loss 0.1729 (0.1736)	
Epoch: [9][200/475],	Time 0.463 (0.416)	Data 0.002 (0.015)	siamese Loss 0.2734 (0.2955)	classifier Loss 0.1703 (0.1736)	
Epoch: [9][220/475],	Time 0.457 (0.415)	Data 0.002 (0.014)	siamese Loss 0.3872 (0.2964)	classifier Loss 0.1707 (0.1736)	
Epoch: [9][240/475],	Time 0.458 (0.414)	Data 0.002 (0.013)	siamese Loss 0.4807 (0.2999)	classifier Loss 0.1635 (0.1735)	
Epoch: [9][260/475],	Time 0.461 (0.413)	Data 0.002 (0.012)	siamese Loss 0.2904 (0.3002)	classifier Loss 0.1738 (0.1735)	
Epoch: [9][280/475],	Time 0.461 (0.412)	Data 0.002 (0.012)	siamese Loss 0.3101 (0.3005)	classifier Loss 0.1727 (0.1735)	
Epoch: [9][300/475],	Time 0.462 (0.412)	Data 0.003 (0.011)	siamese Loss 0.2240 (0.3003)	classifier Loss 0.1800 (0.1735)	
Epoch: [9][320/475],	Time 0.459 (0.411)	Data 0.003 (0.010)	siamese Loss 0.4451 (0.2997)	classifier Loss 0.1686 (0.1735)	
Epoch: [9][340/475],	Time 0.474 (0.411)	Data 0.002 (0.010)	siamese Loss 0.3081 (0.2996)	classifier Loss 0.1719 (0.1735)	
Epoch: [9][360/475],	Time 0.470 (0.411)	Data 0.007 (0.010)	siamese Loss 0.3109 (0.2994)	classifier Loss 0.1742 (0.1735)	
Epoch: [9][380/475],	Time 0.460 (0.410)	Data 0.002 (0.009)	siamese Loss 0.2732 (0.3000)	classifier Loss 0.1736 (0.1735)	
Epoch: [9][400/475],	Time 0.463 (0.410)	Data 0.002 (0.009)	siamese Loss 0.2446 (0.2994)	classifier Loss 0.1732 (0.1735)	
Epoch: [9][420/475],	Time 0.455 (0.410)	Data 0.003 (0.009)	siamese Loss 0.3063 (0.2994)	classifier Loss 0.1737 (0.1735)	
Epoch: [9][440/475],	Time 0.448 (0.410)	Data 0.003 (0.008)	siamese Loss 0.3127 (0.2988)	classifier Loss 0.1734 (0.1735)	
Epoch: [9][460/475],	Time 0.450 (0.409)	Data 0.002 (0.008)	siamese Loss 0.4678 (0.2990)	classifier Loss 0.1757 (0.1735)	
Epoch: [10][0/475],	Time 2.770 (2.770)	Data 2.462 (2.462)	siamese Loss 0.2749 (0.2749)	classifier Loss 0.1730 (0.1730)	
Epoch: [10][20/475],	Time 0.459 (0.523)	Data 0.002 (0.119)	siamese Loss 0.3282 (0.2681)	classifier Loss 0.1737 (0.1731)	
Epoch: [10][40/475],	Time 0.462 (0.464)	Data 0.002 (0.062)	siamese Loss 0.3267 (0.2833)	classifier Loss 0.1732 (0.1731)	
Epoch: [10][60/475],	Time 0.461 (0.444)	Data 0.003 (0.043)	siamese Loss 0.2937 (0.2872)	classifier Loss 0.1737 (0.1734)	
Epoch: [10][80/475],	Time 0.459 (0.434)	Data 0.003 (0.033)	siamese Loss 0.2581 (0.2905)	classifier Loss 0.1729 (0.1734)	
Epoch: [10][100/475],	Time 0.455 (0.428)	Data 0.003 (0.027)	siamese Loss 0.2448 (0.2915)	classifier Loss 0.1749 (0.1732)	
Epoch: [10][120/475],	Time 0.459 (0.424)	Data 0.002 (0.023)	siamese Loss 0.3360 (0.2931)	classifier Loss 0.1743 (0.1734)	
Epoch: [10][140/475],	Time 0.458 (0.421)	Data 0.002 (0.020)	siamese Loss 0.2658 (0.2952)	classifier Loss 0.1837 (0.1732)	
Epoch: [10][160/475],	Time 0.461 (0.419)	Data 0.002 (0.017)	siamese Loss 0.3278 (0.2921)	classifier Loss 0.1645 (0.1735)	
Epoch: [10][180/475],	Time 0.461 (0.417)	Data 0.002 (0.016)	siamese Loss 0.2936 (0.2921)	classifier Loss 0.1760 (0.1735)	
Epoch: [10][200/475],	Time 0.461 (0.416)	Data 0.003 (0.014)	siamese Loss 0.1463 (0.2924)	classifier Loss 0.1796 (0.1735)	
Epoch: [10][220/475],	Time 0.467 (0.415)	Data 0.002 (0.013)	siamese Loss 0.2807 (0.2929)	classifier Loss 0.1726 (0.1735)	
Epoch: [10][240/475],	Time 0.468 (0.414)	Data 0.002 (0.012)	siamese Loss 0.3221 (0.2921)	classifier Loss 0.1740 (0.1734)	
Epoch: [10][260/475],	Time 0.462 (0.413)	Data 0.002 (0.012)	siamese Loss 0.2856 (0.2946)	classifier Loss 0.1751 (0.1735)	
Epoch: [10][280/475],	Time 0.455 (0.412)	Data 0.002 (0.011)	siamese Loss 0.2935 (0.2928)	classifier Loss 0.1720 (0.1735)	
Epoch: [10][300/475],	Time 0.463 (0.411)	Data 0.002 (0.010)	siamese Loss 0.2087 (0.2926)	classifier Loss 0.1738 (0.1735)	
Epoch: [10][320/475],	Time 0.459 (0.411)	Data 0.002 (0.010)	siamese Loss 0.2316 (0.2920)	classifier Loss 0.1731 (0.1735)	
Epoch: [10][340/475],	Time 0.460 (0.410)	Data 0.003 (0.009)	siamese Loss 0.2647 (0.2932)	classifier Loss 0.1732 (0.1735)	
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 36
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r2plus1d_18.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
Epoch: [0][0/159],	Time 21.868 (21.868)	Data 2.999 (2.999)	siamese Loss 16.0944 (16.0944)	classifier Loss 0.1734 (0.1734)	
Epoch: [0][20/159],	Time 1.079 (1.883)	Data 0.004 (0.146)	siamese Loss 2.1081 (27.1856)	classifier Loss 0.1736 (0.2188)	
Epoch: [0][40/159],	Time 1.089 (1.398)	Data 0.003 (0.076)	siamese Loss 0.4172 (14.0956)	classifier Loss 0.1702 (0.1986)	
Epoch: [0][60/159],	Time 1.082 (1.232)	Data 0.003 (0.052)	siamese Loss 0.2911 (9.5948)	classifier Loss 0.1764 (0.1911)	
Epoch: [0][80/159],	Time 1.087 (1.148)	Data 0.003 (0.040)	siamese Loss 0.3590 (7.2981)	classifier Loss 0.1747 (0.1872)	
Epoch: [0][100/159],	Time 1.088 (1.096)	Data 0.002 (0.033)	siamese Loss 0.3127 (5.9150)	classifier Loss 0.1758 (0.1849)	
Epoch: [0][120/159],	Time 1.087 (1.062)	Data 0.002 (0.028)	siamese Loss 0.2311 (4.9885)	classifier Loss 0.1760 (0.1833)	
Epoch: [0][140/159],	Time 1.091 (1.037)	Data 0.002 (0.024)	siamese Loss 0.2969 (4.3248)	classifier Loss 0.1774 (0.1824)	
Validate: [0/16]	Time 4.916 (4.916)	siamese loss 1.5213 (1.5213)	clf loss 0.7743 (0.7743)	
Validating Results: siamese Loss 1.28081, classification loss 0.72017, Accuracy: 50.000%
Epoch: [1][0/159],	Time 4.515 (4.515)	Data 3.633 (3.633)	siamese Loss 0.3942 (0.3942)	classifier Loss 0.1789 (0.1789)	
Epoch: [1][20/159],	Time 1.074 (1.049)	Data 0.003 (0.176)	siamese Loss 0.2650 (0.3336)	classifier Loss 0.1768 (0.1769)	
Epoch: [1][40/159],	Time 1.081 (0.966)	Data 0.003 (0.091)	siamese Loss 0.2571 (0.3203)	classifier Loss 0.1731 (0.1754)	
Epoch: [1][60/159],	Time 1.083 (0.938)	Data 0.003 (0.062)	siamese Loss 0.2112 (0.3179)	classifier Loss 0.1726 (0.1751)	
Epoch: [1][80/159],	Time 1.088 (0.924)	Data 0.002 (0.047)	siamese Loss 0.2945 (0.3166)	classifier Loss 0.1734 (0.1753)	
Epoch: [1][100/159],	Time 1.086 (0.916)	Data 0.003 (0.039)	siamese Loss 0.3078 (0.3182)	classifier Loss 0.1723 (0.1756)	
Epoch: [1][120/159],	Time 1.083 (0.910)	Data 0.002 (0.033)	siamese Loss 0.3766 (0.3171)	classifier Loss 0.1875 (0.1758)	
Epoch: [1][140/159],	Time 1.086 (0.906)	Data 0.003 (0.028)	siamese Loss 0.3502 (0.3160)	classifier Loss 0.1760 (0.1755)	
Epoch: [2][0/159],	Time 3.915 (3.915)	Data 3.264 (3.264)	siamese Loss 0.4312 (0.4312)	classifier Loss 0.1834 (0.1834)	
Epoch: [2][20/159],	Time 1.081 (1.034)	Data 0.000 (0.159)	siamese Loss 0.2622 (0.3107)	classifier Loss 0.1854 (0.1781)	
Epoch: [2][40/159],	Time 1.095 (0.962)	Data 0.003 (0.082)	siamese Loss 0.3190 (0.3099)	classifier Loss 0.1753 (0.1763)	
Epoch: [2][60/159],	Time 1.083 (0.937)	Data 0.003 (0.056)	siamese Loss 0.3046 (0.3089)	classifier Loss 0.1685 (0.1754)	
Epoch: [2][80/159],	Time 1.092 (0.924)	Data 0.002 (0.043)	siamese Loss 0.3269 (0.3085)	classifier Loss 0.1756 (0.1752)	
Epoch: [2][100/159],	Time 1.091 (0.916)	Data 0.002 (0.035)	siamese Loss 0.4243 (0.3121)	classifier Loss 0.1790 (0.1752)	
Epoch: [2][120/159],	Time 1.109 (0.911)	Data 0.002 (0.029)	siamese Loss 0.3040 (0.3107)	classifier Loss 0.1702 (0.1750)	
Epoch: [2][140/159],	Time 1.101 (0.908)	Data 0.004 (0.026)	siamese Loss 0.2968 (0.3116)	classifier Loss 0.1728 (0.1747)	
Epoch: [3][0/159],	Time 4.464 (4.464)	Data 3.783 (3.783)	siamese Loss 0.2886 (0.2886)	classifier Loss 0.1736 (0.1736)	
Epoch: [3][20/159],	Time 1.077 (1.055)	Data 0.003 (0.182)	siamese Loss 0.2946 (0.3034)	classifier Loss 0.1673 (0.1757)	
Epoch: [3][40/159],	Time 1.084 (0.972)	Data 0.002 (0.094)	siamese Loss 0.3045 (0.3058)	classifier Loss 0.1732 (0.1755)	
Epoch: [3][60/159],	Time 1.081 (0.942)	Data 0.003 (0.064)	siamese Loss 0.2995 (0.3057)	classifier Loss 0.1739 (0.1754)	
Epoch: [3][80/159],	Time 1.084 (0.928)	Data 0.002 (0.049)	siamese Loss 0.3007 (0.3028)	classifier Loss 0.1731 (0.1752)	
Epoch: [3][100/159],	Time 1.108 (0.919)	Data 0.002 (0.040)	siamese Loss 0.3327 (0.3046)	classifier Loss 0.1711 (0.1749)	
Epoch: [3][120/159],	Time 1.088 (0.913)	Data 0.002 (0.034)	siamese Loss 0.2871 (0.3012)	classifier Loss 0.1655 (0.1746)	
Epoch: [3][140/159],	Time 1.086 (0.909)	Data 0.002 (0.029)	siamese Loss 0.3165 (0.3028)	classifier Loss 0.1735 (0.1745)	
Epoch: [4][0/159],	Time 4.298 (4.298)	Data 3.610 (3.610)	siamese Loss 0.2971 (0.2971)	classifier Loss 0.1759 (0.1759)	
Epoch: [4][20/159],	Time 1.090 (1.048)	Data 0.004 (0.174)	siamese Loss 0.3174 (0.2966)	classifier Loss 0.1734 (0.1794)	
Epoch: [4][40/159],	Time 1.091 (0.968)	Data 0.002 (0.090)	siamese Loss 0.2689 (0.2956)	classifier Loss 0.1747 (0.1775)	
Epoch: [4][60/159],	Time 1.088 (0.942)	Data 0.003 (0.062)	siamese Loss 0.2982 (0.2938)	classifier Loss 0.1740 (0.1765)	
Epoch: [4][80/159],	Time 1.091 (0.929)	Data 0.003 (0.047)	siamese Loss 0.3287 (0.2937)	classifier Loss 0.1747 (0.1758)	
Epoch: [4][100/159],	Time 1.096 (0.921)	Data 0.003 (0.038)	siamese Loss 0.2383 (0.2939)	classifier Loss 0.1749 (0.1756)	
Epoch: [4][120/159],	Time 1.091 (0.915)	Data 0.003 (0.032)	siamese Loss 0.2887 (0.2955)	classifier Loss 0.1716 (0.1753)	
Epoch: [4][140/159],	Time 1.096 (0.912)	Data 0.002 (0.028)	siamese Loss 0.2965 (0.2974)	classifier Loss 0.1737 (0.1753)	
Epoch: [5][0/159],	Time 4.639 (4.639)	Data 3.973 (3.973)	siamese Loss 0.2142 (0.2142)	classifier Loss 0.1809 (0.1809)	
Epoch: [5][20/159],	Time 1.082 (1.068)	Data 0.003 (0.191)	siamese Loss 0.2822 (0.2871)	classifier Loss 0.1716 (0.1741)	
Epoch: [5][40/159],	Time 1.087 (0.978)	Data 0.003 (0.099)	siamese Loss 0.2924 (0.2876)	classifier Loss 0.1728 (0.1745)	
Epoch: [5][60/159],	Time 1.088 (0.947)	Data 0.003 (0.067)	siamese Loss 0.2709 (0.2915)	classifier Loss 0.1750 (0.1744)	
Epoch: [5][80/159],	Time 1.082 (0.932)	Data 0.003 (0.051)	siamese Loss 0.3875 (0.2942)	classifier Loss 0.1732 (0.1743)	
Epoch: [5][100/159],	Time 1.084 (0.922)	Data 0.002 (0.042)	siamese Loss 0.2074 (0.2941)	classifier Loss 0.1771 (0.1747)	
Epoch: [5][120/159],	Time 1.089 (0.917)	Data 0.002 (0.035)	siamese Loss 0.2909 (0.2940)	classifier Loss 0.1735 (0.1747)	
Epoch: [5][140/159],	Time 1.097 (0.912)	Data 0.002 (0.031)	siamese Loss 0.2707 (0.2950)	classifier Loss 0.1734 (0.1749)	
Validate: [0/16]	Time 4.751 (4.751)	siamese loss 1.0134 (1.0134)	clf loss 0.6695 (0.6695)	
Validating Results: siamese Loss 1.15424, classification loss 0.69588, Accuracy: 49.000%
Epoch: [6][0/159],	Time 4.549 (4.549)	Data 3.863 (3.863)	siamese Loss 0.2717 (0.2717)	classifier Loss 0.1803 (0.1803)	
Epoch: [6][20/159],	Time 1.083 (1.066)	Data 0.003 (0.187)	siamese Loss 0.3006 (0.2840)	classifier Loss 0.1721 (0.1754)	
Epoch: [6][40/159],	Time 1.123 (0.982)	Data 0.003 (0.097)	siamese Loss 0.2979 (0.2850)	classifier Loss 0.1773 (0.1749)	
Epoch: [6][60/159],	Time 1.087 (0.950)	Data 0.002 (0.066)	siamese Loss 0.2388 (0.2879)	classifier Loss 0.1905 (0.1759)	
Epoch: [6][80/159],	Time 1.099 (0.935)	Data 0.003 (0.050)	siamese Loss 0.3906 (0.2975)	classifier Loss 0.2031 (0.1763)	
Epoch: [6][100/159],	Time 1.070 (0.924)	Data 0.003 (0.041)	siamese Loss 0.3432 (0.2993)	classifier Loss 0.1713 (0.1758)	
Epoch: [6][120/159],	Time 1.085 (0.918)	Data 0.002 (0.035)	siamese Loss 0.3574 (0.3019)	classifier Loss 0.1764 (0.1758)	
Epoch: [6][140/159],	Time 1.090 (0.913)	Data 0.002 (0.030)	siamese Loss 0.3063 (0.3012)	classifier Loss 0.1784 (0.1757)	
Epoch: [7][0/159],	Time 4.457 (4.457)	Data 3.767 (3.767)	siamese Loss 0.3458 (0.3458)	classifier Loss 0.1882 (0.1882)	
Epoch: [7][20/159],	Time 1.081 (1.059)	Data 0.002 (0.181)	siamese Loss 0.2289 (0.3033)	classifier Loss 0.1827 (0.1807)	
Epoch: [7][40/159],	Time 1.081 (0.972)	Data 0.002 (0.094)	siamese Loss 0.2702 (0.3096)	classifier Loss 0.1639 (0.1795)	
Epoch: [7][60/159],	Time 1.085 (0.943)	Data 0.003 (0.064)	siamese Loss 0.3169 (0.3073)	classifier Loss 0.1634 (0.1784)	
Epoch: [7][80/159],	Time 1.093 (0.929)	Data 0.002 (0.049)	siamese Loss 0.2671 (0.3017)	classifier Loss 0.1751 (0.1782)	
Epoch: [7][100/159],	Time 1.084 (0.920)	Data 0.003 (0.040)	siamese Loss 0.2966 (0.3005)	classifier Loss 0.1717 (0.1773)	
Epoch: [7][120/159],	Time 1.097 (0.915)	Data 0.002 (0.033)	siamese Loss 0.2875 (0.3002)	classifier Loss 0.1703 (0.1767)	
Epoch: [7][140/159],	Time 1.088 (0.911)	Data 0.002 (0.029)	siamese Loss 0.2816 (0.2991)	classifier Loss 0.1748 (0.1762)	
Epoch: [8][0/159],	Time 4.365 (4.365)	Data 3.684 (3.684)	siamese Loss 0.3375 (0.3375)	classifier Loss 0.1763 (0.1763)	
Epoch: [8][20/159],	Time 1.082 (1.051)	Data 0.000 (0.178)	siamese Loss 0.2947 (0.2863)	classifier Loss 0.1745 (0.1744)	
Epoch: [8][40/159],	Time 1.089 (0.970)	Data 0.002 (0.092)	siamese Loss 0.2775 (0.2812)	classifier Loss 0.1755 (0.1746)	
Epoch: [8][60/159],	Time 1.088 (0.941)	Data 0.002 (0.063)	siamese Loss 0.2963 (0.2841)	classifier Loss 0.1737 (0.1745)	
Epoch: [8][80/159],	Time 1.088 (0.928)	Data 0.003 (0.048)	siamese Loss 0.3264 (0.2873)	classifier Loss 0.1724 (0.1742)	
Epoch: [8][100/159],	Time 1.091 (0.919)	Data 0.002 (0.039)	siamese Loss 0.3623 (0.2856)	classifier Loss 0.1732 (0.1742)	
Epoch: [8][120/159],	Time 1.098 (0.913)	Data 0.002 (0.033)	siamese Loss 0.2439 (0.2859)	classifier Loss 0.1745 (0.1742)	
Epoch: [8][140/159],	Time 1.099 (0.909)	Data 0.003 (0.028)	siamese Loss 0.2958 (0.2879)	classifier Loss 0.1726 (0.1742)	
Epoch: [9][0/159],	Time 4.184 (4.184)	Data 3.516 (3.516)	siamese Loss 0.3325 (0.3325)	classifier Loss 0.1723 (0.1723)	
Epoch: [9][20/159],	Time 1.085 (1.040)	Data 0.000 (0.170)	siamese Loss 0.2635 (0.3002)	classifier Loss 0.1743 (0.1738)	
Epoch: [9][40/159],	Time 1.090 (0.964)	Data 0.003 (0.088)	siamese Loss 0.3168 (0.3003)	classifier Loss 0.1755 (0.1740)	
Epoch: [9][60/159],	Time 1.088 (0.939)	Data 0.003 (0.060)	siamese Loss 0.3355 (0.3023)	classifier Loss 0.1717 (0.1739)	
Epoch: [9][80/159],	Time 1.088 (0.925)	Data 0.002 (0.046)	siamese Loss 0.3107 (0.2968)	classifier Loss 0.1734 (0.1738)	
Epoch: [9][100/159],	Time 1.083 (0.917)	Data 0.004 (0.037)	siamese Loss 0.2650 (0.2953)	classifier Loss 0.1710 (0.1737)	
Epoch: [9][120/159],	Time 1.084 (0.911)	Data 0.002 (0.031)	siamese Loss 0.2792 (0.2932)	classifier Loss 0.1737 (0.1738)	
Epoch: [9][140/159],	Time 1.082 (0.907)	Data 0.002 (0.027)	siamese Loss 0.3148 (0.2924)	classifier Loss 0.1804 (0.1739)	
Epoch: [10][0/159],	Time 4.171 (4.171)	Data 3.477 (3.477)	siamese Loss 0.2828 (0.2828)	classifier Loss 0.1774 (0.1774)	
Epoch: [10][20/159],	Time 1.078 (1.044)	Data 0.003 (0.168)	siamese Loss 0.3102 (0.2888)	classifier Loss 0.1730 (0.1745)	
Epoch: [10][40/159],	Time 1.086 (0.967)	Data 0.002 (0.087)	siamese Loss 0.3657 (0.2876)	classifier Loss 0.1756 (0.1745)	
Epoch: [10][60/159],	Time 1.096 (0.940)	Data 0.004 (0.059)	siamese Loss 0.2739 (0.2864)	classifier Loss 0.1723 (0.1741)	
Epoch: [10][80/159],	Time 1.100 (0.927)	Data 0.002 (0.045)	siamese Loss 0.2770 (0.2858)	classifier Loss 0.1734 (0.1740)	
Epoch: [10][100/159],	Time 1.085 (0.919)	Data 0.002 (0.037)	siamese Loss 0.2604 (0.2873)	classifier Loss 0.1746 (0.1739)	
Epoch: [10][120/159],	Time 1.084 (0.913)	Data 0.003 (0.031)	siamese Loss 0.2723 (0.2879)	classifier Loss 0.1732 (0.1738)	
Epoch: [10][140/159],	Time 1.095 (0.910)	Data 0.003 (0.027)	siamese Loss 0.2848 (0.2891)	classifier Loss 0.1738 (0.1737)	
Validate: [0/16]	Time 3.987 (3.987)	siamese loss 1.0586 (1.0586)	clf loss 0.7125 (0.7125)	
Validating Results: siamese Loss 1.13805, classification loss 0.69684, Accuracy: 50.000%
Epoch: [11][0/159],	Time 4.195 (4.195)	Data 3.520 (3.520)	siamese Loss 0.3183 (0.3183)	classifier Loss 0.1755 (0.1755)	
Epoch: [11][20/159],	Time 1.087 (1.044)	Data 0.000 (0.170)	siamese Loss 0.2448 (0.2939)	classifier Loss 0.1774 (0.1742)	
Epoch: [11][40/159],	Time 1.075 (0.964)	Data 0.002 (0.088)	siamese Loss 0.2553 (0.2834)	classifier Loss 0.1748 (0.1740)	
Epoch: [11][60/159],	Time 1.078 (0.937)	Data 0.002 (0.060)	siamese Loss 0.2925 (0.2836)	classifier Loss 0.1742 (0.1741)	
Epoch: [11][80/159],	Time 1.094 (0.924)	Data 0.002 (0.046)	siamese Loss 0.2438 (0.2835)	classifier Loss 0.1734 (0.1742)	
Epoch: [11][100/159],	Time 1.095 (0.918)	Data 0.002 (0.037)	siamese Loss 0.3379 (0.2863)	classifier Loss 0.1751 (0.1741)	
Epoch: [11][120/159],	Time 1.096 (0.913)	Data 0.002 (0.031)	siamese Loss 0.3047 (0.2839)	classifier Loss 0.1732 (0.1740)	
Epoch: [11][140/159],	Time 1.075 (0.909)	Data 0.002 (0.027)	siamese Loss 0.2775 (0.2828)	classifier Loss 0.1720 (0.1739)	
Epoch: [12][0/159],	Time 4.631 (4.631)	Data 3.936 (3.936)	siamese Loss 0.2543 (0.2543)	classifier Loss 0.1771 (0.1771)	
Epoch: [12][20/159],	Time 1.093 (1.072)	Data 0.002 (0.189)	siamese Loss 0.3193 (0.2978)	classifier Loss 0.1741 (0.1757)	
Epoch: [12][40/159],	Time 1.089 (0.979)	Data 0.002 (0.098)	siamese Loss 0.2814 (0.3001)	classifier Loss 0.1704 (0.1752)	
Epoch: [12][60/159],	Time 1.082 (0.948)	Data 0.003 (0.067)	siamese Loss 0.2626 (0.2977)	classifier Loss 0.1712 (0.1746)	
Epoch: [12][80/159],	Time 1.091 (0.933)	Data 0.002 (0.051)	siamese Loss 0.2844 (0.2975)	classifier Loss 0.1802 (0.1744)	
Epoch: [12][100/159],	Time 1.093 (0.923)	Data 0.003 (0.041)	siamese Loss 0.2675 (0.2952)	classifier Loss 0.1735 (0.1741)	
Epoch: [12][120/159],	Time 1.102 (0.917)	Data 0.002 (0.035)	siamese Loss 0.3090 (0.2930)	classifier Loss 0.1736 (0.1741)	
Epoch: [12][140/159],	Time 1.085 (0.913)	Data 0.003 (0.030)	siamese Loss 0.2975 (0.2931)	classifier Loss 0.1704 (0.1739)	
Epoch: [13][0/159],	Time 4.490 (4.490)	Data 3.795 (3.795)	siamese Loss 0.3079 (0.3079)	classifier Loss 0.1747 (0.1747)	
Epoch: [13][20/159],	Time 1.087 (1.065)	Data 0.003 (0.183)	siamese Loss 0.3503 (0.3165)	classifier Loss 0.1737 (0.1737)	
Epoch: [13][40/159],	Time 1.081 (0.978)	Data 0.003 (0.095)	siamese Loss 0.3589 (0.3168)	classifier Loss 0.1732 (0.1734)	
Epoch: [13][60/159],	Time 1.088 (0.949)	Data 0.003 (0.064)	siamese Loss 0.2306 (0.3065)	classifier Loss 0.1728 (0.1734)	
Epoch: [13][80/159],	Time 1.091 (0.933)	Data 0.005 (0.049)	siamese Loss 0.3084 (0.3076)	classifier Loss 0.1753 (0.1741)	
Epoch: [13][100/159],	Time 1.091 (0.924)	Data 0.003 (0.040)	siamese Loss 0.3049 (0.3048)	classifier Loss 0.1705 (0.1742)	
Epoch: [13][120/159],	Time 1.098 (0.918)	Data 0.002 (0.034)	siamese Loss 0.2773 (0.3015)	classifier Loss 0.1684 (0.1746)	
Epoch: [13][140/159],	Time 1.086 (0.913)	Data 0.002 (0.029)	siamese Loss 0.2505 (0.2994)	classifier Loss 0.1762 (0.1747)	
Epoch: [14][0/159],	Time 4.196 (4.196)	Data 3.520 (3.520)	siamese Loss 0.3006 (0.3006)	classifier Loss 0.1788 (0.1788)	
Epoch: [14][20/159],	Time 1.086 (1.045)	Data 0.002 (0.170)	siamese Loss 0.3013 (0.2938)	classifier Loss 0.1740 (0.1738)	
Epoch: [14][40/159],	Time 1.083 (0.967)	Data 0.002 (0.088)	siamese Loss 0.2759 (0.2839)	classifier Loss 0.1787 (0.1737)	
Epoch: [14][60/159],	Time 1.094 (0.941)	Data 0.002 (0.060)	siamese Loss 0.2665 (0.2840)	classifier Loss 0.1729 (0.1736)	
Epoch: [14][80/159],	Time 1.088 (0.927)	Data 0.002 (0.046)	siamese Loss 0.2540 (0.2814)	classifier Loss 0.1734 (0.1738)	
Epoch: [14][100/159],	Time 1.078 (0.919)	Data 0.002 (0.037)	siamese Loss 0.3147 (0.2817)	classifier Loss 0.1769 (0.1739)	
Epoch: [14][120/159],	Time 1.078 (0.914)	Data 0.003 (0.031)	siamese Loss 0.3049 (0.2809)	classifier Loss 0.1772 (0.1739)	
Epoch: [14][140/159],	Time 1.089 (0.910)	Data 0.002 (0.027)	siamese Loss 0.2979 (0.2819)	classifier Loss 0.1736 (0.1739)	
Epoch: [15][0/159],	Time 4.328 (4.328)	Data 3.656 (3.656)	siamese Loss 0.2966 (0.2966)	classifier Loss 0.1723 (0.1723)	
Epoch: [15][20/159],	Time 1.088 (1.050)	Data 0.000 (0.177)	siamese Loss 0.3060 (0.2972)	classifier Loss 0.1731 (0.1730)	
Epoch: [15][40/159],	Time 1.082 (0.969)	Data 0.003 (0.092)	siamese Loss 0.3307 (0.2911)	classifier Loss 0.1735 (0.1736)	
Epoch: [15][60/159],	Time 1.090 (0.941)	Data 0.002 (0.062)	siamese Loss 0.2878 (0.2894)	classifier Loss 0.1732 (0.1735)	
Epoch: [15][80/159],	Time 1.110 (0.928)	Data 0.002 (0.047)	siamese Loss 0.2836 (0.2911)	classifier Loss 0.1729 (0.1734)	
Epoch: [15][100/159],	Time 1.087 (0.920)	Data 0.003 (0.039)	siamese Loss 0.2935 (0.2884)	classifier Loss 0.1729 (0.1734)	
Epoch: [15][120/159],	Time 1.092 (0.915)	Data 0.003 (0.033)	siamese Loss 0.2534 (0.2873)	classifier Loss 0.1732 (0.1734)	
Epoch: [15][140/159],	Time 1.091 (0.911)	Data 0.002 (0.028)	siamese Loss 0.3115 (0.2870)	classifier Loss 0.1728 (0.1734)	
Validate: [0/16]	Time 4.064 (4.064)	siamese loss 0.9858 (0.9858)	clf loss 0.6919 (0.6919)	
Validating Results: siamese Loss 1.20523, classification loss 0.69304, Accuracy: 50.000%
Epoch: [16][0/159],	Time 3.974 (3.974)	Data 3.306 (3.306)	siamese Loss 0.3096 (0.3096)	classifier Loss 0.1731 (0.1731)	
Epoch: [16][20/159],	Time 1.086 (1.034)	Data 0.002 (0.160)	siamese Loss 0.2882 (0.2815)	classifier Loss 0.1715 (0.1729)	
Epoch: [16][40/159],	Time 1.099 (0.959)	Data 0.002 (0.083)	siamese Loss 0.2475 (0.2901)	classifier Loss 0.1716 (0.1733)	
Epoch: [16][60/159],	Time 1.087 (0.933)	Data 0.002 (0.056)	siamese Loss 0.3848 (0.2934)	classifier Loss 0.1737 (0.1736)	
Epoch: [16][80/159],	Time 1.090 (0.921)	Data 0.002 (0.043)	siamese Loss 0.2564 (0.2914)	classifier Loss 0.1741 (0.1736)	
Epoch: [16][100/159],	Time 1.093 (0.915)	Data 0.003 (0.035)	siamese Loss 0.3338 (0.2927)	classifier Loss 0.1730 (0.1736)	
Epoch: [16][120/159],	Time 1.088 (0.910)	Data 0.003 (0.029)	siamese Loss 0.2916 (0.2924)	classifier Loss 0.1738 (0.1735)	
Epoch: [16][140/159],	Time 1.085 (0.906)	Data 0.003 (0.026)	siamese Loss 0.3074 (0.2932)	classifier Loss 0.1733 (0.1735)	
Epoch: [17][0/159],	Time 4.264 (4.264)	Data 3.552 (3.552)	siamese Loss 0.3012 (0.3012)	classifier Loss 0.1736 (0.1736)	
Epoch: [17][20/159],	Time 1.088 (1.049)	Data 0.002 (0.171)	siamese Loss 0.2967 (0.2746)	classifier Loss 0.1741 (0.1732)	
Epoch: [17][40/159],	Time 1.094 (0.973)	Data 0.003 (0.089)	siamese Loss 0.3072 (0.2801)	classifier Loss 0.1744 (0.1731)	
Epoch: [17][60/159],	Time 1.087 (0.945)	Data 0.009 (0.061)	siamese Loss 0.2802 (0.2816)	classifier Loss 0.1729 (0.1734)	
Epoch: [17][80/159],	Time 1.088 (0.930)	Data 0.003 (0.046)	siamese Loss 0.2483 (0.2785)	classifier Loss 0.1734 (0.1734)	
Epoch: [17][100/159],	Time 1.085 (0.921)	Data 0.002 (0.038)	siamese Loss 0.3121 (0.2798)	classifier Loss 0.1712 (0.1733)	
Epoch: [17][120/159],	Time 1.083 (0.915)	Data 0.003 (0.032)	siamese Loss 0.2759 (0.2812)	classifier Loss 0.1732 (0.1734)	
Epoch: [17][140/159],	Time 1.080 (0.911)	Data 0.003 (0.028)	siamese Loss 0.2533 (0.2822)	classifier Loss 0.1743 (0.1734)	
Epoch: [18][0/159],	Time 4.220 (4.220)	Data 3.559 (3.559)	siamese Loss 0.2785 (0.2785)	classifier Loss 0.1730 (0.1730)	
Epoch: [18][20/159],	Time 1.082 (1.056)	Data 0.002 (0.172)	siamese Loss 0.2588 (0.2729)	classifier Loss 0.1738 (0.1734)	
Epoch: [18][40/159],	Time 1.087 (0.972)	Data 0.002 (0.089)	siamese Loss 0.2762 (0.2726)	classifier Loss 0.1744 (0.1731)	
Epoch: [18][60/159],	Time 1.085 (0.943)	Data 0.002 (0.061)	siamese Loss 0.3292 (0.2774)	classifier Loss 0.1760 (0.1733)	
Epoch: [18][80/159],	Time 1.113 (0.929)	Data 0.003 (0.046)	siamese Loss 0.3239 (0.2801)	classifier Loss 0.1742 (0.1734)	
Epoch: [18][100/159],	Time 1.089 (0.921)	Data 0.003 (0.038)	siamese Loss 0.2495 (0.2790)	classifier Loss 0.1752 (0.1735)	
Epoch: [18][120/159],	Time 1.084 (0.917)	Data 0.003 (0.032)	siamese Loss 0.3430 (0.2814)	classifier Loss 0.1709 (0.1735)	
Epoch: [18][140/159],	Time 1.096 (0.913)	Data 0.003 (0.028)	siamese Loss 0.3443 (0.2826)	classifier Loss 0.1724 (0.1734)	
Epoch: [19][0/159],	Time 4.312 (4.312)	Data 3.640 (3.640)	siamese Loss 0.2749 (0.2749)	classifier Loss 0.1734 (0.1734)	
Epoch: [19][20/159],	Time 1.099 (1.058)	Data 0.000 (0.182)	siamese Loss 0.2819 (0.2856)	classifier Loss 0.1749 (0.1736)	
Epoch: [19][40/159],	Time 1.078 (0.972)	Data 0.002 (0.095)	siamese Loss 0.3220 (0.2870)	classifier Loss 0.1730 (0.1735)	
Epoch: [19][60/159],	Time 1.091 (0.942)	Data 0.002 (0.064)	siamese Loss 0.2456 (0.2860)	classifier Loss 0.1734 (0.1734)	
Epoch: [19][80/159],	Time 1.086 (0.927)	Data 0.003 (0.049)	siamese Loss 0.3044 (0.2858)	classifier Loss 0.1700 (0.1734)	
Epoch: [19][100/159],	Time 1.082 (0.918)	Data 0.003 (0.040)	siamese Loss 0.3480 (0.2884)	classifier Loss 0.1718 (0.1735)	
Epoch: [19][120/159],	Time 1.089 (0.912)	Data 0.002 (0.034)	siamese Loss 0.3012 (0.2857)	classifier Loss 0.1739 (0.1735)	
Epoch: [19][140/159],	Time 1.088 (0.907)	Data 0.002 (0.029)	siamese Loss 0.2480 (0.2860)	classifier Loss 0.1725 (0.1735)	
Epoch: [20][0/159],	Time 4.275 (4.275)	Data 3.604 (3.604)	siamese Loss 0.3055 (0.3055)	classifier Loss 0.1735 (0.1735)	
Epoch: [20][20/159],	Time 1.088 (1.051)	Data 0.002 (0.173)	siamese Loss 0.2862 (0.2863)	classifier Loss 0.1738 (0.1732)	
Epoch: [20][40/159],	Time 1.090 (0.968)	Data 0.003 (0.090)	siamese Loss 0.3048 (0.2885)	classifier Loss 0.1733 (0.1733)	
Epoch: [20][60/159],	Time 1.083 (0.941)	Data 0.002 (0.061)	siamese Loss 0.3802 (0.2936)	classifier Loss 0.1731 (0.1734)	
Epoch: [20][80/159],	Time 1.092 (0.928)	Data 0.003 (0.047)	siamese Loss 0.3012 (0.2912)	classifier Loss 0.1734 (0.1734)	
Epoch: [20][100/159],	Time 1.080 (0.920)	Data 0.002 (0.038)	siamese Loss 0.3179 (0.2929)	classifier Loss 0.1729 (0.1734)	
Epoch: [20][120/159],	Time 1.082 (0.914)	Data 0.012 (0.032)	siamese Loss 0.3159 (0.2931)	classifier Loss 0.1733 (0.1734)	
Epoch: [20][140/159],	Time 1.088 (0.910)	Data 0.003 (0.028)	siamese Loss 0.2666 (0.2935)	classifier Loss 0.1733 (0.1734)	
Validate: [0/16]	Time 4.457 (4.457)	siamese loss 1.2405 (1.2405)	clf loss 0.6945 (0.6945)	
Validating Results: siamese Loss 1.32582, classification loss 0.69352, Accuracy: 51.000%
Epoch: [21][0/159],	Time 4.383 (4.383)	Data 3.700 (3.700)	siamese Loss 0.3880 (0.3880)	classifier Loss 0.1735 (0.1735)	
Epoch: [21][20/159],	Time 1.118 (1.052)	Data 0.003 (0.178)	siamese Loss 0.3031 (0.2976)	classifier Loss 0.1729 (0.1734)	
Epoch: [21][40/159],	Time 1.098 (0.971)	Data 0.003 (0.093)	siamese Loss 0.3462 (0.2936)	classifier Loss 0.1726 (0.1734)	
Epoch: [21][60/159],	Time 1.106 (0.943)	Data 0.004 (0.063)	siamese Loss 0.2497 (0.2924)	classifier Loss 0.1730 (0.1734)	
Epoch: [21][80/159],	Time 1.093 (0.930)	Data 0.005 (0.048)	siamese Loss 0.3097 (0.2904)	classifier Loss 0.1732 (0.1733)	
Epoch: [21][100/159],	Time 1.100 (0.923)	Data 0.004 (0.039)	siamese Loss 0.2806 (0.2903)	classifier Loss 0.1736 (0.1734)	
Epoch: [21][120/159],	Time 1.097 (0.917)	Data 0.003 (0.033)	siamese Loss 0.2779 (0.2918)	classifier Loss 0.1729 (0.1734)	
Epoch: [21][140/159],	Time 1.096 (0.913)	Data 0.002 (0.029)	siamese Loss 0.2103 (0.2897)	classifier Loss 0.1751 (0.1734)	
Epoch: [22][0/159],	Time 4.571 (4.571)	Data 3.863 (3.863)	siamese Loss 0.2884 (0.2884)	classifier Loss 0.1717 (0.1717)	
Epoch: [22][20/159],	Time 1.089 (1.064)	Data 0.002 (0.186)	siamese Loss 0.2960 (0.2899)	classifier Loss 0.1776 (0.1738)	
Epoch: [22][40/159],	Time 1.085 (0.978)	Data 0.003 (0.097)	siamese Loss 0.2881 (0.2895)	classifier Loss 0.1735 (0.1738)	
Epoch: [22][60/159],	Time 1.090 (0.948)	Data 0.003 (0.066)	siamese Loss 0.2433 (0.2889)	classifier Loss 0.1710 (0.1737)	
Epoch: [22][80/159],	Time 1.082 (0.934)	Data 0.003 (0.050)	siamese Loss 0.2940 (0.2889)	classifier Loss 0.1734 (0.1737)	
Epoch: [22][100/159],	Time 1.088 (0.925)	Data 0.002 (0.041)	siamese Loss 0.2173 (0.2908)	classifier Loss 0.1728 (0.1737)	
Epoch: [22][120/159],	Time 1.084 (0.918)	Data 0.002 (0.034)	siamese Loss 0.2936 (0.2916)	classifier Loss 0.1741 (0.1736)	
Epoch: [22][140/159],	Time 1.087 (0.914)	Data 0.002 (0.030)	siamese Loss 0.2752 (0.2908)	classifier Loss 0.1745 (0.1736)	
Epoch: [23][0/159],	Time 4.531 (4.531)	Data 3.848 (3.848)	siamese Loss 0.2912 (0.2912)	classifier Loss 0.1745 (0.1745)	
Epoch: [23][20/159],	Time 1.086 (1.074)	Data 0.003 (0.186)	siamese Loss 0.2929 (0.2932)	classifier Loss 0.1731 (0.1734)	
Epoch: [23][40/159],	Time 1.113 (0.989)	Data 0.003 (0.096)	siamese Loss 0.2945 (0.2860)	classifier Loss 0.1732 (0.1733)	
Epoch: [23][60/159],	Time 1.089 (0.959)	Data 0.002 (0.066)	siamese Loss 0.2763 (0.2852)	classifier Loss 0.1731 (0.1733)	
Epoch: [23][80/159],	Time 1.120 (0.945)	Data 0.004 (0.050)	siamese Loss 0.2533 (0.2834)	classifier Loss 0.1729 (0.1733)	
Epoch: [23][100/159],	Time 1.113 (0.938)	Data 0.003 (0.041)	siamese Loss 0.2858 (0.2841)	classifier Loss 0.1739 (0.1734)	
Epoch: [23][120/159],	Time 1.120 (0.931)	Data 0.003 (0.035)	siamese Loss 0.2659 (0.2875)	classifier Loss 0.1787 (0.1735)	
Epoch: [23][140/159],	Time 1.098 (0.926)	Data 0.003 (0.030)	siamese Loss 0.2513 (0.2868)	classifier Loss 0.1744 (0.1736)	
Epoch: [24][0/159],	Time 4.674 (4.674)	Data 3.986 (3.986)	siamese Loss 0.2762 (0.2762)	classifier Loss 0.1731 (0.1731)	
Epoch: [24][20/159],	Time 1.084 (1.068)	Data 0.002 (0.194)	siamese Loss 0.2627 (0.2967)	classifier Loss 0.1744 (0.1734)	
Epoch: [24][40/159],	Time 1.097 (0.980)	Data 0.003 (0.101)	siamese Loss 0.3008 (0.2953)	classifier Loss 0.1725 (0.1733)	
Epoch: [24][60/159],	Time 1.093 (0.949)	Data 0.003 (0.069)	siamese Loss 0.2981 (0.2923)	classifier Loss 0.1722 (0.1734)	
Epoch: [24][80/159],	Time 1.085 (0.932)	Data 0.004 (0.052)	siamese Loss 0.2993 (0.2894)	classifier Loss 0.1732 (0.1735)	
Epoch: [24][100/159],	Time 1.084 (0.923)	Data 0.003 (0.042)	siamese Loss 0.2685 (0.2855)	classifier Loss 0.1711 (0.1734)	
Epoch: [24][120/159],	Time 1.088 (0.917)	Data 0.003 (0.036)	siamese Loss 0.2824 (0.2843)	classifier Loss 0.1728 (0.1735)	
Epoch: [24][140/159],	Time 1.095 (0.912)	Data 0.002 (0.031)	siamese Loss 0.2385 (0.2852)	classifier Loss 0.1734 (0.1736)	
Epoch: [25][0/159],	Time 4.800 (4.800)	Data 4.130 (4.130)	siamese Loss 0.2644 (0.2644)	classifier Loss 0.1735 (0.1735)	
Epoch: [25][20/159],	Time 1.093 (1.082)	Data 0.003 (0.198)	siamese Loss 0.2879 (0.2837)	classifier Loss 0.1741 (0.1734)	
Epoch: [25][40/159],	Time 1.084 (0.996)	Data 0.005 (0.103)	siamese Loss 0.3281 (0.2843)	classifier Loss 0.1737 (0.1735)	
Epoch: [25][60/159],	Time 1.083 (0.961)	Data 0.003 (0.070)	siamese Loss 0.2616 (0.2792)	classifier Loss 0.1722 (0.1734)	
Epoch: [25][80/159],	Time 1.083 (0.943)	Data 0.002 (0.054)	siamese Loss 0.2654 (0.2787)	classifier Loss 0.1728 (0.1736)	
Epoch: [25][100/159],	Time 1.095 (0.932)	Data 0.002 (0.043)	siamese Loss 0.3170 (0.2780)	classifier Loss 0.1737 (0.1735)	
Epoch: [25][120/159],	Time 1.093 (0.924)	Data 0.001 (0.037)	siamese Loss 0.2443 (0.2797)	classifier Loss 0.1730 (0.1735)	
Epoch: [25][140/159],	Time 1.094 (0.919)	Data 0.002 (0.032)	siamese Loss 0.2870 (0.2805)	classifier Loss 0.1733 (0.1734)	
Validate: [0/16]	Time 4.706 (4.706)	siamese loss 1.0923 (1.0923)	clf loss 0.6930 (0.6930)	
Validating Results: siamese Loss 1.17186, classification loss 0.69301, Accuracy: 50.000%
Epoch: [26][0/159],	Time 4.053 (4.053)	Data 3.387 (3.387)	siamese Loss 0.2626 (0.2626)	classifier Loss 0.1729 (0.1729)	
Epoch: [26][20/159],	Time 1.073 (1.041)	Data 0.009 (0.164)	siamese Loss 0.3170 (0.2774)	classifier Loss 0.1746 (0.1732)	
Epoch: [26][40/159],	Time 1.081 (0.962)	Data 0.002 (0.085)	siamese Loss 0.2829 (0.2776)	classifier Loss 0.1730 (0.1731)	
Epoch: [26][60/159],	Time 1.076 (0.935)	Data 0.002 (0.058)	siamese Loss 0.3182 (0.2737)	classifier Loss 0.1780 (0.1731)	
Epoch: [26][80/159],	Time 1.116 (0.922)	Data 0.002 (0.044)	siamese Loss 0.2665 (0.2745)	classifier Loss 0.1714 (0.1732)	
Epoch: [26][100/159],	Time 1.083 (0.915)	Data 0.003 (0.036)	siamese Loss 0.2754 (0.2769)	classifier Loss 0.1744 (0.1735)	
Epoch: [26][120/159],	Time 1.118 (0.914)	Data 0.002 (0.030)	siamese Loss 0.2639 (0.2774)	classifier Loss 0.1742 (0.1734)	
Epoch: [26][140/159],	Time 1.083 (0.911)	Data 0.002 (0.027)	siamese Loss 0.3067 (0.2779)	classifier Loss 0.1703 (0.1734)	
Epoch: [27][0/159],	Time 4.106 (4.106)	Data 3.420 (3.420)	siamese Loss 0.3120 (0.3120)	classifier Loss 0.1740 (0.1740)	
Epoch: [27][20/159],	Time 1.073 (1.044)	Data 0.002 (0.165)	siamese Loss 0.3148 (0.2887)	classifier Loss 0.1702 (0.1733)	
Epoch: [27][40/159],	Time 1.087 (0.966)	Data 0.003 (0.086)	siamese Loss 0.2827 (0.2847)	classifier Loss 0.1726 (0.1734)	
Epoch: [27][60/159],	Time 1.086 (0.939)	Data 0.002 (0.059)	siamese Loss 0.3093 (0.2831)	classifier Loss 0.1734 (0.1734)	
Epoch: [27][80/159],	Time 1.087 (0.926)	Data 0.002 (0.045)	siamese Loss 0.3250 (0.2815)	classifier Loss 0.1739 (0.1734)	
Epoch: [27][100/159],	Time 1.077 (0.918)	Data 0.003 (0.036)	siamese Loss 0.3016 (0.2821)	classifier Loss 0.1741 (0.1734)	
Epoch: [27][120/159],	Time 1.088 (0.912)	Data 0.004 (0.031)	siamese Loss 0.2854 (0.2806)	classifier Loss 0.1750 (0.1733)	
Epoch: [27][140/159],	Time 1.092 (0.909)	Data 0.003 (0.027)	siamese Loss 0.2929 (0.2823)	classifier Loss 0.1747 (0.1734)	
Epoch: [28][0/159],	Time 4.754 (4.754)	Data 4.089 (4.089)	siamese Loss 0.3115 (0.3115)	classifier Loss 0.1733 (0.1733)	
Epoch: [28][20/159],	Time 1.091 (1.076)	Data 0.002 (0.196)	siamese Loss 0.3115 (0.2909)	classifier Loss 0.1737 (0.1733)	
Epoch: [28][40/159],	Time 1.099 (0.983)	Data 0.002 (0.102)	siamese Loss 0.2693 (0.2914)	classifier Loss 0.1737 (0.1733)	
Epoch: [28][60/159],	Time 1.085 (0.950)	Data 0.003 (0.069)	siamese Loss 0.3522 (0.2943)	classifier Loss 0.1732 (0.1734)	
Epoch: [28][80/159],	Time 1.092 (0.934)	Data 0.002 (0.053)	siamese Loss 0.2271 (0.2879)	classifier Loss 0.1737 (0.1734)	
Epoch: [28][100/159],	Time 1.092 (0.925)	Data 0.003 (0.043)	siamese Loss 0.3859 (0.2863)	classifier Loss 0.1757 (0.1734)	
Epoch: [28][120/159],	Time 1.095 (0.918)	Data 0.002 (0.036)	siamese Loss 0.3246 (0.2856)	classifier Loss 0.1781 (0.1735)	
Epoch: [28][140/159],	Time 1.084 (0.913)	Data 0.002 (0.031)	siamese Loss 0.3234 (0.2856)	classifier Loss 0.1728 (0.1735)	
Epoch: [29][0/159],	Time 4.647 (4.647)	Data 3.963 (3.963)	siamese Loss 0.2197 (0.2197)	classifier Loss 0.1733 (0.1733)	
Epoch: [29][20/159],	Time 1.086 (1.067)	Data 0.002 (0.190)	siamese Loss 0.2265 (0.2781)	classifier Loss 0.1769 (0.1742)	
Epoch: [29][40/159],	Time 1.087 (0.977)	Data 0.002 (0.099)	siamese Loss 0.3181 (0.2842)	classifier Loss 0.1748 (0.1740)	
Epoch: [29][60/159],	Time 1.088 (0.946)	Data 0.002 (0.067)	siamese Loss 0.2409 (0.2815)	classifier Loss 0.1727 (0.1736)	
Epoch: [29][80/159],	Time 1.094 (0.931)	Data 0.002 (0.051)	siamese Loss 0.2491 (0.2821)	classifier Loss 0.1726 (0.1738)	
Epoch: [29][100/159],	Time 1.090 (0.922)	Data 0.002 (0.041)	siamese Loss 0.3020 (0.2825)	classifier Loss 0.1759 (0.1736)	
Epoch: [29][120/159],	Time 1.100 (0.916)	Data 0.002 (0.035)	siamese Loss 0.2541 (0.2813)	classifier Loss 0.1736 (0.1736)	
Epoch: [29][140/159],	Time 1.095 (0.913)	Data 0.003 (0.030)	siamese Loss 0.2836 (0.2831)	classifier Loss 0.1735 (0.1736)	
Epoch: [30][0/159],	Time 4.389 (4.389)	Data 3.699 (3.699)	siamese Loss 0.2944 (0.2944)	classifier Loss 0.1755 (0.1755)	
Epoch: [30][20/159],	Time 1.087 (1.055)	Data 0.002 (0.181)	siamese Loss 0.3840 (0.2912)	classifier Loss 0.1681 (0.1732)	
Epoch: [30][40/159],	Time 1.091 (0.972)	Data 0.003 (0.094)	siamese Loss 0.2941 (0.2942)	classifier Loss 0.1784 (0.1730)	
Epoch: [30][60/159],	Time 1.092 (0.943)	Data 0.002 (0.064)	siamese Loss 0.3013 (0.2909)	classifier Loss 0.1788 (0.1737)	
Epoch: [30][80/159],	Time 1.093 (0.929)	Data 0.003 (0.049)	siamese Loss 0.3271 (0.2894)	classifier Loss 0.1732 (0.1736)	
Epoch: [30][100/159],	Time 1.093 (0.920)	Data 0.003 (0.039)	siamese Loss 0.2673 (0.2881)	classifier Loss 0.1693 (0.1735)	
Epoch: [30][120/159],	Time 1.087 (0.914)	Data 0.002 (0.033)	siamese Loss 0.2784 (0.2898)	classifier Loss 0.1741 (0.1737)	
Epoch: [30][140/159],	Time 1.088 (0.911)	Data 0.002 (0.029)	siamese Loss 0.2448 (0.2899)	classifier Loss 0.1706 (0.1737)	
Validate: [0/16]	Time 3.931 (3.931)	siamese loss 1.0733 (1.0733)	clf loss 0.6912 (0.6912)	
Validating Results: siamese Loss 1.20476, classification loss 0.69301, Accuracy: 49.000%
Epoch: [31][0/159],	Time 3.985 (3.985)	Data 3.331 (3.331)	siamese Loss 0.3076 (0.3076)	classifier Loss 0.1731 (0.1731)	
Epoch: [31][20/159],	Time 1.084 (1.037)	Data 0.003 (0.162)	siamese Loss 0.2867 (0.2906)	classifier Loss 0.1746 (0.1735)	
Epoch: [31][40/159],	Time 1.093 (0.963)	Data 0.002 (0.084)	siamese Loss 0.2456 (0.2824)	classifier Loss 0.1747 (0.1735)	
Epoch: [31][60/159],	Time 1.091 (0.939)	Data 0.002 (0.057)	siamese Loss 0.2783 (0.2844)	classifier Loss 0.1720 (0.1734)	
Epoch: [31][80/159],	Time 1.075 (0.926)	Data 0.003 (0.044)	siamese Loss 0.3571 (0.2862)	classifier Loss 0.1721 (0.1734)	
Epoch: [31][100/159],	Time 1.098 (0.919)	Data 0.002 (0.036)	siamese Loss 0.3056 (0.2866)	classifier Loss 0.1735 (0.1734)	
Epoch: [31][120/159],	Time 1.083 (0.913)	Data 0.002 (0.030)	siamese Loss 0.2728 (0.2858)	classifier Loss 0.1738 (0.1734)	
Epoch: [31][140/159],	Time 1.095 (0.910)	Data 0.002 (0.026)	siamese Loss 0.3286 (0.2856)	classifier Loss 0.1741 (0.1734)	
Epoch: [32][0/159],	Time 4.128 (4.128)	Data 3.439 (3.439)	siamese Loss 0.3264 (0.3264)	classifier Loss 0.1731 (0.1731)	
Epoch: [32][20/159],	Time 1.075 (1.043)	Data 0.002 (0.166)	siamese Loss 0.2701 (0.2823)	classifier Loss 0.1727 (0.1734)	
Epoch: [32][40/159],	Time 1.086 (0.966)	Data 0.003 (0.086)	siamese Loss 0.2778 (0.2842)	classifier Loss 0.1732 (0.1734)	
Epoch: [32][60/159],	Time 1.090 (0.940)	Data 0.002 (0.059)	siamese Loss 0.2707 (0.2851)	classifier Loss 0.1738 (0.1733)	
Epoch: [32][80/159],	Time 1.090 (0.927)	Data 0.003 (0.045)	siamese Loss 0.2404 (0.2832)	classifier Loss 0.1737 (0.1733)	
Epoch: [32][100/159],	Time 1.087 (0.919)	Data 0.003 (0.036)	siamese Loss 0.2426 (0.2823)	classifier Loss 0.1747 (0.1733)	
Epoch: [32][120/159],	Time 1.094 (0.914)	Data 0.002 (0.031)	siamese Loss 0.2388 (0.2828)	classifier Loss 0.1740 (0.1734)	
Epoch: [32][140/159],	Time 1.088 (0.910)	Data 0.004 (0.027)	siamese Loss 0.2929 (0.2835)	classifier Loss 0.1724 (0.1734)	
Epoch: [33][0/159],	Time 4.518 (4.518)	Data 3.858 (3.858)	siamese Loss 0.2503 (0.2503)	classifier Loss 0.1731 (0.1731)	
Epoch: [33][20/159],	Time 1.097 (1.064)	Data 0.002 (0.185)	siamese Loss 0.3139 (0.2937)	classifier Loss 0.1784 (0.1739)	
Epoch: [33][40/159],	Time 1.108 (0.978)	Data 0.003 (0.096)	siamese Loss 0.2299 (0.2848)	classifier Loss 0.1720 (0.1736)	
Epoch: [33][60/159],	Time 1.087 (0.948)	Data 0.002 (0.065)	siamese Loss 0.2921 (0.2855)	classifier Loss 0.1733 (0.1734)	
Epoch: [33][80/159],	Time 1.088 (0.932)	Data 0.002 (0.050)	siamese Loss 0.2481 (0.2845)	classifier Loss 0.1744 (0.1734)	
Epoch: [33][100/159],	Time 1.089 (0.923)	Data 0.003 (0.040)	siamese Loss 0.2653 (0.2824)	classifier Loss 0.1740 (0.1734)	
Epoch: [33][120/159],	Time 1.084 (0.916)	Data 0.002 (0.034)	siamese Loss 0.3065 (0.2836)	classifier Loss 0.1723 (0.1734)	
Epoch: [33][140/159],	Time 1.095 (0.912)	Data 0.003 (0.029)	siamese Loss 0.2882 (0.2843)	classifier Loss 0.1741 (0.1734)	
Epoch: [34][0/159],	Time 4.521 (4.521)	Data 3.833 (3.833)	siamese Loss 0.3174 (0.3174)	classifier Loss 0.1731 (0.1731)	
Epoch: [34][20/159],	Time 1.090 (1.074)	Data 0.002 (0.185)	siamese Loss 0.3211 (0.2999)	classifier Loss 0.1738 (0.1732)	
Epoch: [34][40/159],	Time 1.093 (0.982)	Data 0.002 (0.096)	siamese Loss 0.3155 (0.2908)	classifier Loss 0.1727 (0.1734)	
Epoch: [34][60/159],	Time 1.095 (0.951)	Data 0.002 (0.065)	siamese Loss 0.2359 (0.2821)	classifier Loss 0.1734 (0.1733)	
Epoch: [34][80/159],	Time 1.094 (0.935)	Data 0.002 (0.050)	siamese Loss 0.2989 (0.2818)	classifier Loss 0.1754 (0.1734)	
Epoch: [34][100/159],	Time 1.093 (0.926)	Data 0.002 (0.040)	siamese Loss 0.2212 (0.2826)	classifier Loss 0.1712 (0.1734)	
Epoch: [34][120/159],	Time 1.104 (0.921)	Data 0.002 (0.034)	siamese Loss 0.2776 (0.2825)	classifier Loss 0.1742 (0.1735)	
Epoch: [34][140/159],	Time 1.103 (0.916)	Data 0.003 (0.030)	siamese Loss 0.2385 (0.2822)	classifier Loss 0.1731 (0.1735)	
Epoch: [35][0/159],	Time 4.311 (4.311)	Data 3.643 (3.643)	siamese Loss 0.2887 (0.2887)	classifier Loss 0.1724 (0.1724)	
Epoch: [35][20/159],	Time 1.080 (1.052)	Data 0.000 (0.176)	siamese Loss 0.2299 (0.2802)	classifier Loss 0.1753 (0.1739)	
Epoch: [35][40/159],	Time 1.092 (0.970)	Data 0.002 (0.091)	siamese Loss 0.3234 (0.2880)	classifier Loss 0.1735 (0.1736)	
Epoch: [35][60/159],	Time 1.083 (0.941)	Data 0.002 (0.062)	siamese Loss 0.2451 (0.2900)	classifier Loss 0.1730 (0.1735)	
Epoch: [35][80/159],	Time 1.082 (0.926)	Data 0.002 (0.047)	siamese Loss 0.2328 (0.2889)	classifier Loss 0.1727 (0.1734)	
Epoch: [35][100/159],	Time 1.090 (0.918)	Data 0.002 (0.038)	siamese Loss 0.2248 (0.2866)	classifier Loss 0.1729 (0.1734)	
Epoch: [35][120/159],	Time 1.091 (0.912)	Data 0.003 (0.032)	siamese Loss 0.2531 (0.2859)	classifier Loss 0.1730 (0.1734)	
Epoch: [35][140/159],	Time 1.092 (0.908)	Data 0.002 (0.028)	siamese Loss 0.3015 (0.2839)	classifier Loss 0.1733 (0.1734)	
Validate: [0/16]	Time 4.244 (4.244)	siamese loss 0.8727 (0.8727)	clf loss 0.6858 (0.6858)	
Validating Results: siamese Loss 1.15376, classification loss 0.69264, Accuracy: 49.000%
Epoch     7: reducing learning rate of group 0 to 2.0000e-03.
Epoch     7: reducing learning rate of group 1 to 2.0000e-03.
Epoch     7: reducing learning rate of group 2 to 2.0000e-03.
Epoch     7: reducing learning rate of group 3 to 2.0000e-03.
Epoch     7: reducing learning rate of group 4 to 2.0000e-03.
Epoch     7: reducing learning rate of group 5 to 2.0000e-03.
Epoch     7: reducing learning rate of group 6 to 2.0000e-03.
Epoch     7: reducing learning rate of group 7 to 2.0000e-03.
Epoch     7: reducing learning rate of group 8 to 2.0000e-03.
Epoch     7: reducing learning rate of group 9 to 2.0000e-03.
Epoch     7: reducing learning rate of group 10 to 2.0000e-03.
Epoch     7: reducing learning rate of group 11 to 2.0000e-03.
Epoch     7: reducing learning rate of group 12 to 2.0000e-03.
Epoch     7: reducing learning rate of group 13 to 2.0000e-03.
Epoch     7: reducing learning rate of group 14 to 2.0000e-03.
Epoch     7: reducing learning rate of group 15 to 2.0000e-03.
Epoch     7: reducing learning rate of group 16 to 2.0000e-03.
Epoch     7: reducing learning rate of group 17 to 2.0000e-03.
Epoch     7: reducing learning rate of group 18 to 2.0000e-03.
Epoch     7: reducing learning rate of group 19 to 2.0000e-03.
Epoch     7: reducing learning rate of group 20 to 2.0000e-03.
Epoch     7: reducing learning rate of group 21 to 2.0000e-03.
Epoch     7: reducing learning rate of group 22 to 2.0000e-03.
Epoch     7: reducing learning rate of group 23 to 2.0000e-03.
Epoch     7: reducing learning rate of group 24 to 2.0000e-03.
Epoch     7: reducing learning rate of group 25 to 2.0000e-03.
Epoch     7: reducing learning rate of group 26 to 2.0000e-03.
Epoch     7: reducing learning rate of group 27 to 2.0000e-03.
Epoch     7: reducing learning rate of group 28 to 2.0000e-03.
Epoch     7: reducing learning rate of group 29 to 2.0000e-03.
Epoch     7: reducing learning rate of group 30 to 2.0000e-03.
Epoch     7: reducing learning rate of group 31 to 2.0000e-03.
Epoch     7: reducing learning rate of group 32 to 2.0000e-03.
Epoch     7: reducing learning rate of group 33 to 2.0000e-03.
Epoch     7: reducing learning rate of group 34 to 2.0000e-03.
Epoch     7: reducing learning rate of group 35 to 2.0000e-03.
Epoch     7: reducing learning rate of group 36 to 2.0000e-03.
Epoch     7: reducing learning rate of group 37 to 2.0000e-03.
Epoch     7: reducing learning rate of group 38 to 2.0000e-03.
Epoch     7: reducing learning rate of group 39 to 2.0000e-03.
Epoch     7: reducing learning rate of group 40 to 2.0000e-03.
Epoch     7: reducing learning rate of group 41 to 2.0000e-03.
Epoch     7: reducing learning rate of group 42 to 2.0000e-03.
Epoch     7: reducing learning rate of group 43 to 2.0000e-03.
Epoch     7: reducing learning rate of group 44 to 2.0000e-03.
Epoch     7: reducing learning rate of group 45 to 2.0000e-03.
Epoch     7: reducing learning rate of group 46 to 2.0000e-03.
Epoch     7: reducing learning rate of group 47 to 2.0000e-03.
Epoch     7: reducing learning rate of group 48 to 2.0000e-03.
Epoch     7: reducing learning rate of group 49 to 2.0000e-03.
Epoch     7: reducing learning rate of group 50 to 2.0000e-03.
Epoch     7: reducing learning rate of group 51 to 2.0000e-03.
Epoch     7: reducing learning rate of group 52 to 2.0000e-03.
Epoch     7: reducing learning rate of group 53 to 2.0000e-03.
Epoch     7: reducing learning rate of group 54 to 2.0000e-03.
Epoch     7: reducing learning rate of group 55 to 2.0000e-03.
Epoch     7: reducing learning rate of group 56 to 2.0000e-03.
Epoch     7: reducing learning rate of group 57 to 2.0000e-03.
Epoch     7: reducing learning rate of group 58 to 2.0000e-03.
Epoch     7: reducing learning rate of group 59 to 2.0000e-03.
Epoch     7: reducing learning rate of group 60 to 2.0000e-03.
Epoch     7: reducing learning rate of group 61 to 2.0000e-03.
Epoch     7: reducing learning rate of group 62 to 2.0000e-03.
Epoch     7: reducing learning rate of group 63 to 2.0000e-03.
Epoch     7: reducing learning rate of group 64 to 2.0000e-03.
Epoch     7: reducing learning rate of group 65 to 2.0000e-03.
Epoch     7: reducing learning rate of group 66 to 2.0000e-03.
Epoch     7: reducing learning rate of group 67 to 2.0000e-03.
Epoch     7: reducing learning rate of group 68 to 2.0000e-03.
Epoch     7: reducing learning rate of group 69 to 2.0000e-03.
Epoch     7: reducing learning rate of group 70 to 2.0000e-03.
Epoch     7: reducing learning rate of group 71 to 2.0000e-03.
Epoch: [36][0/159],	Time 4.605 (4.605)	Data 3.889 (3.889)	siamese Loss 0.2278 (0.2278)	classifier Loss 0.1734 (0.1734)	
Epoch: [36][20/159],	Time 1.094 (1.065)	Data 0.003 (0.188)	siamese Loss 0.3412 (0.2820)	classifier Loss 0.1737 (0.1732)	
Epoch: [36][40/159],	Time 1.090 (0.977)	Data 0.003 (0.097)	siamese Loss 0.2440 (0.2796)	classifier Loss 0.1728 (0.1733)	
Epoch: [36][60/159],	Time 1.073 (0.946)	Data 0.003 (0.066)	siamese Loss 0.2995 (0.2801)	classifier Loss 0.1728 (0.1732)	
Epoch: [36][80/159],	Time 1.106 (0.933)	Data 0.005 (0.051)	siamese Loss 0.2693 (0.2803)	classifier Loss 0.1729 (0.1732)	
Epoch: [36][100/159],	Time 1.086 (0.924)	Data 0.003 (0.041)	siamese Loss 0.2483 (0.2782)	classifier Loss 0.1729 (0.1732)	
Epoch: [36][120/159],	Time 1.088 (0.917)	Data 0.002 (0.035)	siamese Loss 0.2783 (0.2778)	classifier Loss 0.1733 (0.1733)	
Epoch: [36][140/159],	Time 1.103 (0.913)	Data 0.002 (0.030)	siamese Loss 0.2785 (0.2771)	classifier Loss 0.1732 (0.1733)	
Epoch: [37][0/159],	Time 3.991 (3.991)	Data 3.321 (3.321)	siamese Loss 0.2767 (0.2767)	classifier Loss 0.1734 (0.1734)	
Epoch: [37][20/159],	Time 1.092 (1.040)	Data 0.004 (0.161)	siamese Loss 0.2825 (0.2827)	classifier Loss 0.1737 (0.1733)	
Epoch: [37][40/159],	Time 1.085 (0.965)	Data 0.003 (0.084)	siamese Loss 0.2780 (0.2822)	classifier Loss 0.1727 (0.1733)	
Epoch: [37][60/159],	Time 1.081 (0.940)	Data 0.003 (0.057)	siamese Loss 0.2475 (0.2810)	classifier Loss 0.1735 (0.1733)	
Epoch: [37][80/159],	Time 1.093 (0.927)	Data 0.002 (0.044)	siamese Loss 0.3125 (0.2805)	classifier Loss 0.1732 (0.1733)	
Epoch: [37][100/159],	Time 1.082 (0.920)	Data 0.004 (0.035)	siamese Loss 0.2785 (0.2782)	classifier Loss 0.1735 (0.1733)	
Epoch: [37][120/159],	Time 1.124 (0.916)	Data 0.002 (0.030)	siamese Loss 0.3137 (0.2797)	classifier Loss 0.1735 (0.1733)	
Epoch: [37][140/159],	Time 1.096 (0.913)	Data 0.003 (0.026)	siamese Loss 0.2717 (0.2800)	classifier Loss 0.1729 (0.1733)	
Epoch: [38][0/159],	Time 5.917 (5.917)	Data 5.235 (5.235)	siamese Loss 0.2865 (0.2865)	classifier Loss 0.1737 (0.1737)	
Epoch: [38][20/159],	Time 1.084 (1.132)	Data 0.002 (0.251)	siamese Loss 0.2138 (0.2700)	classifier Loss 0.1738 (0.1734)	
Epoch: [38][40/159],	Time 1.087 (1.010)	Data 0.002 (0.130)	siamese Loss 0.2878 (0.2733)	classifier Loss 0.1734 (0.1733)	
Epoch: [38][60/159],	Time 1.086 (0.971)	Data 0.002 (0.088)	siamese Loss 0.2926 (0.2751)	classifier Loss 0.1738 (0.1733)	
Epoch: [38][80/159],	Time 1.081 (0.951)	Data 0.002 (0.067)	siamese Loss 0.3117 (0.2753)	classifier Loss 0.1733 (0.1733)	
Epoch: [38][100/159],	Time 1.083 (0.938)	Data 0.002 (0.054)	siamese Loss 0.2590 (0.2775)	classifier Loss 0.1728 (0.1733)	
Epoch: [38][120/159],	Time 1.086 (0.929)	Data 0.003 (0.046)	siamese Loss 0.2926 (0.2783)	classifier Loss 0.1731 (0.1733)	
Epoch: [38][140/159],	Time 1.081 (0.923)	Data 0.003 (0.040)	siamese Loss 0.2507 (0.2794)	classifier Loss 0.1732 (0.1733)	
Epoch: [39][0/159],	Time 4.148 (4.148)	Data 3.482 (3.482)	siamese Loss 0.2510 (0.2510)	classifier Loss 0.1731 (0.1731)	
Epoch: [39][20/159],	Time 1.088 (1.057)	Data 0.003 (0.171)	siamese Loss 0.2397 (0.2789)	classifier Loss 0.1731 (0.1734)	
Epoch: [39][40/159],	Time 1.094 (0.980)	Data 0.003 (0.089)	siamese Loss 0.2706 (0.2790)	classifier Loss 0.1725 (0.1733)	
Epoch: [39][60/159],	Time 1.120 (0.956)	Data 0.003 (0.061)	siamese Loss 0.2988 (0.2788)	classifier Loss 0.1732 (0.1733)	
Epoch: [39][80/159],	Time 1.074 (0.943)	Data 0.003 (0.047)	siamese Loss 0.2538 (0.2780)	classifier Loss 0.1753 (0.1733)	
Epoch: [39][100/159],	Time 1.091 (0.933)	Data 0.003 (0.038)	siamese Loss 0.2898 (0.2774)	classifier Loss 0.1733 (0.1733)	
Epoch: [39][120/159],	Time 1.087 (0.929)	Data 0.003 (0.032)	siamese Loss 0.2250 (0.2751)	classifier Loss 0.1741 (0.1733)	
Epoch: [39][140/159],	Time 1.093 (0.923)	Data 0.003 (0.028)	siamese Loss 0.3129 (0.2761)	classifier Loss 0.1736 (0.1733)	
Epoch: [40][0/159],	Time 4.627 (4.627)	Data 3.925 (3.925)	siamese Loss 0.2680 (0.2680)	classifier Loss 0.1727 (0.1727)	
Epoch: [40][20/159],	Time 1.119 (1.077)	Data 0.002 (0.189)	siamese Loss 0.2687 (0.2863)	classifier Loss 0.1740 (0.1732)	
Epoch: [40][40/159],	Time 1.092 (0.989)	Data 0.005 (0.098)	siamese Loss 0.2683 (0.2780)	classifier Loss 0.1734 (0.1733)	
Epoch: [40][60/159],	Time 1.114 (0.958)	Data 0.003 (0.067)	siamese Loss 0.2386 (0.2763)	classifier Loss 0.1730 (0.1733)	
Epoch: [40][80/159],	Time 1.119 (0.944)	Data 0.003 (0.051)	siamese Loss 0.2399 (0.2771)	classifier Loss 0.1735 (0.1733)	
Epoch: [40][100/159],	Time 1.124 (0.935)	Data 0.003 (0.041)	siamese Loss 0.2868 (0.2787)	classifier Loss 0.1735 (0.1733)	
Epoch: [40][120/159],	Time 1.084 (0.928)	Data 0.003 (0.035)	siamese Loss 0.2514 (0.2787)	classifier Loss 0.1737 (0.1733)	
Epoch: [40][140/159],	Time 1.114 (0.924)	Data 0.003 (0.031)	siamese Loss 0.2992 (0.2794)	classifier Loss 0.1733 (0.1733)	
Validate: [0/16]	Time 4.895 (4.895)	siamese loss 0.9191 (0.9191)	clf loss 0.6940 (0.6940)	
Validating Results: siamese Loss 1.17491, classification loss 0.69323, Accuracy: 50.000%
Epoch: [41][0/159],	Time 4.303 (4.303)	Data 3.627 (3.627)	siamese Loss 0.2431 (0.2431)	classifier Loss 0.1735 (0.1735)	
Epoch: [41][20/159],	Time 1.116 (1.066)	Data 0.002 (0.175)	siamese Loss 0.2845 (0.2803)	classifier Loss 0.1732 (0.1733)	
Epoch: [41][40/159],	Time 1.083 (0.982)	Data 0.005 (0.092)	siamese Loss 0.2254 (0.2738)	classifier Loss 0.1734 (0.1733)	
Epoch: [41][60/159],	Time 1.081 (0.952)	Data 0.003 (0.062)	siamese Loss 0.2725 (0.2759)	classifier Loss 0.1733 (0.1733)	
Epoch: [41][80/159],	Time 1.093 (0.936)	Data 0.002 (0.048)	siamese Loss 0.3162 (0.2771)	classifier Loss 0.1729 (0.1733)	
Epoch: [41][100/159],	Time 1.094 (0.926)	Data 0.003 (0.039)	siamese Loss 0.2847 (0.2784)	classifier Loss 0.1736 (0.1733)	
Epoch: [41][120/159],	Time 1.084 (0.921)	Data 0.003 (0.033)	siamese Loss 0.2928 (0.2780)	classifier Loss 0.1737 (0.1733)	
Epoch: [41][140/159],	Time 1.091 (0.917)	Data 0.002 (0.029)	siamese Loss 0.3312 (0.2775)	classifier Loss 0.1734 (0.1733)	
Epoch: [42][0/159],	Time 4.223 (4.223)	Data 3.552 (3.552)	siamese Loss 0.2575 (0.2575)	classifier Loss 0.1732 (0.1732)	
Epoch: [42][20/159],	Time 1.089 (1.059)	Data 0.003 (0.172)	siamese Loss 0.2562 (0.2724)	classifier Loss 0.1733 (0.1732)	
Epoch: [42][40/159],	Time 1.080 (0.972)	Data 0.002 (0.089)	siamese Loss 0.3006 (0.2824)	classifier Loss 0.1746 (0.1734)	
Epoch: [42][60/159],	Time 1.093 (0.945)	Data 0.002 (0.061)	siamese Loss 0.2805 (0.2823)	classifier Loss 0.1747 (0.1734)	
Epoch: [42][80/159],	Time 1.118 (0.932)	Data 0.003 (0.046)	siamese Loss 0.2988 (0.2863)	classifier Loss 0.1743 (0.1735)	
Epoch: [42][100/159],	Time 1.082 (0.923)	Data 0.002 (0.038)	siamese Loss 0.2622 (0.2840)	classifier Loss 0.1734 (0.1734)	
Epoch: [42][120/159],	Time 1.095 (0.917)	Data 0.002 (0.032)	siamese Loss 0.2834 (0.2840)	classifier Loss 0.1731 (0.1734)	
Epoch: [42][140/159],	Time 1.086 (0.912)	Data 0.002 (0.028)	siamese Loss 0.3184 (0.2824)	classifier Loss 0.1731 (0.1734)	
Epoch: [43][0/159],	Time 4.359 (4.359)	Data 3.686 (3.686)	siamese Loss 0.3082 (0.3082)	classifier Loss 0.1734 (0.1734)	
Epoch: [43][20/159],	Time 1.086 (1.056)	Data 0.003 (0.178)	siamese Loss 0.2617 (0.2862)	classifier Loss 0.1731 (0.1733)	
Epoch: [43][40/159],	Time 1.084 (0.972)	Data 0.004 (0.092)	siamese Loss 0.3255 (0.2882)	classifier Loss 0.1730 (0.1732)	
Epoch: [43][60/159],	Time 1.077 (0.943)	Data 0.002 (0.063)	siamese Loss 0.2489 (0.2856)	classifier Loss 0.1740 (0.1733)	
Epoch: [43][80/159],	Time 1.082 (0.928)	Data 0.002 (0.048)	siamese Loss 0.2983 (0.2834)	classifier Loss 0.1734 (0.1733)	
Epoch: [43][100/159],	Time 1.089 (0.920)	Data 0.003 (0.039)	siamese Loss 0.2279 (0.2827)	classifier Loss 0.1747 (0.1733)	
Epoch: [43][120/159],	Time 1.086 (0.914)	Data 0.003 (0.033)	siamese Loss 0.2663 (0.2827)	classifier Loss 0.1731 (0.1733)	
Epoch: [43][140/159],	Time 1.085 (0.909)	Data 0.003 (0.029)	siamese Loss 0.2848 (0.2821)	classifier Loss 0.1732 (0.1733)	
Epoch: [44][0/159],	Time 4.183 (4.183)	Data 3.515 (3.515)	siamese Loss 0.2745 (0.2745)	classifier Loss 0.1731 (0.1731)	
Epoch: [44][20/159],	Time 1.091 (1.052)	Data 0.002 (0.170)	siamese Loss 0.2899 (0.2687)	classifier Loss 0.1730 (0.1733)	
Epoch: [44][40/159],	Time 1.088 (0.972)	Data 0.003 (0.088)	siamese Loss 0.2864 (0.2753)	classifier Loss 0.1741 (0.1733)	
Epoch: [44][60/159],	Time 1.088 (0.943)	Data 0.002 (0.060)	siamese Loss 0.3187 (0.2741)	classifier Loss 0.1735 (0.1733)	
Epoch: [44][80/159],	Time 1.083 (0.928)	Data 0.002 (0.046)	siamese Loss 0.2604 (0.2770)	classifier Loss 0.1734 (0.1733)	
Epoch: [44][100/159],	Time 1.087 (0.919)	Data 0.003 (0.037)	siamese Loss 0.3082 (0.2756)	classifier Loss 0.1732 (0.1733)	
Epoch: [44][120/159],	Time 1.089 (0.913)	Data 0.002 (0.031)	siamese Loss 0.2518 (0.2741)	classifier Loss 0.1734 (0.1733)	
Epoch: [44][140/159],	Time 1.093 (0.909)	Data 0.002 (0.027)	siamese Loss 0.2828 (0.2760)	classifier Loss 0.1733 (0.1733)	
Epoch: [45][0/159],	Time 4.667 (4.667)	Data 3.971 (3.971)	siamese Loss 0.3210 (0.3210)	classifier Loss 0.1731 (0.1731)	
Epoch: [45][20/159],	Time 1.083 (1.071)	Data 0.002 (0.191)	siamese Loss 0.2648 (0.2798)	classifier Loss 0.1734 (0.1733)	
Epoch: [45][40/159],	Time 1.098 (0.981)	Data 0.002 (0.099)	siamese Loss 0.2755 (0.2799)	classifier Loss 0.1733 (0.1733)	
Epoch: [45][60/159],	Time 1.084 (0.950)	Data 0.003 (0.067)	siamese Loss 0.2741 (0.2770)	classifier Loss 0.1732 (0.1733)	
Epoch: [45][80/159],	Time 1.089 (0.935)	Data 0.002 (0.051)	siamese Loss 0.2628 (0.2759)	classifier Loss 0.1730 (0.1733)	
Epoch: [45][100/159],	Time 1.091 (0.926)	Data 0.002 (0.042)	siamese Loss 0.2742 (0.2756)	classifier Loss 0.1727 (0.1733)	
Epoch: [45][120/159],	Time 1.119 (0.922)	Data 0.004 (0.035)	siamese Loss 0.2464 (0.2769)	classifier Loss 0.1735 (0.1733)	
Epoch: [45][140/159],	Time 1.117 (0.919)	Data 0.004 (0.031)	siamese Loss 0.2644 (0.2764)	classifier Loss 0.1730 (0.1733)	
Validate: [0/16]	Time 4.318 (4.318)	siamese loss 0.8701 (0.8701)	clf loss 0.6928 (0.6928)	
Validating Results: siamese Loss 1.18086, classification loss 0.69312, Accuracy: 49.000%
Epoch: [46][0/159],	Time 4.602 (4.602)	Data 3.927 (3.927)	siamese Loss 0.2651 (0.2651)	classifier Loss 0.1735 (0.1735)	
Epoch: [46][20/159],	Time 1.103 (1.080)	Data 0.003 (0.189)	siamese Loss 0.3067 (0.2908)	classifier Loss 0.1739 (0.1734)	
Epoch: [46][40/159],	Time 1.084 (0.982)	Data 0.004 (0.098)	siamese Loss 0.2273 (0.2938)	classifier Loss 0.1729 (0.1734)	
Epoch: [46][60/159],	Time 1.083 (0.948)	Data 0.003 (0.067)	siamese Loss 0.3010 (0.2915)	classifier Loss 0.1734 (0.1733)	
Epoch: [46][80/159],	Time 1.077 (0.931)	Data 0.003 (0.051)	siamese Loss 0.2769 (0.2871)	classifier Loss 0.1733 (0.1733)	
Epoch: [46][100/159],	Time 1.090 (0.922)	Data 0.002 (0.041)	siamese Loss 0.2908 (0.2847)	classifier Loss 0.1731 (0.1733)	
Epoch: [46][120/159],	Time 1.081 (0.915)	Data 0.003 (0.035)	siamese Loss 0.2378 (0.2841)	classifier Loss 0.1731 (0.1733)	
Epoch: [46][140/159],	Time 1.091 (0.911)	Data 0.003 (0.030)	siamese Loss 0.3212 (0.2853)	classifier Loss 0.1731 (0.1733)	
Epoch: [47][0/159],	Time 4.286 (4.286)	Data 3.601 (3.601)	siamese Loss 0.2748 (0.2748)	classifier Loss 0.1733 (0.1733)	
Epoch: [47][20/159],	Time 1.080 (1.069)	Data 0.004 (0.173)	siamese Loss 0.2714 (0.2735)	classifier Loss 0.1732 (0.1734)	
Epoch: [47][40/159],	Time 1.079 (0.988)	Data 0.003 (0.090)	siamese Loss 0.3249 (0.2741)	classifier Loss 0.1732 (0.1733)	
Epoch: [47][60/159],	Time 1.086 (0.961)	Data 0.002 (0.062)	siamese Loss 0.2638 (0.2730)	classifier Loss 0.1738 (0.1733)	
Epoch: [47][80/159],	Time 1.108 (0.947)	Data 0.004 (0.047)	siamese Loss 0.2246 (0.2725)	classifier Loss 0.1739 (0.1734)	
Epoch: [47][100/159],	Time 1.085 (0.938)	Data 0.003 (0.038)	siamese Loss 0.2368 (0.2735)	classifier Loss 0.1734 (0.1734)	
Epoch: [47][120/159],	Time 1.112 (0.932)	Data 0.003 (0.033)	siamese Loss 0.2812 (0.2742)	classifier Loss 0.1736 (0.1734)	
Epoch: [47][140/159],	Time 1.125 (0.928)	Data 0.002 (0.029)	siamese Loss 0.2324 (0.2742)	classifier Loss 0.1729 (0.1733)	
Epoch: [48][0/159],	Time 3.965 (3.965)	Data 3.306 (3.306)	siamese Loss 0.2762 (0.2762)	classifier Loss 0.1725 (0.1725)	
Epoch: [48][20/159],	Time 1.074 (1.048)	Data 0.003 (0.160)	siamese Loss 0.2306 (0.2646)	classifier Loss 0.1736 (0.1732)	
Epoch: [48][40/159],	Time 1.089 (0.973)	Data 0.003 (0.083)	siamese Loss 0.2614 (0.2736)	classifier Loss 0.1725 (0.1732)	
Epoch: [48][60/159],	Time 1.133 (0.949)	Data 0.003 (0.057)	siamese Loss 0.2608 (0.2760)	classifier Loss 0.1742 (0.1733)	
Epoch: [48][80/159],	Time 1.105 (0.937)	Data 0.003 (0.044)	siamese Loss 0.2285 (0.2777)	classifier Loss 0.1732 (0.1733)	
Epoch: [48][100/159],	Time 1.124 (0.931)	Data 0.003 (0.035)	siamese Loss 0.3150 (0.2784)	classifier Loss 0.1737 (0.1734)	
Epoch: [48][120/159],	Time 1.116 (0.928)	Data 0.002 (0.030)	siamese Loss 0.2523 (0.2776)	classifier Loss 0.1735 (0.1734)	
Epoch: [48][140/159],	Time 1.111 (0.925)	Data 0.003 (0.026)	siamese Loss 0.3264 (0.2773)	classifier Loss 0.1730 (0.1734)	
Epoch: [49][0/159],	Time 4.447 (4.447)	Data 3.772 (3.772)	siamese Loss 0.2960 (0.2960)	classifier Loss 0.1734 (0.1734)	
Epoch: [49][20/159],	Time 1.112 (1.070)	Data 0.003 (0.182)	siamese Loss 0.2844 (0.2810)	classifier Loss 0.1732 (0.1733)	
Epoch: [49][40/159],	Time 1.098 (0.988)	Data 0.003 (0.095)	siamese Loss 0.3064 (0.2868)	classifier Loss 0.1734 (0.1733)	
Epoch: [49][60/159],	Time 1.108 (0.958)	Data 0.003 (0.065)	siamese Loss 0.2787 (0.2841)	classifier Loss 0.1733 (0.1733)	
Epoch: [49][80/159],	Time 1.083 (0.943)	Data 0.003 (0.049)	siamese Loss 0.3160 (0.2853)	classifier Loss 0.1732 (0.1733)	
Epoch: [49][100/159],	Time 1.122 (0.935)	Data 0.003 (0.040)	siamese Loss 0.2410 (0.2844)	classifier Loss 0.1735 (0.1733)	
Epoch: [49][120/159],	Time 1.117 (0.930)	Data 0.004 (0.034)	siamese Loss 0.3298 (0.2836)	classifier Loss 0.1732 (0.1733)	
Epoch: [49][140/159],	Time 1.080 (0.925)	Data 0.004 (0.030)	siamese Loss 0.2548 (0.2808)	classifier Loss 0.1734 (0.1733)	
Epoch: [50][0/159],	Time 4.477 (4.477)	Data 3.792 (3.792)	siamese Loss 0.2609 (0.2609)	classifier Loss 0.1733 (0.1733)	
Epoch: [50][20/159],	Time 1.151 (1.076)	Data 0.003 (0.183)	siamese Loss 0.3005 (0.2799)	classifier Loss 0.1734 (0.1734)	
Epoch: [50][40/159],	Time 1.121 (0.988)	Data 0.003 (0.095)	siamese Loss 0.2376 (0.2713)	classifier Loss 0.1738 (0.1733)	
Epoch: [50][60/159],	Time 1.090 (0.963)	Data 0.003 (0.065)	siamese Loss 0.2850 (0.2762)	classifier Loss 0.1734 (0.1734)	
Epoch: [50][80/159],	Time 1.115 (0.949)	Data 0.004 (0.050)	siamese Loss 0.2759 (0.2763)	classifier Loss 0.1734 (0.1734)	
Epoch: [50][100/159],	Time 1.120 (0.941)	Data 0.002 (0.040)	siamese Loss 0.2818 (0.2783)	classifier Loss 0.1737 (0.1734)	
Epoch: [50][120/159],	Time 1.078 (0.935)	Data 0.003 (0.034)	siamese Loss 0.2174 (0.2777)	classifier Loss 0.1732 (0.1734)	
Epoch: [50][140/159],	Time 1.109 (0.930)	Data 0.003 (0.030)	siamese Loss 0.2930 (0.2774)	classifier Loss 0.1733 (0.1734)	
Validate: [0/16]	Time 3.939 (3.939)	siamese loss 0.9044 (0.9044)	clf loss 0.6932 (0.6932)	
Validating Results: siamese Loss 1.18048, classification loss 0.69315, Accuracy: 49.000%
Epoch: [51][0/159],	Time 4.453 (4.453)	Data 3.778 (3.778)	siamese Loss 0.3138 (0.3138)	classifier Loss 0.1733 (0.1733)	
Epoch: [51][20/159],	Time 1.078 (1.072)	Data 0.003 (0.187)	siamese Loss 0.3150 (0.2791)	classifier Loss 0.1719 (0.1732)	
Epoch: [51][40/159],	Time 1.082 (0.986)	Data 0.003 (0.097)	siamese Loss 0.2858 (0.2797)	classifier Loss 0.1771 (0.1734)	
Epoch: [51][60/159],	Time 1.106 (0.958)	Data 0.003 (0.066)	siamese Loss 0.2732 (0.2800)	classifier Loss 0.1718 (0.1734)	
Epoch: [51][80/159],	Time 1.119 (0.948)	Data 0.003 (0.051)	siamese Loss 0.2726 (0.2778)	classifier Loss 0.1749 (0.1735)	
Epoch: [51][100/159],	Time 1.114 (0.939)	Data 0.002 (0.041)	siamese Loss 0.2921 (0.2787)	classifier Loss 0.1741 (0.1735)	
Epoch: [51][120/159],	Time 1.130 (0.934)	Data 0.003 (0.035)	siamese Loss 0.2543 (0.2784)	classifier Loss 0.1729 (0.1734)	
Epoch: [51][140/159],	Time 1.106 (0.931)	Data 0.002 (0.030)	siamese Loss 0.2576 (0.2792)	classifier Loss 0.1731 (0.1734)	
Epoch: [52][0/159],	Time 4.177 (4.177)	Data 3.512 (3.512)	siamese Loss 0.3118 (0.3118)	classifier Loss 0.1734 (0.1734)	
Epoch: [52][20/159],	Time 1.076 (1.062)	Data 0.003 (0.170)	siamese Loss 0.3126 (0.2781)	classifier Loss 0.1735 (0.1734)	
Epoch: [52][40/159],	Time 1.120 (0.983)	Data 0.003 (0.088)	siamese Loss 0.2854 (0.2837)	classifier Loss 0.1731 (0.1733)	
Epoch: [52][60/159],	Time 1.092 (0.951)	Data 0.003 (0.060)	siamese Loss 0.2041 (0.2790)	classifier Loss 0.1739 (0.1733)	
Epoch: [52][80/159],	Time 1.110 (0.937)	Data 0.002 (0.046)	siamese Loss 0.2895 (0.2791)	classifier Loss 0.1734 (0.1733)	
Epoch: [52][100/159],	Time 1.084 (0.930)	Data 0.004 (0.038)	siamese Loss 0.3208 (0.2804)	classifier Loss 0.1733 (0.1733)	
Epoch: [52][120/159],	Time 1.121 (0.924)	Data 0.003 (0.032)	siamese Loss 0.2755 (0.2805)	classifier Loss 0.1736 (0.1733)	
Epoch: [52][140/159],	Time 1.082 (0.919)	Data 0.003 (0.028)	siamese Loss 0.2317 (0.2799)	classifier Loss 0.1733 (0.1733)	
Epoch: [53][0/159],	Time 4.606 (4.606)	Data 3.918 (3.918)	siamese Loss 0.2860 (0.2860)	classifier Loss 0.1733 (0.1733)	
Epoch: [53][20/159],	Time 1.110 (1.072)	Data 0.003 (0.188)	siamese Loss 0.2751 (0.2768)	classifier Loss 0.1727 (0.1734)	
Epoch: [53][40/159],	Time 1.081 (0.980)	Data 0.002 (0.098)	siamese Loss 0.3122 (0.2800)	classifier Loss 0.1743 (0.1734)	
Epoch: [53][60/159],	Time 1.111 (0.954)	Data 0.003 (0.067)	siamese Loss 0.2427 (0.2808)	classifier Loss 0.1723 (0.1734)	
Epoch: [53][80/159],	Time 1.114 (0.943)	Data 0.003 (0.051)	siamese Loss 0.2752 (0.2804)	classifier Loss 0.1722 (0.1733)	
Epoch: [53][100/159],	Time 1.122 (0.936)	Data 0.003 (0.042)	siamese Loss 0.3094 (0.2836)	classifier Loss 0.1726 (0.1734)	
Epoch: [53][120/159],	Time 1.120 (0.930)	Data 0.003 (0.035)	siamese Loss 0.3107 (0.2855)	classifier Loss 0.1734 (0.1734)	
Epoch: [53][140/159],	Time 1.095 (0.925)	Data 0.003 (0.031)	siamese Loss 0.2948 (0.2848)	classifier Loss 0.1727 (0.1734)	
Epoch: [54][0/159],	Time 4.498 (4.498)	Data 3.808 (3.808)	siamese Loss 0.2970 (0.2970)	classifier Loss 0.1736 (0.1736)	
Epoch: [54][20/159],	Time 1.113 (1.067)	Data 0.003 (0.183)	siamese Loss 0.2810 (0.2819)	classifier Loss 0.1744 (0.1734)	
Epoch: [54][40/159],	Time 1.082 (0.984)	Data 0.003 (0.095)	siamese Loss 0.2943 (0.2855)	classifier Loss 0.1735 (0.1732)	
Epoch: [54][60/159],	Time 1.112 (0.955)	Data 0.002 (0.065)	siamese Loss 0.2920 (0.2842)	classifier Loss 0.1736 (0.1734)	
Epoch: [54][80/159],	Time 1.090 (0.940)	Data 0.002 (0.049)	siamese Loss 0.3102 (0.2828)	classifier Loss 0.1732 (0.1734)	
Epoch: [54][100/159],	Time 1.088 (0.931)	Data 0.004 (0.040)	siamese Loss 0.2929 (0.2829)	classifier Loss 0.1732 (0.1734)	
Epoch: [54][120/159],	Time 1.091 (0.924)	Data 0.003 (0.034)	siamese Loss 0.2222 (0.2797)	classifier Loss 0.1736 (0.1734)	
Epoch: [54][140/159],	Time 1.102 (0.920)	Data 0.002 (0.029)	siamese Loss 0.2910 (0.2784)	classifier Loss 0.1733 (0.1734)	
Epoch: [55][0/159],	Time 4.903 (4.903)	Data 4.227 (4.227)	siamese Loss 0.2942 (0.2942)	classifier Loss 0.1730 (0.1730)	
Epoch: [55][20/159],	Time 1.099 (1.079)	Data 0.002 (0.205)	siamese Loss 0.2979 (0.2811)	classifier Loss 0.1733 (0.1734)	
Epoch: [55][40/159],	Time 1.112 (0.989)	Data 0.002 (0.106)	siamese Loss 0.2727 (0.2816)	classifier Loss 0.1731 (0.1734)	
Epoch: [55][60/159],	Time 1.113 (0.957)	Data 0.002 (0.072)	siamese Loss 0.2496 (0.2805)	classifier Loss 0.1735 (0.1734)	
Epoch: [55][80/159],	Time 1.074 (0.939)	Data 0.002 (0.055)	siamese Loss 0.2727 (0.2791)	classifier Loss 0.1733 (0.1733)	
Epoch: [55][100/159],	Time 1.086 (0.928)	Data 0.003 (0.045)	siamese Loss 0.2605 (0.2785)	classifier Loss 0.1734 (0.1733)	
Epoch: [55][120/159],	Time 1.077 (0.921)	Data 0.003 (0.038)	siamese Loss 0.2712 (0.2789)	classifier Loss 0.1736 (0.1733)	
Epoch: [55][140/159],	Time 1.083 (0.916)	Data 0.003 (0.033)	siamese Loss 0.2577 (0.2788)	classifier Loss 0.1715 (0.1733)	
Validate: [0/16]	Time 4.161 (4.161)	siamese loss 0.9103 (0.9103)	clf loss 0.6876 (0.6876)	
Validating Results: siamese Loss 1.15138, classification loss 0.69270, Accuracy: 49.000%
Epoch: [56][0/159],	Time 4.122 (4.122)	Data 3.456 (3.456)	siamese Loss 0.3306 (0.3306)	classifier Loss 0.1735 (0.1735)	
Epoch: [56][20/159],	Time 1.069 (1.035)	Data 0.003 (0.171)	siamese Loss 0.2650 (0.2765)	classifier Loss 0.1743 (0.1731)	
Epoch: [56][40/159],	Time 1.080 (0.959)	Data 0.003 (0.089)	siamese Loss 0.2771 (0.2768)	classifier Loss 0.1749 (0.1736)	
Epoch: [56][60/159],	Time 1.087 (0.933)	Data 0.003 (0.061)	siamese Loss 0.3383 (0.2791)	classifier Loss 0.1734 (0.1735)	
Epoch: [56][80/159],	Time 1.085 (0.923)	Data 0.002 (0.046)	siamese Loss 0.2734 (0.2787)	classifier Loss 0.1732 (0.1734)	
Epoch: [56][100/159],	Time 1.101 (0.917)	Data 0.002 (0.038)	siamese Loss 0.2316 (0.2773)	classifier Loss 0.1731 (0.1734)	
Epoch: [56][120/159],	Time 1.094 (0.913)	Data 0.003 (0.032)	siamese Loss 0.2512 (0.2785)	classifier Loss 0.1732 (0.1734)	
Epoch: [56][140/159],	Time 1.095 (0.909)	Data 0.003 (0.028)	siamese Loss 0.2603 (0.2794)	classifier Loss 0.1732 (0.1734)	
Epoch: [57][0/159],	Time 4.514 (4.514)	Data 3.828 (3.828)	siamese Loss 0.2984 (0.2984)	classifier Loss 0.1731 (0.1731)	
Epoch: [57][20/159],	Time 1.076 (1.073)	Data 0.003 (0.184)	siamese Loss 0.2855 (0.2852)	classifier Loss 0.1734 (0.1734)	
Epoch: [57][40/159],	Time 1.120 (0.984)	Data 0.003 (0.096)	siamese Loss 0.2650 (0.2868)	classifier Loss 0.1729 (0.1734)	
Epoch: [57][60/159],	Time 1.125 (0.953)	Data 0.003 (0.065)	siamese Loss 0.2637 (0.2824)	classifier Loss 0.1729 (0.1733)	
Epoch: [57][80/159],	Time 1.092 (0.938)	Data 0.003 (0.050)	siamese Loss 0.2720 (0.2807)	classifier Loss 0.1735 (0.1733)	
Epoch: [57][100/159],	Time 1.090 (0.929)	Data 0.003 (0.041)	siamese Loss 0.2884 (0.2800)	classifier Loss 0.1735 (0.1733)	
Epoch: [57][120/159],	Time 1.085 (0.922)	Data 0.003 (0.034)	siamese Loss 0.2271 (0.2812)	classifier Loss 0.1738 (0.1733)	
Epoch: [57][140/159],	Time 1.083 (0.917)	Data 0.002 (0.030)	siamese Loss 0.2625 (0.2814)	classifier Loss 0.1731 (0.1733)	
Epoch: [58][0/159],	Time 4.560 (4.560)	Data 3.893 (3.893)	siamese Loss 0.3012 (0.3012)	classifier Loss 0.1733 (0.1733)	
Epoch: [58][20/159],	Time 1.122 (1.086)	Data 0.003 (0.187)	siamese Loss 0.2862 (0.2747)	classifier Loss 0.1730 (0.1735)	
Epoch: [58][40/159],	Time 1.109 (1.001)	Data 0.003 (0.098)	siamese Loss 0.2961 (0.2807)	classifier Loss 0.1733 (0.1734)	
Epoch: [58][60/159],	Time 1.106 (0.972)	Data 0.003 (0.067)	siamese Loss 0.2401 (0.2814)	classifier Loss 0.1724 (0.1733)	
Epoch: [58][80/159],	Time 1.089 (0.951)	Data 0.003 (0.051)	siamese Loss 0.2553 (0.2812)	classifier Loss 0.1728 (0.1734)	
Epoch: [58][100/159],	Time 1.109 (0.943)	Data 0.003 (0.041)	siamese Loss 0.2392 (0.2776)	classifier Loss 0.1732 (0.1733)	
Epoch: [58][120/159],	Time 1.114 (0.936)	Data 0.002 (0.035)	siamese Loss 0.2390 (0.2790)	classifier Loss 0.1731 (0.1734)	
Epoch: [58][140/159],	Time 1.122 (0.932)	Data 0.002 (0.030)	siamese Loss 0.3095 (0.2793)	classifier Loss 0.1732 (0.1734)	
Epoch: [59][0/159],	Time 4.560 (4.560)	Data 3.894 (3.894)	siamese Loss 0.2648 (0.2648)	classifier Loss 0.1729 (0.1729)	
Epoch: [59][20/159],	Time 1.094 (1.071)	Data 0.002 (0.187)	siamese Loss 0.2556 (0.2806)	classifier Loss 0.1723 (0.1733)	
Epoch: [59][40/159],	Time 1.091 (0.979)	Data 0.002 (0.097)	siamese Loss 0.2898 (0.2795)	classifier Loss 0.1708 (0.1731)	
Epoch: [59][60/159],	Time 1.091 (0.950)	Data 0.002 (0.066)	siamese Loss 0.2746 (0.2780)	classifier Loss 0.1759 (0.1733)	
Epoch: [59][80/159],	Time 1.084 (0.935)	Data 0.002 (0.050)	siamese Loss 0.2116 (0.2744)	classifier Loss 0.1744 (0.1735)	
Epoch: [59][100/159],	Time 1.126 (0.926)	Data 0.003 (0.041)	siamese Loss 0.3162 (0.2734)	classifier Loss 0.1737 (0.1735)	
Epoch: [59][120/159],	Time 1.093 (0.921)	Data 0.003 (0.034)	siamese Loss 0.2515 (0.2737)	classifier Loss 0.1726 (0.1734)	
Epoch: [59][140/159],	Time 1.089 (0.917)	Data 0.002 (0.030)	siamese Loss 0.3029 (0.2749)	classifier Loss 0.1756 (0.1734)	
Epoch: [60][0/159],	Time 4.203 (4.203)	Data 3.491 (3.491)	siamese Loss 0.2360 (0.2360)	classifier Loss 0.1742 (0.1742)	
Epoch: [60][20/159],	Time 1.084 (1.056)	Data 0.003 (0.173)	siamese Loss 0.2900 (0.2794)	classifier Loss 0.1752 (0.1736)	
Epoch: [60][40/159],	Time 1.085 (0.975)	Data 0.004 (0.090)	siamese Loss 0.2845 (0.2770)	classifier Loss 0.1736 (0.1734)	
Epoch: [60][60/159],	Time 1.079 (0.945)	Data 0.003 (0.062)	siamese Loss 0.2621 (0.2777)	classifier Loss 0.1738 (0.1734)	
Epoch: [60][80/159],	Time 1.085 (0.930)	Data 0.004 (0.047)	siamese Loss 0.2684 (0.2755)	classifier Loss 0.1731 (0.1734)	
Epoch: [60][100/159],	Time 1.112 (0.923)	Data 0.003 (0.038)	siamese Loss 0.2663 (0.2776)	classifier Loss 0.1734 (0.1734)	
Epoch: [60][120/159],	Time 1.083 (0.916)	Data 0.003 (0.032)	siamese Loss 0.3601 (0.2795)	classifier Loss 0.1735 (0.1734)	
Epoch: [60][140/159],	Time 1.082 (0.912)	Data 0.004 (0.028)	siamese Loss 0.2544 (0.2790)	classifier Loss 0.1737 (0.1734)	
Validate: [0/16]	Time 4.615 (4.615)	siamese loss 0.9216 (0.9216)	clf loss 0.6967 (0.6967)	
Validating Results: siamese Loss 1.13679, classification loss 0.69357, Accuracy: 50.000%
Epoch: [61][0/159],	Time 4.001 (4.001)	Data 3.336 (3.336)	siamese Loss 0.2883 (0.2883)	classifier Loss 0.1729 (0.1729)	
Epoch: [61][20/159],	Time 1.082 (1.035)	Data 0.002 (0.161)	siamese Loss 0.2394 (0.2838)	classifier Loss 0.1734 (0.1732)	
Epoch: [61][40/159],	Time 1.082 (0.964)	Data 0.003 (0.084)	siamese Loss 0.2481 (0.2788)	classifier Loss 0.1736 (0.1732)	
Epoch: [61][60/159],	Time 1.093 (0.939)	Data 0.002 (0.057)	siamese Loss 0.2591 (0.2804)	classifier Loss 0.1734 (0.1733)	
Epoch: [61][80/159],	Time 1.087 (0.926)	Data 0.003 (0.044)	siamese Loss 0.2683 (0.2793)	classifier Loss 0.1727 (0.1733)	
Epoch: [61][100/159],	Time 1.091 (0.920)	Data 0.003 (0.036)	siamese Loss 0.2722 (0.2787)	classifier Loss 0.1733 (0.1733)	
Epoch: [61][120/159],	Time 1.089 (0.915)	Data 0.003 (0.030)	siamese Loss 0.3149 (0.2778)	classifier Loss 0.1736 (0.1733)	
Epoch: [61][140/159],	Time 1.088 (0.910)	Data 0.002 (0.026)	siamese Loss 0.2768 (0.2781)	classifier Loss 0.1739 (0.1733)	
Epoch: [62][0/159],	Time 4.569 (4.569)	Data 3.859 (3.859)	siamese Loss 0.2927 (0.2927)	classifier Loss 0.1735 (0.1735)	
Epoch: [62][20/159],	Time 1.127 (1.067)	Data 0.003 (0.187)	siamese Loss 0.2462 (0.2912)	classifier Loss 0.1731 (0.1733)	
Epoch: [62][40/159],	Time 1.091 (0.978)	Data 0.002 (0.097)	siamese Loss 0.3298 (0.2879)	classifier Loss 0.1738 (0.1734)	
Epoch: [62][60/159],	Time 1.079 (0.947)	Data 0.003 (0.066)	siamese Loss 0.2457 (0.2843)	classifier Loss 0.1732 (0.1733)	
Epoch: [62][80/159],	Time 1.090 (0.932)	Data 0.003 (0.050)	siamese Loss 0.2376 (0.2810)	classifier Loss 0.1725 (0.1733)	
Epoch: [62][100/159],	Time 1.090 (0.923)	Data 0.003 (0.041)	siamese Loss 0.1801 (0.2815)	classifier Loss 0.1692 (0.1732)	
Epoch: [62][120/159],	Time 1.081 (0.917)	Data 0.003 (0.035)	siamese Loss 0.3055 (0.2812)	classifier Loss 0.1749 (0.1733)	
Epoch: [62][140/159],	Time 1.095 (0.913)	Data 0.003 (0.030)	siamese Loss 0.3086 (0.2812)	classifier Loss 0.1742 (0.1734)	
Epoch: [63][0/159],	Time 4.315 (4.315)	Data 3.651 (3.651)	siamese Loss 0.2406 (0.2406)	classifier Loss 0.1733 (0.1733)	
Epoch: [63][20/159],	Time 1.088 (1.055)	Data 0.002 (0.176)	siamese Loss 0.3540 (0.2768)	classifier Loss 0.1727 (0.1733)	
Epoch: [63][40/159],	Time 1.088 (0.972)	Data 0.003 (0.091)	siamese Loss 0.2460 (0.2821)	classifier Loss 0.1743 (0.1733)	
Epoch: [63][60/159],	Time 1.088 (0.944)	Data 0.003 (0.062)	siamese Loss 0.2718 (0.2813)	classifier Loss 0.1737 (0.1733)	
Epoch: [63][80/159],	Time 1.095 (0.930)	Data 0.002 (0.048)	siamese Loss 0.2812 (0.2820)	classifier Loss 0.1733 (0.1733)	
Epoch: [63][100/159],	Time 1.094 (0.921)	Data 0.004 (0.039)	siamese Loss 0.2759 (0.2811)	classifier Loss 0.1732 (0.1733)	
Epoch: [63][120/159],	Time 1.087 (0.915)	Data 0.003 (0.033)	siamese Loss 0.2869 (0.2806)	classifier Loss 0.1732 (0.1733)	
Epoch: [63][140/159],	Time 1.112 (0.910)	Data 0.004 (0.028)	siamese Loss 0.2598 (0.2803)	classifier Loss 0.1733 (0.1733)	
Epoch: [64][0/159],	Time 4.421 (4.421)	Data 3.761 (3.761)	siamese Loss 0.2771 (0.2771)	classifier Loss 0.1734 (0.1734)	
Epoch: [64][20/159],	Time 1.077 (1.049)	Data 0.002 (0.181)	siamese Loss 0.2845 (0.2753)	classifier Loss 0.1742 (0.1733)	
Epoch: [64][40/159],	Time 1.081 (0.966)	Data 0.003 (0.094)	siamese Loss 0.3049 (0.2773)	classifier Loss 0.1728 (0.1733)	
Epoch: [64][60/159],	Time 1.085 (0.939)	Data 0.002 (0.064)	siamese Loss 0.2166 (0.2743)	classifier Loss 0.1723 (0.1733)	
Epoch: [64][80/159],	Time 1.082 (0.927)	Data 0.003 (0.049)	siamese Loss 0.2711 (0.2731)	classifier Loss 0.1722 (0.1733)	
Epoch: [64][100/159],	Time 1.111 (0.919)	Data 0.002 (0.039)	siamese Loss 0.2551 (0.2741)	classifier Loss 0.1732 (0.1733)	
Epoch: [64][120/159],	Time 1.093 (0.914)	Data 0.002 (0.033)	siamese Loss 0.2935 (0.2743)	classifier Loss 0.1739 (0.1733)	
Epoch: [64][140/159],	Time 1.086 (0.910)	Data 0.003 (0.029)	siamese Loss 0.3100 (0.2736)	classifier Loss 0.1730 (0.1733)	
Epoch: [65][0/159],	Time 4.023 (4.023)	Data 3.353 (3.353)	siamese Loss 0.2584 (0.2584)	classifier Loss 0.1729 (0.1729)	
Epoch: [65][20/159],	Time 1.080 (1.037)	Data 0.002 (0.162)	siamese Loss 0.3007 (0.2745)	classifier Loss 0.1747 (0.1732)	
Epoch: [65][40/159],	Time 1.095 (0.961)	Data 0.002 (0.084)	siamese Loss 0.3036 (0.2765)	classifier Loss 0.1753 (0.1734)	
Epoch: [65][60/159],	Time 1.081 (0.935)	Data 0.003 (0.057)	siamese Loss 0.2542 (0.2754)	classifier Loss 0.1733 (0.1735)	
Epoch: [65][80/159],	Time 1.092 (0.922)	Data 0.002 (0.044)	siamese Loss 0.2686 (0.2752)	classifier Loss 0.1735 (0.1734)	
Epoch: [65][100/159],	Time 1.093 (0.915)	Data 0.002 (0.035)	siamese Loss 0.2900 (0.2739)	classifier Loss 0.1737 (0.1734)	
Epoch: [65][120/159],	Time 1.113 (0.910)	Data 0.003 (0.030)	siamese Loss 0.3274 (0.2763)	classifier Loss 0.1733 (0.1734)	
Epoch: [65][140/159],	Time 1.083 (0.907)	Data 0.003 (0.026)	siamese Loss 0.3154 (0.2759)	classifier Loss 0.1723 (0.1734)	
Validate: [0/16]	Time 4.162 (4.162)	siamese loss 0.9534 (0.9534)	clf loss 0.7047 (0.7047)	
Validating Results: siamese Loss 1.17521, classification loss 0.69478, Accuracy: 50.000%
Epoch: [66][0/159],	Time 4.445 (4.445)	Data 3.771 (3.771)	siamese Loss 0.2782 (0.2782)	classifier Loss 0.1725 (0.1725)	
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
Traceback (most recent call last):
  File "train_baseline.py", line 289, in <module>
    main()
  File "train_baseline.py", line 67, in main
    checkpoint = torch.load(LAST_SAVE_PATH)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torch/serialization.py", line 419, in load
    f = open(f, 'rb')
FileNotFoundError: [Errno 2] No such file or directory: 'r2plus1d_18_bt_24_seg_10_baseline_using_frame__best.pth.tar'
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
Traceback (most recent call last):
  File "train_baseline.py", line 289, in <module>
    main()
  File "train_baseline.py", line 67, in main
    checkpoint = torch.load(LAST_SAVE_PATH)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torch/serialization.py", line 419, in load
    f = open(f, 'rb')
FileNotFoundError: [Errno 2] No such file or directory: '/home/sjhu/projects/compressed_video_compare/baseline/r2plus1d_18_bt_24_seg_10_baseline_using_frame__best.pth.tar'
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
model epoch 16 lowest loss 0.21340054646134377
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 2060238848 bytes == 0x560fce39a000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x560feba7e000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x560f9bd30000 @ 
tcmalloc: large alloc 2285584384 bytes == 0x560ff73c2000 @ 
tcmalloc: large alloc 1399087104 bytes == 0x560f89668000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fae45c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fcd67a000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x560fba828000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x560fea8cc000 @ 
tcmalloc: large alloc 1703550976 bytes == 0x560faf382000 @ 
tcmalloc: large alloc 2058674176 bytes == 0x560fdb82c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fae790000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x560fad918000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fcd61a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fb920a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fe96c2000 @ 
tcmalloc: large alloc 1495179264 bytes == 0x560ff0128000 @ 
tcmalloc: large alloc 1987911680 bytes == 0x560febe54000 @ 
tcmalloc: large alloc 2030215168 bytes == 0x560fdb618000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x560fe7d4a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fea4b2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fcd5b6000 @ 
Epoch: [16][0/475],	Time 91.220 (91.220)	Data 71.737 (71.737)	siamese Loss 0.0601 (0.0601)	classifier Loss 0.0363 (0.0363)	
tcmalloc: large alloc 2453487616 bytes == 0x561002b14000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fadef8000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fe6b6e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560faf0b8000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x560fea482000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x560fd55d4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fcd59e000 @ 
tcmalloc: large alloc 1737498624 bytes == 0x560fd365e000 @ 
tcmalloc: large alloc 2600755200 bytes == 0x561074b2e000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x560fd55b6000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x560fc9c4c000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x560fad026000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x560fd2044000 @ 
tcmalloc: large alloc 2269659136 bytes == 0x560ff5fb8000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x5610018cc000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x560fdc282000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x56100183a000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x560fdd03c000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x560fdc342000 @ 
tcmalloc: large alloc 2633924608 bytes == 0x56104aea4000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5610c66ce000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x560fee7be000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x560ffbf46000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x560fec13a000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x560fdea2c000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x5610b2b84000 @ 
Epoch: [16][20/475],	Time 0.251 (16.324)	Data 0.000 (15.152)	siamese Loss 0.1063 (0.1332)	classifier Loss 0.0522 (0.0961)	
tcmalloc: large alloc 3412058112 bytes == 0x561078c2c000 @ 
tcmalloc: large alloc 2488999936 bytes == 0x561010d34000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x561098bd6000 @ 
tcmalloc: large alloc 2943606784 bytes == 0x561077080000 @ 
tcmalloc: large alloc 4335902720 bytes == 0x5611bce06000 @ 
tcmalloc: large alloc 3496402944 bytes == 0x561095616000 @ 
tcmalloc: large alloc 3587792896 bytes == 0x561094ea8000 @ 
tcmalloc: large alloc 3587792896 bytes == 0x561094ea8000 @ 
Epoch: [16][40/475],	Time 0.250 (14.223)	Data 0.000 (13.503)	siamese Loss 0.1313 (0.1354)	classifier Loss 0.1138 (0.0998)	
tcmalloc: large alloc 2269659136 bytes == 0x560ffb966000 @ 
tcmalloc: large alloc 4203511808 bytes == 0x5610d49f0000 @ 
tcmalloc: large alloc 4375330816 bytes == 0x5610fbbe0000 @ 
tcmalloc: large alloc 4760485888 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x5611c78a2000 @ 
tcmalloc: large alloc 4378984448 bytes == 0x5610fc146000 @ 
tcmalloc: large alloc 2531303424 bytes == 0x560fbab14000 @ 
Epoch: [16][60/475],	Time 0.264 (13.445)	Data 0.000 (12.878)	siamese Loss 0.1372 (0.1336)	classifier Loss 0.0913 (0.0965)	
tcmalloc: large alloc 4226752512 bytes == 0x56119f318000 @ 
tcmalloc: large alloc 2786418688 bytes == 0x5611b2474000 @ 
tcmalloc: large alloc 2454528000 bytes == 0x5610de03e000 @ 
Epoch: [16][80/475],	Time 0.289 (12.750)	Data 0.000 (12.262)	siamese Loss 0.0959 (0.1311)	classifier Loss 0.0726 (0.0941)	
tcmalloc: large alloc 3791724544 bytes == 0x56107550c000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x5610fc768000 @ 
tcmalloc: large alloc 3570556928 bytes == 0x5610974c2000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5610c64e0000 @ 
Epoch: [16][100/475],	Time 0.267 (11.772)	Data 0.000 (11.332)	siamese Loss 0.1816 (0.1301)	classifier Loss 0.1122 (0.0934)	
tcmalloc: large alloc 6823067648 bytes == 0x56120ebce000 @ 
tcmalloc: large alloc 3642892288 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 4203249664 bytes == 0x5610e6398000 @ 
Epoch: [16][120/475],	Time 0.292 (12.519)	Data 0.000 (12.111)	siamese Loss 0.0448 (0.1261)	classifier Loss 0.0214 (0.0902)	
tcmalloc: large alloc 4258086912 bytes == 0x56120581a000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 2786418688 bytes == 0x5610ac112000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 3673964544 bytes == 0x5610a3d60000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x56125bdbc000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x56125aee0000 @ 
Epoch: [16][140/475],	Time 0.250 (12.334)	Data 0.000 (11.949)	siamese Loss 0.1623 (0.1262)	classifier Loss 0.1233 (0.0899)	
tcmalloc: large alloc 3788070912 bytes == 0x56116b43e000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x561098316000 @ 
tcmalloc: large alloc 3638968320 bytes == 0x5610a0ed6000 @ 
tcmalloc: large alloc 5126307840 bytes == 0x5611ba522000 @ 
Epoch: [16][160/475],	Time 0.257 (12.189)	Data 0.000 (11.821)	siamese Loss 0.0913 (0.1240)	classifier Loss 0.0774 (0.0879)	
tcmalloc: large alloc 4226490368 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 4687888384 bytes == 0x561171fb2000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x5611f34b6000 @ 
tcmalloc: large alloc 5362106368 bytes == 0x56122bfde000 @ 
tcmalloc: large alloc 17377542144 bytes == (nil) @ 
Traceback (most recent call last):
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
model epoch 16 lowest loss 0.21340054646134377
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1112375296 bytes == 0x55e3071a0000 @ 
tcmalloc: large alloc 1077125120 bytes == 0x55e308c84000 @ 
tcmalloc: large alloc 1643495424 bytes == 0x55e34dc5a000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x55e371d06000 @ 
tcmalloc: large alloc 1637228544 bytes == 0x55e34ceb8000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x55e36a686000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x55e3706e0000 @ 
tcmalloc: large alloc 1201152000 bytes == 0x55e318730000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x55e40eb04000 @ 
tcmalloc: large alloc 1198546944 bytes == 0x55e316bf2000 @ 
tcmalloc: large alloc 1246855168 bytes == 0x55e2dc674000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e371294000 @ 
tcmalloc: large alloc 1214210048 bytes == 0x55e31ad66000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e3542a8000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x55e3e8e68000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e371294000 @ 
Epoch: [16][0/475],	Time 65.609 (65.609)	Data 46.665 (46.665)	siamese Loss 0.0855 (0.0855)	classifier Loss 0.0382 (0.0382)	
tcmalloc: large alloc 1605894144 bytes == 0x55e361758000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x55e46bc94000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x55e36a84e000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x55e33eb08000 @ 
tcmalloc: large alloc 2317443072 bytes == 0x55e41bf38000 @ 
tcmalloc: large alloc 1827840000 bytes == 0x55e367910000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e308666000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x55e3e8466000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x55e33f9b6000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x55e39a0e4000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x55e31bb7a000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x55e3e7144000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x55e46acfa000 @ 
tcmalloc: large alloc 1859174400 bytes == 0x55e360572000 @ 
tcmalloc: large alloc 2136752128 bytes == 0x55e3302a6000 @ 
tcmalloc: large alloc 1840898048 bytes == 0x55e365a8e000 @ 
tcmalloc: large alloc 1936465920 bytes == 0x55e3e3ba4000 @ 
tcmalloc: large alloc 2175655936 bytes == 0x55e3ea082000 @ 
tcmalloc: large alloc 2240413696 bytes == 0x55e389908000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x55e371e12000 @ 
tcmalloc: large alloc 1936465920 bytes == 0x55e3e9cb6000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x55e39173a000 @ 
tcmalloc: large alloc 17377542144 bytes == (nil) @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e370a68000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x55e34c0f4000 @ 
tcmalloc: large alloc 2453487616 bytes == 0x55e3ead82000 @ 
tcmalloc: large alloc 1727053824 bytes == 0x55e36fdda000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55e3b4a76000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x55e458248000 @ 
Traceback (most recent call last):
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
tcmalloc: large alloc 3570556928 bytes == 0x55e43d4b2000 @ 
Epoch: [16][20/475],	Time 0.251 (17.195)	Data 0.000 (16.040)	siamese Loss 0.1450 (0.1423)	classifier Loss 0.0953 (0.1007)	
tcmalloc: large alloc 2191319040 bytes == 0x55e3e3ba4000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x55e391102000 @ 
tcmalloc: large alloc 4171653120 bytes == 0x55e48729a000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x55e44d64c000 @ 
tcmalloc: large alloc 2465234944 bytes == 0x55e4468ec000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1172168704 bytes == 0x557cdda24000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d1bb8c000 @ 
tcmalloc: large alloc 1445306368 bytes == 0x557cfd722000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d1bebe000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x557d33794000 @ 
tcmalloc: large alloc 1297506304 bytes == 0x557ceb274000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x557d2d424000 @ 
tcmalloc: large alloc 1737498624 bytes == 0x557d216ec000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x557d2e974000 @ 
tcmalloc: large alloc 1346076672 bytes == 0x557cf12ba000 @ 
tcmalloc: large alloc 1401176064 bytes == 0x557d1b532000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d1c776000 @ 
tcmalloc: large alloc 2120818688 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 1408745472 bytes == 0x557cfba78000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x557d203f8000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x557d2f0be000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x557d190d0000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d200c4000 @ 
tcmalloc: large alloc 1390206976 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 1987911680 bytes == 0x557d383fe000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x557d5a0b0000 @ 
tcmalloc: large alloc 2052407296 bytes == 0x557d4859e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d30c7a000 @ 
Epoch: [0][0/475],	Time 72.562 (72.562)	Data 54.668 (54.668)	siamese Loss 42.0068 (42.0068)	classifier Loss 0.1876 (0.1876)	
tcmalloc: large alloc 2052407296 bytes == 0x557d473b6000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d1eff8000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x557d333c0000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x557d33090000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x557d50ba0000 @ 
tcmalloc: large alloc 1827840000 bytes == 0x557d303ca000 @ 
tcmalloc: large alloc 1526513664 bytes == 0x557d2dda2000 @ 
tcmalloc: large alloc 2211430400 bytes == 0x557d57032000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d2dda2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d2f8a0000 @ 
tcmalloc: large alloc 2240413696 bytes == 0x557d52ac6000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d34742000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x557e21182000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x557d361c8000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x557d4d89a000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x557d51fa4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 2052407296 bytes == 0x557d51fa0000 @ 
tcmalloc: large alloc 4378984448 bytes == 0x557e693cc000 @ 
tcmalloc: large alloc 4335902720 bytes == 0x557f0d934000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x557f36226000 @ 
tcmalloc: large alloc 4195418112 bytes == 0x557e5a114000 @ 
tcmalloc: large alloc 2060238848 bytes == 0x557db21de000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x557f0b400000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x557f3558e000 @ 
Epoch: [0][20/475],	Time 0.217 (24.626)	Data 0.000 (23.564)	siamese Loss 14.5956 (19.2963)	classifier Loss 0.1747 (0.1743)	
tcmalloc: large alloc 2786418688 bytes == 0x557db0c58000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x557f3558e000 @ 
tcmalloc: large alloc 2136752128 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x557d35e02000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 3642892288 bytes == 0x557e293ea000 @ 
tcmalloc: large alloc 2578563072 bytes == 0x557d9fdf8000 @ 
tcmalloc: large alloc 3791724544 bytes == 0x557e2121e000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x557e06968000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x557f7a8e0000 @ 
Epoch: [0][40/475],	Time 71.564 (17.651)	Data 71.349 (17.000)	siamese Loss 8.6334 (14.4827)	classifier Loss 0.1729 (0.1737)	
tcmalloc: large alloc 6022995968 bytes == 0x557f78bf8000 @ 
tcmalloc: large alloc 2914099200 bytes == 0x557d2dda2000 @ 
Epoch: [0][60/475],	Time 0.245 (14.446)	Data 0.000 (13.937)	siamese Loss 2.9614 (11.3982)	classifier Loss 0.1724 (0.1733)	
tcmalloc: large alloc 4461240320 bytes == 0x557f6eccc000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x557f0dd12000 @ 
Epoch: [0][80/475],	Time 0.227 (12.761)	Data 0.000 (12.324)	siamese Loss 1.1402 (8.9929)	classifier Loss 0.1682 (0.1736)	
tcmalloc: large alloc 3226927104 bytes == 0x557e27d4a000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x557e60eb4000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x557e14946000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x557e20b14000 @ 
tcmalloc: large alloc 2943606784 bytes == 0x557d2dda2000 @ 
Epoch: [0][100/475],	Time 0.233 (12.225)	Data 0.000 (11.832)	siamese Loss 0.3969 (7.3083)	classifier Loss 0.1715 (0.1736)	
tcmalloc: large alloc 3788070912 bytes == 0x557e20e42000 @ 
tcmalloc: large alloc 2261827584 bytes == 0x557d5440c000 @ 
tcmalloc: large alloc 2872844288 bytes == 0x557db94aa000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x557e019c6000 @ 
Epoch: [0][120/475],	Time 0.224 (11.372)	Data 0.000 (11.008)	siamese Loss 0.2661 (6.1441)	classifier Loss 0.1730 (0.1737)	
tcmalloc: large alloc 4203249664 bytes == 0x557e53f2c000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x5580e20da000 @ 
tcmalloc: large alloc 3102105600 bytes == 0x557dc15f0000 @ 
tcmalloc: large alloc 4760485888 bytes == 0x557f5a4f4000 @ 
Epoch: [0][140/475],	Time 0.254 (12.119)	Data 0.000 (11.776)	siamese Loss 0.2497 (5.3091)	classifier Loss 0.1803 (0.1736)	
tcmalloc: large alloc 6823067648 bytes == 0x5580768e8000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x557e33df0000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x557e33ef4000 @ 
Epoch: [0][160/475],	Time 0.222 (11.640)	Data 0.000 (11.312)	siamese Loss 0.2407 (4.6825)	classifier Loss 0.1769 (0.1737)	
tcmalloc: large alloc 4258086912 bytes == 0x557e0135c000 @ 
tcmalloc: large alloc 3740549120 bytes == 0x557e21198000 @ 
tcmalloc: large alloc 3740549120 bytes == 0x557e21270000 @ 
tcmalloc: large alloc 3515981824 bytes == 0x557e9a8e4000 @ 
tcmalloc: large alloc 3756474368 bytes == 0x557e10786000 @ 
tcmalloc: large alloc 4813225984 bytes == 0x557df59c0000 @ 
Epoch: [0][180/475],	Time 38.261 (11.595)	Data 38.052 (11.280)	siamese Loss 0.2486 (4.1929)	classifier Loss 0.1734 (0.1738)	
tcmalloc: large alloc 4195418112 bytes == 0x557e531ea000 @ 
tcmalloc: large alloc 4375068672 bytes == 0x557df59c0000 @ 
Epoch: [0][200/475],	Time 0.217 (11.063)	Data 0.000 (10.758)	siamese Loss 0.3221 (3.8007)	classifier Loss 0.1687 (0.1737)	
tcmalloc: large alloc 5126307840 bytes == 0x557f6b2a4000 @ 
tcmalloc: large alloc 4203511808 bytes == 0x557e44ad8000 @ 
tcmalloc: large alloc 4202987520 bytes == 0x557f0365c000 @ 
tcmalloc: large alloc 3570556928 bytes == 0x557e9a8e4000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x55820dbe4000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x55820dbe4000 @ 
tcmalloc: large alloc 17377542144 bytes == (nil) @ 
Epoch: [0][220/475],	Time 182.623 (11.708)	Data 182.412 (11.412)	siamese Loss 0.2323 (3.4804)	classifier Loss 0.1777 (0.1738)	
Traceback (most recent call last):
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1297506304 bytes == 0x5611d1758000 @ 
tcmalloc: large alloc 1275838464 bytes == 0x5611cef14000 @ 
tcmalloc: large alloc 1400913920 bytes == 0x5611de6ac000 @ 
tcmalloc: large alloc 1886593024 bytes == 0x56121449e000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x5611fd3c8000 @ 
tcmalloc: large alloc 1308213248 bytes == 0x5611d333e000 @ 
tcmalloc: large alloc 4226490368 bytes == 0x56133d3b4000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x56121f71c000 @ 
tcmalloc: large alloc 1346076672 bytes == 0x5611d7a2e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x561202cc6000 @ 
tcmalloc: large alloc 1408745472 bytes == 0x561213762000 @ 
tcmalloc: large alloc 1529380864 bytes == 0x5611f19c4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x561203116000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x5611f19b4000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x5612ed2a8000 @ 
tcmalloc: large alloc 6823067648 bytes == 0x5614640e6000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x561214252000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x5611e8736000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x561212a94000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x561212a6c000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x56120f35e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x561203acc000 @ 
tcmalloc: large alloc 1271398400 bytes == 0x5611d1f44000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5612197ba000 @ 
tcmalloc: large alloc 2030215168 bytes == 0x56128dbf8000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x5611d56f4000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x5612100a2000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x5611d56f0000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x561218dbe000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x56121f274000 @ 
Epoch: [0][0/475],	Time 163.263 (163.263)	Data 141.674 (141.674)	siamese Loss 28.6199 (28.6199)	classifier Loss 0.1735 (0.1735)	
tcmalloc: large alloc 2191319040 bytes == 0x5612d479e000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x56128dbf8000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x56128852c000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x5612d479e000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x561241526000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5611ccc56000 @ 
tcmalloc: large alloc 1790763008 bytes == 0x56128dbf8000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x56137c810000 @ 
tcmalloc: large alloc 1976942592 bytes == 0x56128852c000 @ 
tcmalloc: large alloc 2058412032 bytes == 0x5611498d2000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
Traceback (most recent call last):
  File "train_baseline.py", line 289, in <module>
    main()
  File "train_baseline.py", line 63, in main
    base_model=args.arch)
  File "/home/sjhu/projects/compressed_video_compare/baseline/model_baseline.py", line 45, in __init__
    self._prepare_base_model(base_model)
  File "/home/sjhu/projects/compressed_video_compare/baseline/model_baseline.py", line 61, in _prepare_base_model
    self.base_model_channel_3 = torchvision.models.video.r2plus2plus1d_18(pretrained=True)
AttributeError: module 'torchvision.models.video' has no attribute 'r2plus2plus1d_18'
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1330413568 bytes == 0x556698274000 @ 
tcmalloc: large alloc 1198546944 bytes == 0x556687ec8000 @ 
tcmalloc: large alloc 1807998976 bytes == 0x5566cbf10000 @ 
tcmalloc: large alloc 1207681024 bytes == 0x55668b86e000 @ 
tcmalloc: large alloc 1633312768 bytes == 0x5566be2c8000 @ 
tcmalloc: large alloc 2224742400 bytes == 0x5566f9822000 @ 
tcmalloc: large alloc 1083654144 bytes == 0x55667bf30000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x5566efa1c000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x5566cb01c000 @ 
tcmalloc: large alloc 1463582720 bytes == 0x5566a932c000 @ 
tcmalloc: large alloc 3638968320 bytes == 0x5567b7fce000 @ 
tcmalloc: large alloc 3515981824 bytes == 0x5567afaf0000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x5567038ce000 @ 
tcmalloc: large alloc 1400913920 bytes == 0x55666c988000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5566c5c7e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5566f8800000 @ 
tcmalloc: large alloc 1790763008 bytes == 0x5566ee75c000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x5566f76c6000 @ 
tcmalloc: large alloc 2277752832 bytes == 0x556770708000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x5566d683a000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5566f87b0000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x5566f87b0000 @ 
tcmalloc: large alloc 1401176064 bytes == 0x5566f6494000 @ 
tcmalloc: large alloc 2633924608 bytes == 0x5567aed3e000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5566f63c8000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5567aed3e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5566d1882000 @ 
tcmalloc: large alloc 2402304000 bytes == 0x5567aed3e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5566fe43e000 @ 
tcmalloc: large alloc 1976942592 bytes == 0x55676d9c8000 @ 
tcmalloc: large alloc 1878237184 bytes == 0x5566dcd82000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5566fd3e8000 @ 
tcmalloc: large alloc 2786418688 bytes == 0x5567a710e000 @ 
tcmalloc: large alloc 2052407296 bytes == 0x5566ea0aa000 @ 
tcmalloc: large alloc 1903566848 bytes == 0x5566e8e48000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x556715c9c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5567a710e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55668fe9a000 @ 
Epoch: [0][0/475],	Time 388.541 (388.541)	Data 369.158 (369.158)	siamese Loss 17.2239 (17.2239)	classifier Loss 0.1762 (0.1762)	
tcmalloc: large alloc 2344075264 bytes == 0x55677a656000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5566f52dc000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x55668fdce000 @ 
tcmalloc: large alloc 3102105600 bytes == 0x5567ad428000 @ 
tcmalloc: large alloc 2914099200 bytes == 0x55676d9c8000 @ 
tcmalloc: large alloc 3102105600 bytes == 0x55679c944000 @ 
tcmalloc: large alloc 4203511808 bytes == 0x5567eb416000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x55677a656000 @ 
tcmalloc: large alloc 4786855936 bytes == 0x55688d998000 @ 
tcmalloc: large alloc 4171653120 bytes == 0x5567f98f6000 @ 
Epoch: [0][20/475],	Time 0.259 (28.009)	Data 0.055 (26.891)	siamese Loss 6.2827 (14.8080)	classifier Loss 0.1757 (0.1736)	
tcmalloc: large alloc 4390739968 bytes == 0x5568a6660000 @ 
tcmalloc: large alloc 1936465920 bytes == 0x55668f928000 @ 
tcmalloc: large alloc 2052407296 bytes == 0x5566f1f8c000 @ 
tcmalloc: large alloc 2058674176 bytes == 0x5566f4384000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x5567a7d64000 @ 
tcmalloc: large alloc 2230493184 bytes == 0x5566c8db8000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x5566e1e18000 @ 
tcmalloc: large alloc 2222653440 bytes == 0x55677c9f2000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x55688d998000 @ 
Epoch: [0][40/475],	Time 42.785 (19.286)	Data 42.539 (18.617)	siamese Loss 8.0885 (10.1169)	classifier Loss 0.1760 (0.1736)	
tcmalloc: large alloc 3496402944 bytes == 0x5567ad48e000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x556e7ed94000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x556e7daca000 @ 
tcmalloc: large alloc 3611295744 bytes == 0x556da18b6000 @ 
tcmalloc: large alloc 2786418688 bytes == 0x5567a7d64000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x5568dee4a000 @ 
tcmalloc: large alloc 4378984448 bytes == 0x55681307c000 @ 
Epoch: [0][60/475],	Time 0.272 (20.828)	Data 0.064 (20.312)	siamese Loss 1.4981 (7.4392)	classifier Loss 0.1782 (0.1740)	
tcmalloc: large alloc 6022995968 bytes == 0x5568de1b2000 @ 
tcmalloc: large alloc 4316577792 bytes == 0x55680b2d0000 @ 
tcmalloc: large alloc 4429905920 bytes == 0x5568c863c000 @ 
tcmalloc: large alloc 3570556928 bytes == 0x5567fc1b0000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x556896156000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x556a66df0000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x55693e0f2000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x55693e0f2000 @ 
Epoch: [0][80/475],	Time 0.262 (19.452)	Data 0.065 (19.015)	siamese Loss 0.4835 (5.7659)	classifier Loss 0.1737 (0.1742)	
tcmalloc: large alloc 4382900224 bytes == 0x5568a6660000 @ 
tcmalloc: large alloc 2600755200 bytes == 0x5567a710e000 @ 
tcmalloc: large alloc 2230493184 bytes == 0x5567801c0000 @ 
tcmalloc: large alloc 4202987520 bytes == 0x556934ff6000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x556781044000 @ 
Epoch: [0][100/475],	Time 0.273 (16.728)	Data 0.052 (16.338)	siamese Loss 0.4719 (4.7091)	classifier Loss 0.1666 (0.1742)	
tcmalloc: large alloc 3948134400 bytes == 0x5567ddae8000 @ 
tcmalloc: large alloc 2910969856 bytes == 0x55677bd24000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x55688d998000 @ 
tcmalloc: large alloc 5362106368 bytes == 0x5568f0e26000 @ 
tcmalloc: large alloc 17377542144 bytes == (nil) @ 
Traceback (most recent call last):
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
tcmalloc: large alloc 4760485888 bytes == 0x5569239c4000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x55688d998000 @ 
tcmalloc: large alloc 3412058112 bytes == 0x5568f0e26000 @ 
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1203503104 bytes == 0x55878b288000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587c7948000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x5587d8a16000 @ 
tcmalloc: large alloc 1886076928 bytes == 0x5587df6c8000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5587e5a9e000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x5587deaac000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5587e7dc4000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x5587f890c000 @ 
tcmalloc: large alloc 2269659136 bytes == 0x55880f494000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587e734e000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x55880e6ee000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587f78be000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587f7892000 @ 
tcmalloc: large alloc 2046926848 bytes == 0x55880396e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587f7892000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587c8e30000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5587f7892000 @ 
tcmalloc: large alloc 1496481792 bytes == 0x5587b5aa6000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x5587f783e000 @ 
tcmalloc: large alloc 1318658048 bytes == 0x5587debde000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x55884b420000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587da3b2000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x5587e436a000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5588e0eb4000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x5587da3b2000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x5587c7e5a000 @ 
tcmalloc: large alloc 5362106368 bytes == 0x55897bc3c000 @ 
tcmalloc: large alloc 1440342016 bytes == 0x5587dee08000 @ 
tcmalloc: large alloc 1408745472 bytes == 0x5587eeb56000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5588556c4000 @ 
Epoch: [0][0/475],	Time 214.376 (214.376)	Data 190.518 (190.518)	siamese Loss 12.6332 (12.6332)	classifier Loss 0.1707 (0.1707)	
tcmalloc: large alloc 1753161728 bytes == 0x5587c8de4000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x5587de666000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5587e0554000 @ 
tcmalloc: large alloc 1906442240 bytes == 0x5587de542000 @ 
tcmalloc: large alloc 1936465920 bytes == 0x55884b420000 @ 
tcmalloc: large alloc 1827840000 bytes == 0x5587de1fe000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x5587dc514000 @ 
tcmalloc: large alloc 2050842624 bytes == 0x55884b420000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5587e3f30000 @ 
tcmalloc: large alloc 2230493184 bytes == 0x55880c112000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587e34a2000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x5588505b0000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x55884b420000 @ 
tcmalloc: large alloc 3633487872 bytes == 0x5588b92a6000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5587e1234000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x55880ab76000 @ 
tcmalloc: large alloc 6022995968 bytes == 0x558ac3210000 @ 
tcmalloc: large alloc 2254249984 bytes == 0x55880fe4a000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x558919468000 @ 
tcmalloc: large alloc 3515981824 bytes == 0x5588b48cc000 @ 
Epoch: [0][20/475],	Time 0.584 (22.032)	Data 0.057 (20.506)	siamese Loss 6.3775 (7.0743)	classifier Loss 0.1735 (0.1726)	
tcmalloc: large alloc 1893122048 bytes == 0x5587df7a8000 @ 
tcmalloc: large alloc 2488999936 bytes == 0x5588077b0000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587ded1a000 @ 
tcmalloc: large alloc 2262343680 bytes == 0x5588634a8000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5588df742000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5587dfb14000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x5588505b0000 @ 
Epoch: [0][40/475],	Time 0.592 (14.192)	Data 0.057 (13.200)	siamese Loss 3.7720 (5.5701)	classifier Loss 0.1690 (0.1726)	
tcmalloc: large alloc 2914099200 bytes == 0x55886057e000 @ 
tcmalloc: large alloc 4375330816 bytes == 0x5589d5e5a000 @ 
tcmalloc: large alloc 3791724544 bytes == 0x5588cda52000 @ 
tcmalloc: large alloc 6823067648 bytes == 0x558a28716000 @ 
tcmalloc: large alloc 3054845952 bytes == 0x5588f295e000 @ 
tcmalloc: large alloc 4687888384 bytes == 0x5589d767e000 @ 
tcmalloc: large alloc 3611295744 bytes == 0x5589cb4c8000 @ 
Epoch: [0][60/475],	Time 0.585 (13.041)	Data 0.057 (12.236)	siamese Loss 0.9698 (4.3178)	classifier Loss 0.1737 (0.1735)	
tcmalloc: large alloc 4226490368 bytes == 0x5589cccec000 @ 
tcmalloc: large alloc 3642892288 bytes == 0x5588e766e000 @ 
tcmalloc: large alloc 3756474368 bytes == 0x55895e8fe000 @ 
tcmalloc: large alloc 3756474368 bytes == 0x55895e8fe000 @ 
tcmalloc: large alloc 4429905920 bytes == 0x5589cb4c8000 @ 
Epoch: [0][80/475],	Time 78.818 (13.018)	Data 78.383 (12.311)	siamese Loss 0.7498 (3.4563)	classifier Loss 0.1740 (0.1737)	
tcmalloc: large alloc 3791724544 bytes == 0x55895e8fe000 @ 
Epoch: [0][100/475],	Time 37.318 (11.499)	Data 36.888 (10.846)	siamese Loss 0.4042 (2.8436)	classifier Loss 0.1738 (0.1734)	
tcmalloc: large alloc 3788070912 bytes == 0x558ac3210000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x5588b5a70000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x5588d967e000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x5588d99aa000 @ 
tcmalloc: large alloc 7215529984 bytes == (nil) @ 
tcmalloc: large alloc 17377542144 bytes == (nil) @ 
Traceback (most recent call last):
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
tcmalloc: large alloc 4203249664 bytes == 0x55895e8fe000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x558ac3210000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x5588d7494000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1401176064 bytes == 0x55807ea92000 @ 
tcmalloc: large alloc 1784496128 bytes == 0x5580af4ae000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x55809b8fa000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5580a31c0000 @ 
tcmalloc: large alloc 1402216448 bytes == 0x55807fda2000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x55809ef44000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5580bbcd4000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5580b9b54000 @ 
tcmalloc: large alloc 1275838464 bytes == 0x55807b966000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x5580b081a000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x558093cec000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x5580ba78c000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x5580a2134000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x55808ee04000 @ 
tcmalloc: large alloc 1440342016 bytes == 0x5580b90a8000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5580b5b7e000 @ 
tcmalloc: large alloc 2062852096 bytes == 0x5580d84b2000 @ 
Epoch: [0][0/475],	Time 65.323 (65.323)	Data 44.804 (44.804)	siamese Loss 12.7588 (12.7588)	classifier Loss 0.1950 (0.1950)	
tcmalloc: large alloc 4226490368 bytes == 0x5581df0c8000 @ 
tcmalloc: large alloc 4375068672 bytes == 0x5581efcf0000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x5580a1234000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5580b2da6000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x55885cb50000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5580d06d2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5580a5102000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5580d06d2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558132c30000 @ 
tcmalloc: large alloc 2402304000 bytes == 0x558110d98000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x55885b636000 @ 
tcmalloc: large alloc 1807998976 bytes == 0x558c69866000 @ 
tcmalloc: large alloc 2046926848 bytes == 0x5580c6e48000 @ 
tcmalloc: large alloc 2293424128 bytes == 0x558124b44000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x55820981c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55812f8fa000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5580c1572000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x558124b44000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5580c18a6000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5580d06d2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558124b44000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:61: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  "follow-up version. Please use pts_unit 'sec'.")
tcmalloc: large alloc 1727053824 bytes == 0x559b9f4ce000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b9b626000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b9b93e000 @ 
tcmalloc: large alloc 1306910720 bytes == 0x559b6caea000 @ 
tcmalloc: large alloc 1831239680 bytes == 0x559bac754000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x559bda2f0000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x559bb7a64000 @ 
tcmalloc: large alloc 1455751168 bytes == 0x559b7f73e000 @ 
tcmalloc: large alloc 1840898048 bytes == 0x559bae83a000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x559bb9f68000 @ 
tcmalloc: large alloc 1656807424 bytes == 0x559bb6ff6000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x559bb24c6000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b9de42000 @ 
tcmalloc: large alloc 1790763008 bytes == 0x559bd913e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559bb6ff6000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x559bd2104000 @ 
tcmalloc: large alloc 1403527168 bytes == 0x559bb19f2000 @ 
tcmalloc: large alloc 1220739072 bytes == 0x559babbba000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x559bc9c84000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b9e47e000 @ 
tcmalloc: large alloc 1895735296 bytes == 0x559bb6634000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559bb722c000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x559c1f0c2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559bce5b2000 @ 
tcmalloc: large alloc 2240413696 bytes == 0x559c4b262000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559ba94d2000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x559bb19f2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559c1def8000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x559bcbe82000 @ 
tcmalloc: large alloc 1886593024 bytes == 0x559bce41a000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x559ca2ec6000 @ 
tcmalloc: large alloc 1737498624 bytes == 0x559c1a206000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x559bb663c000 @ 
Epoch: [0][0/475],	Time 78.439 (78.439)	Data 59.290 (59.290)	siamese Loss 5.9038 (5.9038)	classifier Loss 0.1607 (0.1607)	
tcmalloc: large alloc 1737498624 bytes == 0x559bcbe52000 @ 
tcmalloc: large alloc 2199158784 bytes == 0x559bd4354000 @ 
tcmalloc: large alloc 2030215168 bytes == 0x559bd384e000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x559bcb5be000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559bb442a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b8db1e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x559b9f350000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 1344774144 bytes == 0x558949770000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55897943e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558979a7c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558979fdc000 @ 
tcmalloc: large alloc 2058674176 bytes == 0x5589a72dc000 @ 
tcmalloc: large alloc 1126998016 bytes == 0x558934476000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55897a28a000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x55899179c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55897b694000 @ 
tcmalloc: large alloc 1432248320 bytes == 0x55895a0f8000 @ 
tcmalloc: large alloc 2294988800 bytes == 0x5589bdca8000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x558996324000 @ 
tcmalloc: large alloc 1220739072 bytes == 0x558901ef4000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x5589a4e78000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x558976e42000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55897bbf2000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5589bd202000 @ 
tcmalloc: large alloc 1790763008 bytes == 0x55898c1a2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55897c280000 @ 
tcmalloc: large alloc 1859174400 bytes == 0x558988a9e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5589a3ad8000 @ 
tcmalloc: large alloc 1423106048 bytes == 0x5589769de000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x558993888000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x55898dd26000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x558998e94000 @ 
tcmalloc: large alloc 2175655936 bytes == 0x5589b8666000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5589a12e0000 @ 
tcmalloc: large alloc 2230493184 bytes == 0x5589bfcb4000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x5589b6a86000 @ 
tcmalloc: large alloc 4258086912 bytes == 0x558ab892a000 @ 
Epoch: [0][0/475],	Time 83.623 (83.623)	Data 68.875 (68.875)	siamese Loss 8.4875 (8.4875)	classifier Loss 0.1820 (0.1820)	
tcmalloc: large alloc 2344075264 bytes == 0x5589ca448000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x55898751a000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x5589b1718000 @ 
tcmalloc: large alloc 4691288064 bytes == 0x558bc264c000 @ 
tcmalloc: large alloc 1729658880 bytes == 0x55897f55a000 @ 
tcmalloc: large alloc 2042748928 bytes == 0x5589a478e000 @ 
tcmalloc: large alloc 2453487616 bytes == 0x558a5e4c8000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5589b005a000 @ 
tcmalloc: large alloc 4691288064 bytes == 0x558bb6ef0000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x558a91a3a000 @ 
tcmalloc: large alloc 3756474368 bytes == 0x558a71240000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5589b51a2000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x558976608000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x5589a8250000 @ 
tcmalloc: large alloc 3102105600 bytes == 0x558a20b52000 @ 
Epoch: [0][20/475],	Time 0.587 (9.226)	Data 0.052 (8.123)	siamese Loss 3.6889 (6.7573)	classifier Loss 0.1825 (0.1774)	
tcmalloc: large alloc 1976942592 bytes == 0x5589b0db4000 @ 
tcmalloc: large alloc 3102105600 bytes == 0x558a20e6e000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x558c26f4c000 @ 
tcmalloc: large alloc 4226490368 bytes == 0x558ab5554000 @ 
tcmalloc: large alloc 2140667904 bytes == 0x5589b005a000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x558c25f30000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x55898b316000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x5589b9fb8000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x558c0b9e0000 @ 
tcmalloc: large alloc 5362106368 bytes == 0x558b2c97c000 @ 
tcmalloc: large alloc 3611295744 bytes == 0x558a67e86000 @ 
tcmalloc: large alloc 3226927104 bytes == 0x558a66fd0000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x558aa0d78000 @ 
tcmalloc: large alloc 2199158784 bytes == 0x5589b299c000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 1194631168 bytes == 0x5586e48c2000 @ 
tcmalloc: large alloc 1406918656 bytes == 0x5586fce0a000 @ 
tcmalloc: large alloc 1442693120 bytes == 0x558703b5a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558720830000 @ 
tcmalloc: large alloc 1109762048 bytes == 0x5586dad0e000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x558737cc6000 @ 
tcmalloc: large alloc 1706164224 bytes == 0x558723ce2000 @ 
tcmalloc: large alloc 1432510464 bytes == 0x5587004a0000 @ 
tcmalloc: large alloc 1496481792 bytes == 0x55870b608000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558736a1e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558720e06000 @ 
tcmalloc: large alloc 2066243584 bytes == 0x55874f08e000 @ 
tcmalloc: large alloc 1410048000 bytes == 0x55871f4c8000 @ 
tcmalloc: large alloc 1725743104 bytes == 0x5587216dc000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x558737c74000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558721112000 @ 
tcmalloc: large alloc 1301168128 bytes == 0x558703256000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55874e0aa000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55872176e000 @ 
tcmalloc: large alloc 1292550144 bytes == 0x55871f386000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558721a2e000 @ 
tcmalloc: large alloc 4429905920 bytes == 0x558874f72000 @ 
tcmalloc: large alloc 1635655680 bytes == 0x55871e594000 @ 
tcmalloc: large alloc 1383940096 bytes == 0x5587008ec000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558723c72000 @ 
tcmalloc: large alloc 1292550144 bytes == 0x55871e9f6000 @ 
tcmalloc: large alloc 1936465920 bytes == 0x5587465c8000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5587219a4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558725bbe000 @ 
tcmalloc: large alloc 1627824128 bytes == 0x558713ce8000 @ 
tcmalloc: large alloc 2240413696 bytes == 0x55875a312000 @ 
tcmalloc: large alloc 2058674176 bytes == 0x55874ec70000 @ 
Epoch: [0][0/475],	Time 76.165 (76.165)	Data 61.638 (61.638)	siamese Loss 7.9580 (7.9580)	classifier Loss 0.1800 (0.1800)	
tcmalloc: large alloc 2201247744 bytes == 0x558755372000 @ 
tcmalloc: large alloc 3496402944 bytes == 0x5588096a4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x55872af8e000 @ 
tcmalloc: large alloc 2050842624 bytes == 0x558759734000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x558748ea8000 @ 
tcmalloc: large alloc 2066243584 bytes == 0x55871b6c6000 @ 
tcmalloc: large alloc 1934901248 bytes == 0x5587b816c000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x558750b0a000 @ 
tcmalloc: large alloc 4461240320 bytes == 0x5588793fa000 @ 
tcmalloc: large alloc 2046926848 bytes == 0x558759734000 @ 
tcmalloc: large alloc 2140667904 bytes == 0x558753fec000 @ 
tcmalloc: large alloc 2120818688 bytes == 0x558757bb4000 @ 
tcmalloc: large alloc 3521732608 bytes == 0x5588203e4000 @ 
tcmalloc: large alloc 2914099200 bytes == 0x55886e9a4000 @ 
tcmalloc: large alloc 2120818688 bytes == 0x558758850000 @ 
tcmalloc: large alloc 2240413696 bytes == 0x558759750000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x558759734000 @ 
tcmalloc: large alloc 2454528000 bytes == 0x5587d052a000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x558753668000 @ 
tcmalloc: large alloc 4429905920 bytes == 0x5588743ae000 @ 
Epoch: [0][20/475],	Time 0.586 (11.572)	Data 0.000 (10.454)	siamese Loss 5.2574 (5.8537)	classifier Loss 0.1733 (0.1768)	
tcmalloc: large alloc 2193408000 bytes == 0x5587c71b4000 @ 
tcmalloc: large alloc 3496402944 bytes == 0x55885b452000 @ 
tcmalloc: large alloc 2531303424 bytes == 0x558804cf8000 @ 
tcmalloc: large alloc 3226927104 bytes == 0x5587e10ee000 @ 
tcmalloc: large alloc 3642892288 bytes == 0x558813b24000 @ 
tcmalloc: large alloc 4226752512 bytes == 0x55885e7fe000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x558862640000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x5588069b0000 @ 
Epoch: [0][40/475],	Time 0.574 (10.138)	Data 0.000 (9.339)	siamese Loss 2.3634 (4.6018)	classifier Loss 0.1686 (0.1754)	
tcmalloc: large alloc 4202987520 bytes == 0x55885b452000 @ 
tcmalloc: large alloc 4786855936 bytes == 0x5589897e0000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5588388a2000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 4
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 1306910720 bytes == 0x558b1fb7c000 @ 
tcmalloc: large alloc 1173995520 bytes == 0x558b0f94a000 @ 
tcmalloc: large alloc 1306910720 bytes == 0x558b1b112000 @ 
tcmalloc: large alloc 1233797120 bytes == 0x558b197de000 @ 
tcmalloc: large alloc 1488650240 bytes == 0x558b3949a000 @ 
tcmalloc: large alloc 2453487616 bytes == 0x558ba2fea000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x558b66460000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x558b6c120000 @ 
Epoch: [0][0/1423],	Time 25.190 (25.190)	Data 9.544 (9.544)	siamese Loss 0.2074 (0.2074)	classifier Loss 0.1550 (0.1550)	
tcmalloc: large alloc 2193408000 bytes == 0x558b7f620000 @ 
tcmalloc: large alloc 1436164096 bytes == 0x558b38838000 @ 
tcmalloc: large alloc 1201152000 bytes == 0x558b14232000 @ 
tcmalloc: large alloc 1527554048 bytes == 0x558b65380000 @ 
tcmalloc: large alloc 1488650240 bytes == 0x558b383e8000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x558b49948000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558b7e602000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558b66a7c000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558b4f5f6000 @ 
tcmalloc: large alloc 1827840000 bytes == 0x558b64bce000 @ 
tcmalloc: large alloc 1424416768 bytes == 0x558af98e0000 @ 
tcmalloc: large alloc 2531303424 bytes == 0x558bb7680000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x558b4a226000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x558b7e5c2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558b4f87a000 @ 
tcmalloc: large alloc 1651589120 bytes == 0x558b4dbb8000 @ 
tcmalloc: large alloc 2136489984 bytes == 0x558be4098000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x558b9fabe000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x558b566aa000 @ 
tcmalloc: large alloc 2224742400 bytes == 0x558b8361e000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558b56216000 @ 
tcmalloc: large alloc 1906442240 bytes == 0x558b6a6fe000 @ 
tcmalloc: large alloc 3587792896 bytes == 0x558c2cc42000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x558b56d5c000 @ 
tcmalloc: large alloc 1605894144 bytes == 0x558babcb0000 @ 
tcmalloc: large alloc 2050580480 bytes == 0x558b9fabe000 @ 
tcmalloc: large alloc 3587792896 bytes == 0x558c2cf0a000 @ 
Epoch: [0][20/1423],	Time 40.809 (5.755)	Data 40.541 (4.761)	siamese Loss 0.3145 (0.2230)	classifier Loss 0.1945 (0.1768)	
tcmalloc: large alloc 2191319040 bytes == 0x558b84e3a000 @ 
tcmalloc: large alloc 2211430400 bytes == 0x558b8fb3a000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x558b82550000 @ 
Epoch: [0][40/1423],	Time 0.281 (3.890)	Data 0.000 (3.252)	siamese Loss 0.2059 (0.2244)	classifier Loss 0.1736 (0.1762)	
tcmalloc: large alloc 1827840000 bytes == 0x558b9b9ca000 @ 
tcmalloc: large alloc 2201247744 bytes == 0x558b81028000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x558b8048a000 @ 
tcmalloc: large alloc 2344075264 bytes == 0x558ba0e4a000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x558ba6f1e000 @ 
tcmalloc: large alloc 3706339328 bytes == 0x558c3e892000 @ 
tcmalloc: large alloc 2465234944 bytes == 0x558c3f6a2000 @ 
tcmalloc: large alloc 3496402944 bytes == 0x558c5de5a000 @ 
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 4
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 2465234944 bytes == 0x558c3f6a2000 @ 
tcmalloc: large alloc 2058674176 bytes == 0x5632df956000 @ 
tcmalloc: large alloc 1840898048 bytes == 0x5632c4e3a000 @ 
tcmalloc: large alloc 1271398400 bytes == 0x56328119a000 @ 
tcmalloc: large alloc 1077125120 bytes == 0x563268134000 @ 
tcmalloc: large alloc 1198546944 bytes == 0x5632751a8000 @ 
tcmalloc: large alloc 1400913920 bytes == 0x56328e900000 @ 
tcmalloc: large alloc 1472978944 bytes == 0x5632decf6000 @ 
tcmalloc: large alloc 1214210048 bytes == 0x5632800f8000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5632c7622000 @ 
tcmalloc: large alloc 1637228544 bytes == 0x5632bf870000 @ 
Epoch: [0][0/1423],	Time 29.023 (29.023)	Data 14.288 (14.288)	siamese Loss 0.2345 (0.2345)	classifier Loss 0.1744 (0.1744)	
tcmalloc: large alloc 1737498624 bytes == 0x5632b7f52000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5632bf5a8000 @ 
tcmalloc: large alloc 4786855936 bytes == 0x563429ef0000 @ 
tcmalloc: large alloc 1932288000 bytes == 0x5632d38c4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5632ab3ae000 @ 
tcmalloc: large alloc 2105155584 bytes == 0x5632d9bb4000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5632ccdbe000 @ 
tcmalloc: large alloc 2910969856 bytes == 0x5633549e2000 @ 
tcmalloc: large alloc 3706339328 bytes == 0x5633a0d76000 @ 
tcmalloc: large alloc 1637228544 bytes == 0x5632b21b4000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5632b2b9e000 @ 
tcmalloc: large alloc 1853956096 bytes == 0x5632ab386000 @ 
tcmalloc: large alloc 2060238848 bytes == 0x5632dff8a000 @ 
tcmalloc: large alloc 2672041984 bytes == 0x56332bef2000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5632cf5d0000 @ 
tcmalloc: large alloc 1383940096 bytes == 0x56327a29a000 @ 
tcmalloc: large alloc 2074083328 bytes == 0x5632e1c84000 @ 
tcmalloc: large alloc 1945608192 bytes == 0x56332426c000 @ 
Epoch: [0][20/1423],	Time 26.982 (5.003)	Data 26.783 (4.126)	siamese Loss 0.2044 (0.2659)	classifier Loss 0.1604 (0.1746)	
tcmalloc: large alloc 1753161728 bytes == 0x5632ab386000 @ 
tcmalloc: large alloc 2066243584 bytes == 0x5632e0dbe000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x5633b6be6000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x5632dff56000 @ 
tcmalloc: large alloc 2269659136 bytes == 0x5632b2096000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x563321a84000 @ 
tcmalloc: large alloc 4375330816 bytes == 0x5634a3402000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x56334787e000 @ 
tcmalloc: large alloc 2269659136 bytes == 0x56329fa0c000 @ 
Epoch: [0][40/1423],	Time 13.773 (4.112)	Data 13.605 (3.578)	siamese Loss 0.2186 (0.2556)	classifier Loss 0.1709 (0.1736)	
tcmalloc: large alloc 1987911680 bytes == 0x5632dbea8000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x5632bf106000 @ 
tcmalloc: large alloc 1893122048 bytes == 0x5632b1146000 @ 
Epoch: [0][60/1423],	Time 0.186 (3.475)	Data 0.000 (3.058)	siamese Loss 0.2917 (0.2551)	classifier Loss 0.1753 (0.1736)	
tcmalloc: large alloc 1753161728 bytes == 0x5632b54ec000 @ 
tcmalloc: large alloc 5126307840 bytes == 0x563484894000 @ 
tcmalloc: large alloc 1987911680 bytes == 0x563340e04000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x56331c612000 @ 
tcmalloc: large alloc 4226752512 bytes == 0x5633ed078000 @ 
tcmalloc: large alloc 1948999680 bytes == 0x5632cee50000 @ 
tcmalloc: large alloc 2230493184 bytes == 0x5632a49be000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x56329ec0c000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 1083654144 bytes == 0x5614af60e000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x5614e80d2000 @ 
tcmalloc: large alloc 1095663616 bytes == 0x5614af4ca000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x561515f2a000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x5614ffcee000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x561527d4c000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x561610e30000 @ 
tcmalloc: large alloc 1784496128 bytes == 0x561504fb4000 @ 
tcmalloc: large alloc 1643495424 bytes == 0x5614f37e4000 @ 
tcmalloc: large alloc 1906442240 bytes == 0x561513af2000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x561516016000 @ 
tcmalloc: large alloc 1526513664 bytes == 0x5614e5222000 @ 
tcmalloc: large alloc 2066243584 bytes == 0x561527a98000 @ 
tcmalloc: large alloc 1769095168 bytes == 0x56150614c000 @ 
tcmalloc: large alloc 1455751168 bytes == 0x5614e4246000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5614fa1fa000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x561610382000 @ 
tcmalloc: large alloc 2224742400 bytes == 0x56152d816000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x56151049a000 @ 
tcmalloc: large alloc 1738276864 bytes == 0x56150de26000 @ 
tcmalloc: large alloc 1386553344 bytes == 0x5614ffb8a000 @ 
tcmalloc: large alloc 1625473024 bytes == 0x56152c844000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5614f967e000 @ 
tcmalloc: large alloc 2060238848 bytes == 0x56151a6f4000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x561610382000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x56150dce2000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x561510494000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x5614f9630000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x561519b94000 @ 
tcmalloc: large alloc 1976942592 bytes == 0x56151cd00000 @ 
tcmalloc: large alloc 2910969856 bytes == 0x56159b466000 @ 
tcmalloc: large alloc 1753161728 bytes == 0x56152c810000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x5615279c0000 @ 
tcmalloc: large alloc 1784496128 bytes == 0x561504f10000 @ 
tcmalloc: large alloc 1925242880 bytes == 0x56151b98e000 @ 
tcmalloc: large alloc 2277752832 bytes == 0x561537fde000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x561cb2d88000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x561cb1a42000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x561bb363c000 @ 
Epoch: [0][0/475],	Time 340.740 (340.740)	Data 325.312 (325.312)	siamese Loss 18.8191 (18.8191)	classifier Loss 0.1794 (0.1794)	
tcmalloc: large alloc 2066243584 bytes == 0x561a8767a000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x561605b70000 @ 
tcmalloc: large alloc 2175655936 bytes == 0x56153558c000 @ 
tcmalloc: large alloc 1723392000 bytes == 0x561605b70000 @ 
tcmalloc: large alloc 2454528000 bytes == 0x5615de464000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x561605b70000 @ 
tcmalloc: large alloc 2677006336 bytes == 0x5615c2e26000 @ 
tcmalloc: large alloc 2193408000 bytes == 0x5615938d6000 @ 
tcmalloc: large alloc 2531303424 bytes == 0x5615de464000 @ 
tcmalloc: large alloc 2191319040 bytes == 0x561537c92000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x561605b70000 @ 
tcmalloc: large alloc 2531303424 bytes == 0x5615de464000 @ 
tcmalloc: large alloc 4335902720 bytes == 0x561641ab4000 @ 
tcmalloc: large alloc 3788070912 bytes == 0x5615fd5da000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x561647a30000 @ 
Epoch: [0][20/475],	Time 0.268 (21.930)	Data 0.000 (20.967)	siamese Loss 8.9585 (15.3176)	classifier Loss 0.1744 (0.1756)	
tcmalloc: large alloc 3310485504 bytes == 0x5615de464000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x561646eea000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x56163a4bc000 @ 
tcmalloc: large alloc 2633924608 bytes == 0x5614d5860000 @ 
tcmalloc: large alloc 4461240320 bytes == 0x561596832000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x561611f7e000 @ 
Epoch: [0][40/475],	Time 0.239 (14.584)	Data 0.000 (13.983)	siamese Loss 4.0358 (10.7539)	classifier Loss 0.1718 (0.1746)	
tcmalloc: large alloc 3611295744 bytes == 0x561596832000 @ 
tcmalloc: large alloc 3496402944 bytes == 0x561632572000 @ 
tcmalloc: large alloc 3226927104 bytes == 0x56162dc12000 @ 
tcmalloc: large alloc 4813225984 bytes == 0x561744bba000 @ 
tcmalloc: large alloc 3310485504 bytes == 0x5616722aa000 @ 
tcmalloc: large alloc 4687888384 bytes == 0x561750480000 @ 
Epoch: [0][60/475],	Time 0.243 (12.730)	Data 0.000 (12.250)	siamese Loss 1.0698 (7.8163)	classifier Loss 0.1729 (0.1743)	
tcmalloc: large alloc 3642892288 bytes == 0x561605b70000 @ 
tcmalloc: large alloc 5126307840 bytes == 0x561c9f87a000 @ 
tcmalloc: large alloc 4195418112 bytes == 0x561630044000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x5616061fe000 @ 
tcmalloc: large alloc 4382900224 bytes == 0x561647438000 @ 
Epoch: [0][80/475],	Time 0.969 (11.754)	Data 0.816 (11.338)	siamese Loss 0.6139 (6.0608)	classifier Loss 0.1733 (0.1741)	
tcmalloc: large alloc 5362106368 bytes == 0x5617587b0000 @ 
tcmalloc: large alloc 4429905920 bytes == 0x561711f8a000 @ 
tcmalloc: large alloc 3791724544 bytes == 0x56170b980000 @ 
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn("video_reader video backend is not available")
4
Training arguments:
	data_root: /home/sjhu/datasets/all_datasets
	train_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_train_sample.txt
	test_list: /home/sjhu/projects/compressed_video_compare/data/datalists/all_test_sample.txt
	representation: mixed
	arch: r3d
	num_segments: 5
	no_accumulation: False
	dropout: 0.25
	keyfeatures: 128
	epochs: 300
	batch_size: 12
	lr: 0.01
	weight_decay: 0.0001
	gpus: [0, 1, 2, 3]
	workers: 8
	eval_freq: 5

Initializing model:
    base model:         r3d.
    num_segments:       5.
        
5692 pair videos loaded.
551 pair videos loaded.
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  + "follow-up version. Please use pts_unit 'sec'."
tcmalloc: large alloc 1083654144 bytes == 0x563fc754e000 @ 
tcmalloc: large alloc 1534083072 bytes == 0x563ffff78000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x5647ca088000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x5647c8dc2000 @ 
tcmalloc: large alloc 2068070400 bytes == 0x5646f9760000 @ 
Epoch: [0][0/475],	Time 278.063 (278.063)	Data 263.826 (263.826)	siamese Loss 27.4184 (27.4184)	classifier Loss 0.1805 (0.1805)	
tcmalloc: large alloc 3948134400 bytes == 0x564356876000 @ 
tcmalloc: large alloc 3948134400 bytes == 0x564356876000 @ 
tcmalloc: large alloc 2910969856 bytes == 0x563ff85cc000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x5640ac82e000 @ 
tcmalloc: large alloc 3999580160 bytes == 0x5640ac82e000 @ 
Epoch: [0][20/475],	Time 47.139 (80.692)	Data 46.929 (79.810)	siamese Loss 8.1290 (15.3119)	classifier Loss 0.1744 (0.1778)	
tcmalloc: large alloc 4382900224 bytes == 0x5640ac82e000 @ 
tcmalloc: large alloc 4335902720 bytes == 0x5640ac82e000 @ 
tcmalloc: large alloc 4461240320 bytes == 0x5640ac82e000 @ 
Epoch: [0][40/475],	Time 63.614 (73.512)	Data 63.411 (72.958)	siamese Loss 3.7848 (10.4672)	classifier Loss 0.1739 (0.1761)	
Epoch: [0][60/475],	Time 71.946 (71.113)	Data 71.699 (70.672)	siamese Loss 0.9555 (7.5309)	classifier Loss 0.1741 (0.1752)	
tcmalloc: large alloc 5126307840 bytes == 0x5647b6876000 @ 
Epoch: [0][80/475],	Time 69.015 (71.478)	Data 68.807 (71.093)	siamese Loss 0.5398 (5.8780)	classifier Loss 0.1740 (0.1750)	
Epoch: [0][100/475],	Time 86.807 (71.487)	Data 86.539 (71.135)	siamese Loss 0.2878 (4.8121)	classifier Loss 0.1732 (0.1747)	
Epoch: [0][120/475],	Time 62.298 (70.829)	Data 62.084 (70.500)	siamese Loss 0.4240 (4.0800)	classifier Loss 0.1734 (0.1745)	
tcmalloc: large alloc 6022995968 bytes == 0x5647b6876000 @ 
tcmalloc: large alloc 7215529984 bytes == 0x5647b6876000 @ 
Epoch: [0][140/475],	Time 39.490 (70.384)	Data 39.268 (70.071)	siamese Loss 0.2972 (3.5440)	classifier Loss 0.1734 (0.1743)	
Epoch: [0][160/475],	Time 99.578 (70.353)	Data 99.342 (70.053)	siamese Loss 0.2354 (3.1388)	classifier Loss 0.1733 (0.1742)	
tcmalloc: large alloc 17377542144 bytes == 0x564c47542000 @ 
Epoch: [0][180/475],	Time 103.282 (71.563)	Data 103.074 (71.274)	siamese Loss 0.3231 (2.8193)	classifier Loss 0.1733 (0.1741)	
Epoch: [0][200/475],	Time 74.831 (71.455)	Data 74.627 (71.174)	siamese Loss 0.2693 (2.5637)	classifier Loss 0.1731 (0.1740)	
Epoch: [0][220/475],	Time 36.105 (70.820)	Data 35.890 (70.545)	siamese Loss 0.3336 (2.3542)	classifier Loss 0.1733 (0.1740)	
Epoch: [0][240/475],	Time 38.290 (71.032)	Data 38.077 (70.763)	siamese Loss 0.2105 (2.1791)	classifier Loss 0.1730 (0.1739)	
Epoch: [0][260/475],	Time 62.605 (70.519)	Data 62.393 (70.255)	siamese Loss 0.2313 (2.0319)	classifier Loss 0.1734 (0.1739)	
Epoch: [0][280/475],	Time 54.394 (70.345)	Data 54.186 (70.085)	siamese Loss 0.2832 (1.9048)	classifier Loss 0.1733 (0.1738)	
Epoch: [0][300/475],	Time 56.469 (70.136)	Data 56.262 (69.879)	siamese Loss 0.2473 (1.7942)	classifier Loss 0.1733 (0.1738)	
Epoch: [0][320/475],	Time 61.962 (69.553)	Data 61.712 (69.300)	siamese Loss 0.2459 (1.6972)	classifier Loss 0.1731 (0.1738)	
Epoch: [0][340/475],	Time 52.255 (69.253)	Data 52.050 (69.003)	siamese Loss 0.2354 (1.6120)	classifier Loss 0.1732 (0.1737)	
Epoch: [0][360/475],	Time 60.908 (68.975)	Data 60.690 (68.727)	siamese Loss 0.2292 (1.5362)	classifier Loss 0.1734 (0.1737)	
Epoch: [0][380/475],	Time 29.166 (69.015)	Data 28.948 (68.769)	siamese Loss 0.2153 (1.4675)	classifier Loss 0.1733 (0.1737)	
Epoch: [0][400/475],	Time 82.081 (68.935)	Data 81.860 (68.691)	siamese Loss 0.2322 (1.4064)	classifier Loss 0.1733 (0.1737)	
Epoch: [0][420/475],	Time 102.245 (68.801)	Data 102.032 (68.559)	siamese Loss 0.2708 (1.3505)	classifier Loss 0.1730 (0.1737)	
Epoch: [0][440/475],	Time 84.127 (68.667)	Data 83.897 (68.426)	siamese Loss 0.3097 (1.3001)	classifier Loss 0.1734 (0.1736)	
Epoch: [0][460/475],	Time 89.083 (68.918)	Data 88.860 (68.679)	siamese Loss 0.2092 (1.2538)	classifier Loss 0.1734 (0.1736)	
/home/sjhu/env/anaconda3/envs/pyav-env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Validate: [0/46]	Time 53.845 (53.845)	siamese loss 0.7051 (0.7051)	clf loss 0.6928 (0.6928)	
Validate: [20/46]	Time 62.932 (68.773)	siamese loss 0.6358 (0.7258)	clf loss 0.6950 (0.6933)	
Validate: [40/46]	Time 52.996 (68.931)	siamese loss 0.8915 (0.7108)	clf loss 0.6932 (0.6928)	
/opt/conda/conda-bld/pytorch_1586848004339/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
Validating Results: siamese Loss 0.70174, classification loss 0.69294, Accuracy: 50.000%
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
Epoch: [1][0/475],	Time 290.656 (290.656)	Data 290.211 (290.211)	siamese Loss 0.2300 (0.2300)	classifier Loss 0.1732 (0.1732)	
Epoch: [1][20/475],	Time 44.870 (80.303)	Data 44.660 (80.081)	siamese Loss 0.2482 (0.2291)	classifier Loss 0.1732 (0.1733)	
Epoch: [1][40/475],	Time 62.185 (72.604)	Data 61.970 (72.389)	siamese Loss 0.2614 (0.2232)	classifier Loss 0.1732 (0.1733)	
Epoch: [1][60/475],	Time 63.913 (69.686)	Data 63.704 (69.475)	siamese Loss 0.2169 (0.2203)	classifier Loss 0.1734 (0.1733)	
Epoch: [1][80/475],	Time 63.070 (69.142)	Data 62.830 (68.932)	siamese Loss 0.1515 (0.2204)	classifier Loss 0.1731 (0.1733)	
Epoch: [1][100/475],	Time 79.219 (68.497)	Data 79.000 (68.288)	siamese Loss 0.2331 (0.2176)	classifier Loss 0.1732 (0.1733)	
Epoch: [1][120/475],	Time 61.506 (68.101)	Data 61.295 (67.892)	siamese Loss 0.2459 (0.2178)	classifier Loss 0.1732 (0.1733)	
Epoch: [1][140/475],	Time 39.643 (68.176)	Data 39.427 (67.966)	siamese Loss 0.2855 (0.2190)	classifier Loss 0.1732 (0.1733)	
Epoch: [1][160/475],	Time 99.397 (68.508)	Data 99.182 (68.298)	siamese Loss 0.2176 (0.2191)	classifier Loss 0.1735 (0.1733)	
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
Epoch: [1][180/475],	Time 103.037 (69.649)	Data 102.827 (69.440)	siamese Loss 0.1862 (0.2191)	classifier Loss 0.1735 (0.1733)	
Epoch: [1][200/475],	Time 73.459 (69.651)	Data 73.253 (69.442)	siamese Loss 0.2196 (0.2177)	classifier Loss 0.1735 (0.1733)	
Epoch: [1][220/475],	Time 35.549 (69.128)	Data 35.338 (68.919)	siamese Loss 0.2781 (0.2172)	classifier Loss 0.1737 (0.1733)	
Epoch: [1][240/475],	Time 39.141 (69.527)	Data 38.935 (69.319)	siamese Loss 0.2736 (0.2160)	classifier Loss 0.1735 (0.1733)	
Epoch: [1][260/475],	Time 61.023 (69.078)	Data 60.816 (68.869)	siamese Loss 0.2301 (0.2156)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][280/475],	Time 54.493 (68.935)	Data 54.285 (68.727)	siamese Loss 0.1900 (0.2156)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][300/475],	Time 56.533 (68.759)	Data 56.323 (68.550)	siamese Loss 0.2194 (0.2148)	classifier Loss 0.1734 (0.1733)	
Epoch: [1][320/475],	Time 60.583 (68.125)	Data 60.351 (67.916)	siamese Loss 0.2905 (0.2146)	classifier Loss 0.1731 (0.1733)	
Epoch: [1][340/475],	Time 52.028 (67.789)	Data 51.817 (67.581)	siamese Loss 0.2495 (0.2151)	classifier Loss 0.1734 (0.1733)	
Epoch: [1][360/475],	Time 59.198 (67.473)	Data 58.987 (67.265)	siamese Loss 0.2613 (0.2148)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][380/475],	Time 28.509 (67.528)	Data 28.300 (67.320)	siamese Loss 0.1628 (0.2149)	classifier Loss 0.1735 (0.1733)	
Epoch: [1][400/475],	Time 82.136 (67.419)	Data 81.923 (67.210)	siamese Loss 0.1683 (0.2145)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][420/475],	Time 97.976 (67.250)	Data 97.769 (67.041)	siamese Loss 0.2062 (0.2143)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][440/475],	Time 82.904 (67.074)	Data 82.696 (66.866)	siamese Loss 0.2495 (0.2145)	classifier Loss 0.1733 (0.1733)	
Epoch: [1][460/475],	Time 85.034 (67.288)	Data 84.818 (67.079)	siamese Loss 0.1923 (0.2138)	classifier Loss 0.1731 (0.1733)	
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
Epoch: [2][0/475],	Time 306.335 (306.335)	Data 306.104 (306.104)	siamese Loss 0.2469 (0.2469)	classifier Loss 0.1733 (0.1733)	
Epoch: [2][20/475],	Time 43.243 (78.375)	Data 43.028 (78.170)	siamese Loss 0.2589 (0.2173)	classifier Loss 0.1735 (0.1733)	
Epoch: [2][40/475],	Time 61.118 (71.019)	Data 60.894 (70.815)	siamese Loss 0.1861 (0.2053)	classifier Loss 0.1731 (0.1733)	
Epoch: [2][60/475],	Time 63.581 (68.798)	Data 63.374 (68.593)	siamese Loss 0.2033 (0.2022)	classifier Loss 0.1732 (0.1732)	
Epoch: [2][80/475],	Time 63.687 (68.627)	Data 63.480 (68.420)	siamese Loss 0.2629 (0.2056)	classifier Loss 0.1734 (0.1732)	
Epoch: [2][100/475],	Time 81.587 (68.188)	Data 81.376 (67.981)	siamese Loss 0.2035 (0.2055)	classifier Loss 0.1733 (0.1732)	
Epoch: [2][120/475],	Time 63.024 (67.697)	Data 62.809 (67.489)	siamese Loss 0.2932 (0.2064)	classifier Loss 0.1732 (0.1732)	
Epoch: [2][140/475],	Time 39.579 (67.802)	Data 39.369 (67.595)	siamese Loss 0.1809 (0.2061)	classifier Loss 0.1733 (0.1732)	
Epoch: [2][160/475],	Time 99.244 (68.162)	Data 99.021 (67.954)	siamese Loss 0.2281 (0.2068)	classifier Loss 0.1731 (0.1732)	
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
Epoch: [2][180/475],	Time 130.354 (69.874)	Data 130.094 (69.664)	siamese Loss 0.1866 (0.2077)	classifier Loss 0.1731 (0.1732)	
Epoch: [2][200/475],	Time 77.371 (71.228)	Data 77.080 (71.014)	siamese Loss 0.2503 (0.2072)	classifier Loss 0.1733 (0.1732)	
Epoch: [2][220/475],	Time 43.089 (71.175)	Data 42.838 (70.958)	siamese Loss 0.3177 (0.2067)	classifier Loss 0.1734 (0.1732)	
Epoch: [2][240/475],	Time 41.145 (71.975)	Data 40.919 (71.758)	siamese Loss 0.2334 (0.2050)	classifier Loss 0.1732 (0.1732)	
Epoch: [2][260/475],	Time 62.797 (71.681)	Data 62.578 (71.462)	siamese Loss 0.2423 (0.2039)	classifier Loss 0.1735 (0.1732)	
Epoch: [2][280/475],	Time 61.476 (71.739)	Data 61.241 (71.518)	siamese Loss 0.2035 (0.2036)	classifier Loss 0.1732 (0.1732)	
Epoch: [2][300/475],	Time 64.638 (71.904)	Data 64.390 (71.681)	siamese Loss 0.2440 (0.2038)	classifier Loss 0.1731 (0.1732)	
Epoch: [2][320/475],	Time 62.017 (71.425)	Data 61.749 (71.202)	siamese Loss 0.1699 (0.2028)	classifier Loss 0.1731 (0.1732)	
Epoch: [2][340/475],	Time 51.997 (71.033)	Data 51.777 (70.810)	siamese Loss 0.1923 (0.2032)	classifier Loss 0.1729 (0.1732)	
Epoch: [2][360/475],	Time 60.960 (70.658)	Data 60.688 (70.435)	siamese Loss 0.1850 (0.2037)	classifier Loss 0.1733 (0.1732)	
Epoch: [2][380/475],	Time 30.333 (70.664)	Data 30.095 (70.440)	siamese Loss 0.1448 (0.2040)	classifier Loss 0.1730 (0.1732)	
Epoch: [2][400/475],	Time 81.690 (70.544)	Data 81.438 (70.320)	siamese Loss 0.2082 (0.2038)	classifier Loss 0.1730 (0.1732)	
Epoch: [2][420/475],	Time 101.339 (70.371)	Data 101.101 (70.148)	siamese Loss 0.2326 (0.2030)	classifier Loss 0.1731 (0.1732)	
Epoch: [2][440/475],	Time 80.314 (70.089)	Data 80.099 (69.867)	siamese Loss 0.1954 (0.2031)	classifier Loss 0.1731 (0.1732)	
Epoch: [2][460/475],	Time 88.298 (70.299)	Data 88.095 (70.077)	siamese Loss 0.2653 (0.2028)	classifier Loss 0.1732 (0.1732)	
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
tcmalloc: large alloc 17377542144 bytes == 0x5647b6876000 @ 
Epoch: [3][0/475],	Time 304.402 (304.402)	Data 304.147 (304.147)	siamese Loss 0.2779 (0.2779)	classifier Loss 0.1731 (0.1731)	
Epoch: [3][20/475],	Time 45.840 (82.732)	Data 45.627 (82.504)	siamese Loss 0.2889 (0.2124)	classifier Loss 0.1734 (0.1731)	
Epoch: [3][40/475],	Time 61.163 (74.591)	Data 60.957 (74.362)	siamese Loss 0.1566 (0.1952)	classifier Loss 0.1731 (0.1731)	
Epoch: [3][60/475],	Time 65.269 (71.714)	Data 65.054 (71.489)	siamese Loss 0.2075 (0.1950)	classifier Loss 0.1732 (0.1731)	
Epoch: [3][80/475],	Time 75.469 (73.610)	Data 75.245 (73.380)	siamese Loss 0.2292 (0.2013)	classifier Loss 0.1733 (0.1731)	
Epoch: [3][100/475],	Time 93.359 (74.714)	Data 93.083 (74.480)	siamese Loss 0.1822 (0.2007)	classifier Loss 0.1731 (0.1731)	
Epoch: [3][120/475],	Time 88.659 (75.548)	Data 88.376 (75.310)	siamese Loss 0.3229 (0.2017)	classifier Loss 0.1733 (0.1731)	
Epoch: [3][140/475],	Time 41.381 (77.134)	Data 41.177 (76.893)	siamese Loss 0.1626 (0.1988)	classifier Loss 0.1732 (0.1731)	
